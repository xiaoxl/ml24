<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Logistic regression – Machine Learning Fall 2024</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/6/intro.html" rel="next">
<link href="../../contents/intro2pt/intro.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Fall 2024</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/5/intro.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/1/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/2/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">k-Nearest Neighbors algorithm (k-NN)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/3/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/4/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ensemble methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/intro2pt/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/5/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/6/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Netural networks</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#basic-idea" id="toc-basic-idea" class="nav-link active" data-scroll-target="#basic-idea"><span class="header-section-number">6.1</span> Basic idea</a>
  <ul class="collapse">
  <li><a href="#sigmoid-function" id="toc-sigmoid-function" class="nav-link" data-scroll-target="#sigmoid-function"><span class="header-section-number">6.1.1</span> Sigmoid function</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="header-section-number">6.1.2</span> Gradient descent</a></li>
  <li><a href="#the-formulas" id="toc-the-formulas" class="nav-link" data-scroll-target="#the-formulas"><span class="header-section-number">6.1.3</span> The Formulas</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">6.2</span> Regularization</a>
  <ul class="collapse">
  <li><a href="#three-types-of-errors" id="toc-three-types-of-errors" class="nav-link" data-scroll-target="#three-types-of-errors"><span class="header-section-number">6.2.1</span> Three types of errors</a></li>
  <li><a href="#underfit-vs-overfit" id="toc-underfit-vs-overfit" class="nav-link" data-scroll-target="#underfit-vs-overfit"><span class="header-section-number">6.2.2</span> Underfit vs Overfit</a></li>
  <li><a href="#learning-curves-accuracy-vs-training-size" id="toc-learning-curves-accuracy-vs-training-size" class="nav-link" data-scroll-target="#learning-curves-accuracy-vs-training-size"><span class="header-section-number">6.2.3</span> Learning curves (accuracy vs training size)</a></li>
  <li><a href="#regularization-1" id="toc-regularization-1" class="nav-link" data-scroll-target="#regularization-1"><span class="header-section-number">6.2.4</span> Regularization</a></li>
  </ul></li>
  <li><a href="#neural-network-implement-of-logistic-regression" id="toc-neural-network-implement-of-logistic-regression" class="nav-link" data-scroll-target="#neural-network-implement-of-logistic-regression"><span class="header-section-number">6.3</span> Neural network implement of Logistic regression</a>
  <ul class="collapse">
  <li><a href="#a-simple-example" id="toc-a-simple-example" class="nav-link" data-scroll-target="#a-simple-example"><span class="header-section-number">6.3.1</span> A simple example</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">6.3.2</span> Example</a></li>
  </ul></li>
  <li><a href="#pytorch-crash-course" id="toc-pytorch-crash-course" class="nav-link" data-scroll-target="#pytorch-crash-course"><span class="header-section-number">6.4</span> Pytorch crash course</a>
  <ul class="collapse">
  <li><a href="#tensor" id="toc-tensor" class="nav-link" data-scroll-target="#tensor"><span class="header-section-number">6.4.1</span> Tensor</a></li>
  <li><a href="#gradient-descent-1" id="toc-gradient-descent-1" class="nav-link" data-scroll-target="#gradient-descent-1"><span class="header-section-number">6.4.2</span> Gradient descent</a></li>
  <li><a href="#mini-batch" id="toc-mini-batch" class="nav-link" data-scroll-target="#mini-batch"><span class="header-section-number">6.4.3</span> Mini-batch</a></li>
  <li><a href="#codes" id="toc-codes" class="nav-link" data-scroll-target="#codes"><span class="header-section-number">6.4.4</span> Codes</a></li>
  <li><a href="#several-important-side-topics" id="toc-several-important-side-topics" class="nav-link" data-scroll-target="#several-important-side-topics"><span class="header-section-number">6.4.5</span> Several important side topics</a></li>
  </ul></li>
  <li><a href="#exercises-and-projects" id="toc-exercises-and-projects" class="nav-link" data-scroll-target="#exercises-and-projects"><span class="header-section-number">6.5</span> Exercises and Projects</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Logistic regression is very similar to linear regression, but applied to classification problems. In this chpater our idea is to treat it as the simplest example of a neural network instead of using other methods. The code we developped in the last chapter will be used extensively.</p>
<!-- 
Consider a set of training data $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots$, where $x^{(i)}=(x^{(i)}_1, x^{(i)}_2, \ldots, x^{(i)}_n)$ is a $n$-dim vector, and $y^{(i)}$ is a real number. We would like to use Linear regression to find the relation between $x$ and $y$. 

In this case, we assume that $y$ is a linear function of $x$:

$$
y=\theta_0 + \sum_{j=1}^n\theta_jx_j.
$$
The purpose of Linear regression is to used the given training data to find out the best $\Theta=(\theta_0, \theta_1, \theta_2,\ldots,\theta_n)$. 

If we set $\hat{x}=(1, x_1, \ldots,x_n)$, then the above formula can be reformulated by matrix multiplication.

$$
y=\Theta \hat{x}^T.
$$

When we want to deal with classification problem, we may still use this regression idea, but we have to do some modification.
 -->
<section id="basic-idea" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="basic-idea"><span class="header-section-number">6.1</span> Basic idea</h2>
<p>Assume that we have a binary classfification problem with <span class="math inline">\(N\)</span> features. Our model starts from the <em>logit</em> instead of the label <span class="math inline">\(y\)</span> itself.</p>
<p><span class="math display">\[
logit(y)=\theta_0+\sum_{j=1}^N\theta_jx_j.
\]</span></p>
<p>The logit function is used to describe the logorithm of the binary odds. The odd ratio is the ratio between the probability of success and the probability of failure. Assume the probability of success is <span class="math inline">\(p\)</span>. Then</p>
<p><span class="math display">\[
oddratio(p)=\frac{p}{1-p},\quad logit(p)=z = \log\qty(\frac{p}{1-p}).
\]</span> We could solve the logit function, and get its inverse: the function is the <em>Sigmoid</em> function. Once we have the logit value, we could use it to get the probability. <span class="math display">\[
p=\sigma(z)=\frac{1}{1+\mathrm{e}^{-z}}.
\]</span> <!-- 
The Logsitic regression is used to predict the probability of a data point belonging to a specific class. It is based on linear regression. The major difference is that logistic regreesion will have an activation function $\sigma$ at the final stage to change the predicted values of the linear regression to the values that indicate classes. In the case of binary classification, the outcome of $\sigma$ will be between $0$ and $1$, which is related to the two classes respectively. In this case, the number is interepted as the probability of the data to be in one of the specific class. --></p>
<p>Therefore the model for Logistic regression is as follows:</p>
<p><span class="math display">\[
p=\sigma(L(x))=\sigma\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)=\sigma\left(\Theta \hat{x}^T\right).
\]</span></p>
<!-- In most cases, this activation function is chosen to be the Sigmoid funciton. -->
<section id="sigmoid-function" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="sigmoid-function"><span class="header-section-number">6.1.1</span> Sigmoid function</h3>
<p>The <em>Sigmoid</em> function is defined as follows:</p>
<p><span class="math display">\[
\sigma(z)=\frac{1}{1+\mathrm{e}^{-z}}.
\]</span> The graph of the function is shown below.</p>
<div id="0f9c90bc" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-2-output-1.png" width="571" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The main properties of <span class="math inline">\(\sigma\)</span> are listed below as a Lemma.</p>
<div id="lem-sig" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.1</strong></span> The Sigmoid function <span class="math inline">\(\sigma(z)\)</span> satisfies the following properties.</p>
<ol type="1">
<li><span class="math inline">\(\sigma(z)\rightarrow \infty\)</span> when <span class="math inline">\(z\mapsto \infty\)</span>.</li>
<li><span class="math inline">\(\sigma(z)\rightarrow -\infty\)</span> when <span class="math inline">\(z\mapsto -\infty\)</span>.</li>
<li><span class="math inline">\(\sigma(0)=0.5\)</span>.</li>
<li><span class="math inline">\(\sigma(z)\)</span> is always increasing.</li>
<li><span class="math inline">\(\sigma'(z)=\sigma(z)(1-\sigma(z))\)</span>.</li>
</ol>
</div>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>We will only look at the last one.</p>
<p><span class="math display">\[
\begin{split}
\sigma'(z)&amp;=-\frac{(1+\mathrm e^{-z})'}{(1+\mathrm e^{-z})^2}=\frac{\mathrm e^{-z}}{(1+\mathrm e^{-z})^2}=\frac{1}{1+\mathrm e^{-z}}\frac{\mathrm e^{-z}}{1+\mathrm e^{-z}}\\
&amp;=\sigma(z)\left(\frac{1+\mathrm e^{-z}}{1+\mathrm e^{-z}}-\frac{1}{1+\mathrm e^{-z}}\right)=\sigma(z)(1-\sigma(z)).
\end{split}
\]</span></p>
</div>
</section>
<section id="gradient-descent" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">6.1.2</span> Gradient descent</h3>
<!-- Assume that we would like to minimize a function $J(\Theta)$, where this $\Theta$ is an $N$-dim vector. Geometricly, we could treat $J$ as a height function, and it tells us the height of the mountain. Then to minimize $J$ is the same thing as to find the lowest point. One idea is to move towards the lowest point step by step. During each step we only need to lower our current height. After several steps we will be around the lowest point.

The geometric meaning of $\nabla J$ is the direction that $J$ increase the most. Therefore the opposite direction is the one we want to move in. The formula to update $x$ is 

$$
\Theta_{\text{new}} = \Theta_{\text{old}}-\alpha \nabla J(\Theta_{\text{old}}),
$$
where $\alpha$ is called the *learning rate* which controls how fast you want to learn. Usually if $\alpha$ is small, the learning tends to be slow and stble, and when $\alpha$ is big, the learning tends to be fast and unstable. -->
<!-- In machine learning, in most cases we would like to formulate the problem in terms of finding the lowest point of a *cost function* $J(\Theta)$.  -->
<p>We would like to use Gradient descent to sovle Logistic regression problems. For binary classification problem, the cost function is defined to be</p>
<p><span class="math display">\[
J(\Theta)=-\frac1m\sum_{i=1}^m\left[y^{(i)}\log(p^{(i)})+(1-y^{(i)})\log(1-p^{(i)})\right].
\]</span> Here <span class="math inline">\(m\)</span> is the number of data points, <span class="math inline">\(y^{(i)}\)</span> is the labelled result (which is either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>), <span class="math inline">\(p^{(i)}\)</span> is the predicted value (which is between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The algorithm gets its name since we are using the gradient to find a direction to lower our height.</p>
</div>
</div>
</section>
<section id="the-formulas" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="the-formulas"><span class="header-section-number">6.1.3</span> The Formulas</h3>
<div id="thm-reggrad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1</strong></span> The gradient of <span class="math inline">\(J\)</span> is computed by</p>
<p><span id="eq-nablaJ"><span class="math display">\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\tag{6.1}\]</span></span></p>
</div>
<details>
<summary>
Click for details.
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The formula is an application of the chain rule for the multivariable functions.</p>
<p><span class="math display">\[
\begin{split}
\dfrac{\partial p}{\partial \theta_k}&amp;=\dfrac{\partial}{\partial \theta_k}\sigma\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)=\dfrac{\partial}{\partial \theta_k}\sigma(L(\Theta))\\
&amp;=\sigma(L)(1-\sigma(L))\dfrac{\partial}{\partial \theta_k}\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)\\
&amp;=\begin{cases}
p(1-p)&amp;\text{ if }k=0,\\
p(1-p)x_k&amp;\text{ otherwise}.
\end{cases}
\end{split}
\]</span> Then</p>
<p><span class="math display">\[
\nabla p = \left(\frac{\partial p}{\partial\theta_0},\ldots,\frac{\partial p}{\partial\theta_n}\right) = p(1-p)\hat{x}.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\nabla \log(p) = \frac{\nabla p}p =\frac{p(1-p)\hat{x}}{p}=(1-p)\hat{x}.
\]</span></p>
<p><span class="math display">\[
\nabla \log(1-p) = \frac{-\nabla p}{1-p} =-\frac{p(1-p)\hat{x}}{1-p}=-p\hat{x}.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{split}
\nabla J&amp; = -\frac1m\sum_{i=1}^m\left[y^{(i)}\nabla \log(p^{(i)})+(1-y^{(i)})\nabla \log(1-p^{(i)})\right]\\
&amp;=-\frac1m\sum_{i=1}^m\left[y^{(i)}(1-p^{(i)})\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\hat{x}^{(i)})\right]\\
&amp;=-\frac1m\sum_{i=1}^m\left[(y^{(i)}-p^{(i)})\hat{x}^{(i)}\right].
\end{split}
\]</span></p>
<p>We write <span class="math inline">\(\hat{x}^{(i)}\)</span> as row vectors, and stack all these row vectors vertically. What we get is a matrix <span class="math inline">\(\hat{\textbf X}\)</span> of the size <span class="math inline">\(m\times (1+n)\)</span>. We stack all <span class="math inline">\(y^{(i)}\)</span> (resp. <span class="math inline">\(p^{(i)}\)</span>) vectically to get the <span class="math inline">\(m\)</span>-dim column vector <span class="math inline">\(\textbf y\)</span> (resp. <span class="math inline">\(\textbf p\)</span>).</p>
<p>Using this notation, the previous formula becomes</p>
<p><span class="math display">\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\]</span></p>
<p>After the gradient can be computed, we can start to use the gradient descent method. Note that, although <span class="math inline">\(\Theta\)</span> are not explicitly presented in the formula of <span class="math inline">\(\nabla J\)</span>, this is used to modify <span class="math inline">\(\Theta\)</span>:</p>
<p><span class="math display">\[
\Theta_{s+1} = \Theta_s - \alpha\nabla J.
\]</span></p>
</div>
</details>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you directly use library, like <code>sklearn</code> or <code>PyTorch</code>, they will handle the concrete computation of these gradients.</p>
</div>
</div>
</section>
</section>
<section id="regularization" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="regularization"><span class="header-section-number">6.2</span> Regularization</h2>
<section id="three-types-of-errors" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="three-types-of-errors"><span class="header-section-number">6.2.1</span> Three types of errors</h3>
<p>Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The <strong>bias</strong> of an estimator is its average error for different training sets. The <strong>variance</strong> of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.</p>
</section>
<section id="underfit-vs-overfit" class="level3 page-columns page-full" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="underfit-vs-overfit"><span class="header-section-number">6.2.2</span> Underfit vs Overfit</h3>
<p>When fit a model to data, it is highly possible that the model is underfit or overfit.</p>
<p>Roughly speaking, <strong>underfit</strong> means the model is not sufficient to fit the training samples, and <strong>overfit</strong> means that the models learns too many noise from the data. In many cases, high bias is related to underfit, and high variance is related to overfit.</p>
<p>The following example is from <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py">the <code>sklearn</code> guide</a>. Although it is a polynomial regression example, it grasps the key idea of underfit and overfit.</p>
<div id="717d4014" class="cell page-columns page-full" data-execution_count="2">
<div class="cell-output cell-output-display page-columns page-full">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="intro_files/figure-html/cell-3-output-1.png" width="1079" height="445" class="figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="learning-curves-accuracy-vs-training-size" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="learning-curves-accuracy-vs-training-size"><span class="header-section-number">6.2.3</span> Learning curves (accuracy vs training size)</h3>
<p>A learning curve shows the validation and training score of an estimator for varying a key hyperparameter. In most cases the key hyperparameter is the training size or the number of epochs. It is a tool to find out how much we benefit from altering the hyperparameter by training more data or training for more epochs, and whether the estimator suffers more from a variance error or a bias error.</p>
<p><code>sklearn</code> provides <code>sklearn.model_selection.learning_curve()</code> to generate the values that are required to plot such a learning curve. However this function is just related to the sample size. If we would like to talk about epochs, we need other packages.</p>
<p>Let us first look at the learning curve about sample size. The official document page is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html">here</a>. The function takes input <code>estimator</code>, dataset <code>X</code>, <code>y</code>, and an arry-like argument <code>train_sizes</code>. The dataset <code>(X, y)</code> will be split into pieces using the cross-validation technique. The number of pieces is set by the argument <code>cv</code>. The default value is <code>cv=5</code>. For details about cross-validation please see <a href="../2/intro.html#sec-cross-validation" class="quarto-xref"><span>Section 2.2.6</span></a>.</p>
<p>Then the model is trained over a random sample of the training set, and evaluate the score over the test set. The size of the sample of the training set is set by the argument <code>train_sizes</code>. This argument is array-like. Therefore the process will be repeated several times, and we can see the impact of increasing the training size.</p>
<p>The output contains three pieces. The first is <code>train_sizes_abs</code> which is the number of elements in each training set. This output is mainly for reference. The difference between the output and the input <code>train_sizes</code> is that the input can be float which represents the percentagy. The output is always the exact number of elements.</p>
<p>The second output is <code>train_scores</code> and the third is <code>test_scores</code>, both of which are the scores we get from the training and testing process. Note that both are 2D <code>numpy</code> arrays, of the size <code>(number of different sizes, cv)</code>. Each row is a 1D <code>numpy</code> array representing the cross-validation scores, which is corresponding to a train size. If we want the mean as the cross-validation score, we could use <code>train_scores.mean(axis=1)</code>.</p>
<p>After understanding the input and output, we could plot the learning curve. We still use the <code>horse colic</code> as the example. The details about the dataset can be found <a href="https://xiaoxl.github.io/Datasets/contents/horse_colic.html">here</a>.</p>
<div id="4d53e931" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>url <span class="op">=</span> <span class="st">'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>df <span class="op">=</span> pd.read_csv(url, delim_whitespace<span class="op">=</span><span class="va">True</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>df <span class="op">=</span> df.replace(<span class="st">"?"</span>, np.NaN)</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>df.fillna(<span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a>df.drop(columns<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">24</span>, <span class="dv">25</span>, <span class="dv">26</span>, <span class="dv">27</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a>df[<span class="dv">23</span>].replace({<span class="dv">1</span>: <span class="dv">1</span>, <span class="dv">2</span>: <span class="dv">0</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-11"><a href="#cb1-11"></a>X <span class="op">=</span> df.iloc[:, :<span class="op">-</span><span class="dv">1</span>].to_numpy().astype(<span class="bu">float</span>)</span>
<span id="cb1-12"><a href="#cb1-12"></a>y <span class="op">=</span> df[<span class="dv">23</span>].to_numpy().astype(<span class="bu">int</span>)</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-15"><a href="#cb1-15"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\Xinli\AppData\Local\Temp\ipykernel_65712\73942173.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\s+'`` instead
  df = pd.read_csv(url, delim_whitespace=True, header=None)
C:\Users\Xinli\AppData\Local\Temp\ipykernel_65712\73942173.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df[23].replace({1: 1, 2: 0}, inplace=True)</code></pre>
</div>
</div>
<p>We use the model <code>LogisticRegression</code>. The following code plot the learning curve for this model.</p>
<div id="5600981b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>clf <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>steps <span class="op">=</span> [(<span class="st">'scalar'</span>, MinMaxScaler()),</span>
<span id="cb3-7"><a href="#cb3-7"></a>         (<span class="st">'log'</span>, clf)]</span>
<span id="cb3-8"><a href="#cb3-8"></a>pipe <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-11"><a href="#cb3-11"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(pipe, X_train, y_train,</span>
<span id="cb3-12"><a href="#cb3-12"></a>                                                        train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">20</span>))</span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-15"><a href="#cb3-15"></a>plt.plot(train_sizes, train_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'train'</span>)</span>
<span id="cb3-16"><a href="#cb3-16"></a>plt.plot(train_sizes, test_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'test'</span>)</span>
<span id="cb3-17"><a href="#cb3-17"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-5-output-1.png" width="579" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The learning curve is a primary tool for us to study the bias and variance. Usually</p>
<ul>
<li>If the two training curve and the testing curve are very close to each other, this means that the variance is low. Otherwise the variance is high, and this means that the model probabily suffer from overfitting.</li>
<li>If the absolute training curve score is high, this means that the bias is low. Otherwise the bias is high, and this means that the model probabily suffer from underfitting.</li>
</ul>
<p>In the above example, although regularization is applied by default, you may still notice some overfitting there.</p>
</section>
<section id="regularization-1" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="regularization-1"><span class="header-section-number">6.2.4</span> Regularization</h3>
<p>Regularization is a technique to deal with overfitting. Here we only talk about the simplest method: ridge regression, also known as Tikhonov regularizaiton. Because of the formula given below, it is also called <em><span class="math inline">\(L_2\)</span> regularization</em>. The idea is to add an additional term <span class="math inline">\(\dfrac{\alpha}{2m}\sum_{i=1}^m\theta_i^2\)</span> to the original cost function. When training with the new cost function, this additional term will force the parameters in the original term to be as small as possible. After finishing training, the additional term will be dropped, and we use the original cost function for validation and testing. Note that in the additional term <span class="math inline">\(\theta_0\)</span> is not presented.</p>
<p>The hyperparameter <span class="math inline">\(\alpha\)</span> is the <em>regularization strength</em>. If <span class="math inline">\(\alpha=0\)</span>, the new cost function becomes the original one; If <span class="math inline">\(\alpha\)</span> is very large, the additional term dominates, and it will force all parameters to be almost <span class="math inline">\(0\)</span>. In different context, the regularization strength is also given by <span class="math inline">\(C=\dfrac{1}{2\alpha}\)</span>, called <em>inverse of regularization strength</em>.</p>
<section id="the-math-of-regularization" class="level4" data-number="6.2.4.1">
<h4 data-number="6.2.4.1" class="anchored" data-anchor-id="the-math-of-regularization"><span class="header-section-number">6.2.4.1</span> The math of regularization</h4>
<div id="thm-ridgegrad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2</strong></span> The gradient of the ridge regression cost function is</p>
<p><span class="math display">\[
\nabla J=\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}+\frac{\alpha}{m}\Theta.
\]</span></p>
<p>Note that <span class="math inline">\(\Theta\)</span> doesn’t contain <span class="math inline">\(\theta_0\)</span>, or you may treat <span class="math inline">\(\theta_0=0\)</span>.</p>
</div>
<p>The computation is straightforward.</p>
</section>
<section id="the-code" class="level4" data-number="6.2.4.2">
<h4 data-number="6.2.4.2" class="anchored" data-anchor-id="the-code"><span class="header-section-number">6.2.4.2</span> The code</h4>
<p>Regularization is directly provided by the logistic regression functions.</p>
<ul>
<li>In <code>LogisticRegression</code>, the regularization is given by the argument <code>penalty</code> and <code>C</code>. <code>penalty</code> specifies the regularizaiton method. It is <code>l2</code> by default, which is the method above. <code>C</code> is the inverse of regularization strength, whose default value is <code>1</code>.</li>
<li>In <code>SGDClassifier</code>, the regularization is given by the argument <code>penalty</code> and <code>alpha</code>. <code>penalty</code> is the same as that in <code>LogisticRegression</code>, and <code>alpha</code> is the regularization strength, whose default value is <code>0.0001</code>.</li>
</ul>
<p>Let us see the above example.</p>
<div id="d08aac53" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>clf <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, C<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a>steps <span class="op">=</span> [(<span class="st">'scalar'</span>, MinMaxScaler()),</span>
<span id="cb4-3"><a href="#cb4-3"></a>         (<span class="st">'log'</span>, clf)]</span>
<span id="cb4-4"><a href="#cb4-4"></a>pipe <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-7"><a href="#cb4-7"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(pipe, X_train, y_train,</span>
<span id="cb4-8"><a href="#cb4-8"></a>                                                        train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">20</span>))</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-11"><a href="#cb4-11"></a>plt.plot(train_sizes, train_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'train'</span>)</span>
<span id="cb4-12"><a href="#cb4-12"></a>plt.plot(train_sizes, test_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'test'</span>)</span>
<span id="cb4-13"><a href="#cb4-13"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-6-output-1.png" width="588" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>After we reduce <code>C</code> from <code>1</code> to <code>0.1</code>, the regularization strength is increased. Then you may find that the gap between the two curves are reduced. However the overall performace is also reduced, from 85%~90% in <code>C=1</code> case to around 80% in <code>C=0.1</code> case. This means that the model doesn’t fit the data well as the previous one. Therefore this is a trade-off: decrease the variance but increase the bias.</p>
</section>
</section>
</section>
<section id="neural-network-implement-of-logistic-regression" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="neural-network-implement-of-logistic-regression"><span class="header-section-number">6.3</span> Neural network implement of Logistic regression</h2>
<p>In the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. We are going to use <code>PyTorch</code> to implement it. We will reuse many codes we wrote in the previous chapter.</p>
<section id="a-simple-example" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="a-simple-example"><span class="header-section-number">6.3.1</span> A simple example</h3>
<p>We</p>
</section>
<section id="example" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="example"><span class="header-section-number">6.3.2</span> Example</h3>
<p>We still use the horse colic dataset as an example. We first prepare the dataset.</p>
<div id="b8e32009" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>url <span class="op">=</span> <span class="st">'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>df <span class="op">=</span> pd.read_csv(url, sep<span class="op">=</span><span class="st">'</span><span class="ch">\\</span><span class="st">s+'</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb5-7"><a href="#cb5-7"></a>df <span class="op">=</span> df.replace(<span class="st">"?"</span>, np.NaN)</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>df.fillna(<span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-10"><a href="#cb5-10"></a>df <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">24</span>, <span class="dv">25</span>, <span class="dv">26</span>, <span class="dv">27</span>])</span>
<span id="cb5-11"><a href="#cb5-11"></a>df[<span class="dv">23</span>] <span class="op">=</span> df[<span class="dv">23</span>].replace({<span class="dv">1</span>: <span class="dv">1</span>, <span class="dv">2</span>: <span class="dv">0</span>})</span>
<span id="cb5-12"><a href="#cb5-12"></a>X <span class="op">=</span> df.iloc[:, :<span class="op">-</span><span class="dv">1</span>].to_numpy().astype(<span class="bu">float</span>)</span>
<span id="cb5-13"><a href="#cb5-13"></a>y <span class="op">=</span> df[<span class="dv">23</span>].to_numpy().astype(<span class="bu">int</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a></span>
<span id="cb5-15"><a href="#cb5-15"></a>SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb5-16"><a href="#cb5-16"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span>SEED)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We need to perform normalization before throwing the data into the model. Here we use the <code>MinMaxScaler()</code> from <code>sklearn</code> package.</p>
<div id="82dde565" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb6-2"><a href="#cb6-2"></a>mms <span class="op">=</span> MinMaxScaler()</span>
<span id="cb6-3"><a href="#cb6-3"></a>X_train <span class="op">=</span> mms.fit_transform(X_train, y_train)</span>
<span id="cb6-4"><a href="#cb6-4"></a>X_test <span class="op">=</span> mms.transform(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we write a <code>Dataset</code> class to build the dataset and create the dataloaders. Since the set is already split, we don’t need to <code>random_split</code> here.</p>
<div id="8da0cffe" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">import</span> torch</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="kw">class</span> MyData(Dataset):</span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, X, y):</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="va">self</span>.X <span class="op">=</span> torch.tensor(X, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb7-7"><a href="#cb7-7"></a>        <span class="va">self</span>.y <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span><span class="bu">float</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb7-10"><a href="#cb7-10"></a>        <span class="cf">return</span> (<span class="va">self</span>.X[index], <span class="va">self</span>.y[index])</span>
<span id="cb7-11"><a href="#cb7-11"></a></span>
<span id="cb7-12"><a href="#cb7-12"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-13"><a href="#cb7-13"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.y)</span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>train_set <span class="op">=</span> MyData(X_train, y_train)</span>
<span id="cb7-17"><a href="#cb7-17"></a>val_set <span class="op">=</span> MyData(X_test, y_test)</span>
<span id="cb7-18"><a href="#cb7-18"></a></span>
<span id="cb7-19"><a href="#cb7-19"></a>train_loader <span class="op">=</span> DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-20"><a href="#cb7-20"></a>val_loader <span class="op">=</span> DataLoader(val_set, batch_size<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the following code, we first set up the original model.</p>
<div id="f83da1ec" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">from</span> torch.nn.modules <span class="im">import</span> Linear</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="kw">class</span> LoR(nn.Module):</span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb8-6"><a href="#cb8-6"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb8-7"><a href="#cb8-7"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(in_features<span class="op">=</span><span class="dv">22</span>, out_features<span class="op">=</span><span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb8-8"><a href="#cb8-8"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb8-9"><a href="#cb8-9"></a></span>
<span id="cb8-10"><a href="#cb8-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb8-11"><a href="#cb8-11"></a>        <span class="co"># pred = self.activation(self.linear(X))</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>        pred <span class="op">=</span> <span class="va">self</span>.linear(X)</span>
<span id="cb8-13"><a href="#cb8-13"></a>        <span class="co"># return (pred &gt;= 0).float()</span></span>
<span id="cb8-14"><a href="#cb8-14"></a>        <span class="cf">return</span> pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we derive the base <code>ModelTemplate</code> class.</p>
<div id="7183db24" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> LoRModel(ModelTemplate):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, loss_fn, optimizer):</span>
<span id="cb9-3"><a href="#cb9-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(model, loss_fn, optimizer)</span>
<span id="cb9-4"><a href="#cb9-4"></a>        <span class="va">self</span>.stats[<span class="st">'acc_train'</span>] <span class="op">=</span> []</span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="va">self</span>.stats[<span class="st">'acc_val'</span>] <span class="op">=</span> []</span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a>    <span class="kw">def</span> compute_acc(<span class="va">self</span>, dataloader):</span>
<span id="cb9-8"><a href="#cb9-8"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-9"><a href="#cb9-9"></a>            acc <span class="op">=</span> []</span>
<span id="cb9-10"><a href="#cb9-10"></a>            <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> dataloader:</span>
<span id="cb9-11"><a href="#cb9-11"></a>                yhat <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.model(X_batch))</span>
<span id="cb9-12"><a href="#cb9-12"></a>                y_pred <span class="op">=</span> (yhat<span class="op">&gt;=</span><span class="fl">0.5</span>).to(<span class="bu">float</span>)</span>
<span id="cb9-13"><a href="#cb9-13"></a>                acc.append((y_pred<span class="op">==</span>y_batch).<span class="bu">sum</span>().item())</span>
<span id="cb9-14"><a href="#cb9-14"></a>            <span class="co"># print(acc_train)</span></span>
<span id="cb9-15"><a href="#cb9-15"></a>        <span class="cf">return</span> np.<span class="bu">sum</span>(acc)<span class="op">/</span><span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb9-16"><a href="#cb9-16"></a></span>
<span id="cb9-17"><a href="#cb9-17"></a>    <span class="kw">def</span> log_update(<span class="va">self</span>, train_time, loss, val_time, val_loss, train_loader, val_loader):</span>
<span id="cb9-18"><a href="#cb9-18"></a>        <span class="bu">super</span>().log_update(train_time, loss, val_time, val_loss, train_loader, val_loader)</span>
<span id="cb9-19"><a href="#cb9-19"></a>        acc_train <span class="op">=</span> <span class="va">self</span>.compute_acc(train_loader)</span>
<span id="cb9-20"><a href="#cb9-20"></a>        acc_val <span class="op">=</span> <span class="va">self</span>.compute_acc(val_loader)</span>
<span id="cb9-21"><a href="#cb9-21"></a>        <span class="va">self</span>.stats[<span class="st">'acc_train'</span>].append(acc_train)</span>
<span id="cb9-22"><a href="#cb9-22"></a>        <span class="va">self</span>.stats[<span class="st">'acc_val'</span>].append(acc_val)</span>
<span id="cb9-23"><a href="#cb9-23"></a></span>
<span id="cb9-24"><a href="#cb9-24"></a></span>
<span id="cb9-25"><a href="#cb9-25"></a>        <span class="co"># p = self.model.state_dict()</span></span>
<span id="cb9-26"><a href="#cb9-26"></a>        <span class="co"># self.stats['acc'].append([p['linear.bias'].item(), p['linear.weight'].item()])</span></span>
<span id="cb9-27"><a href="#cb9-27"></a></span>
<span id="cb9-28"><a href="#cb9-28"></a>    <span class="kw">def</span> log_output(<span class="va">self</span>, verbose<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb9-29"><a href="#cb9-29"></a>        s <span class="op">=</span> <span class="bu">super</span>().log_output(verbose<span class="op">=</span><span class="dv">0</span>, formatstr<span class="op">=</span><span class="st">':.6f'</span>)</span>
<span id="cb9-30"><a href="#cb9-30"></a>        s.append(<span class="ss">f'acc_train: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>stats[<span class="st">'acc_train'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.6f}</span><span class="ss">'</span>)</span>
<span id="cb9-31"><a href="#cb9-31"></a>        s.append(<span class="ss">f'acc_val: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>stats[<span class="st">'acc_val'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.6f}</span><span class="ss">'</span>)</span>
<span id="cb9-32"><a href="#cb9-32"></a>        <span class="co"># s.append(f'p: [{self.stats['p'][-1][0]:.6f}, {self.stats['p'][-1][1]:.6f}]')</span></span>
<span id="cb9-33"><a href="#cb9-33"></a>        <span class="cf">if</span> verbose <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb9-34"><a href="#cb9-34"></a>            <span class="bu">print</span>(<span class="st">' '</span>.join(s))</span>
<span id="cb9-35"><a href="#cb9-35"></a>        <span class="cf">return</span> s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="85840af9" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> SGD</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">from</span> torch.nn <span class="im">import</span> BCEWithLogitsLoss, BCELoss</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a>original_model <span class="op">=</span> LoR()</span>
<span id="cb10-5"><a href="#cb10-5"></a>model <span class="op">=</span> LoRModel(model<span class="op">=</span>original_model, loss_fn<span class="op">=</span>BCEWithLogitsLoss(),</span>
<span id="cb10-6"><a href="#cb10-6"></a>                 optimizer<span class="op">=</span>SGD(original_model.parameters(), lr <span class="op">=</span> <span class="fl">0.1</span>))</span>
<span id="cb10-7"><a href="#cb10-7"></a></span>
<span id="cb10-8"><a href="#cb10-8"></a>model.train(train_loader, val_loader, epoch_num<span class="op">=</span><span class="dv">100</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 1 train_time: 0.009149 loss: 0.674627 val_time: 0.001110 val_loss: 0.619009 acc_train: 0.627451 acc_val: 0.688889
epoch 2 train_time: 0.004470 loss: 0.647466 val_time: 0.001009 val_loss: 0.606279 acc_train: 0.627451 acc_val: 0.688889
epoch 3 train_time: 0.004992 loss: 0.631382 val_time: 0.001513 val_loss: 0.587835 acc_train: 0.627451 acc_val: 0.688889
epoch 4 train_time: 0.004513 loss: 0.619163 val_time: 0.000999 val_loss: 0.579341 acc_train: 0.631373 acc_val: 0.688889
epoch 5 train_time: 0.005146 loss: 0.609541 val_time: 0.000000 val_loss: 0.570777 acc_train: 0.631373 acc_val: 0.711111
epoch 6 train_time: 0.005018 loss: 0.600289 val_time: 0.000999 val_loss: 0.564059 acc_train: 0.639216 acc_val: 0.711111
epoch 7 train_time: 0.005141 loss: 0.590659 val_time: 0.001001 val_loss: 0.561695 acc_train: 0.654902 acc_val: 0.688889
epoch 8 train_time: 0.006529 loss: 0.581817 val_time: 0.001006 val_loss: 0.556573 acc_train: 0.658824 acc_val: 0.666667
epoch 9 train_time: 0.006056 loss: 0.575897 val_time: 0.001006 val_loss: 0.555410 acc_train: 0.701961 acc_val: 0.711111
epoch 10 train_time: 0.004529 loss: 0.568207 val_time: 0.001007 val_loss: 0.549390 acc_train: 0.709804 acc_val: 0.711111
epoch 11 train_time: 0.006530 loss: 0.561794 val_time: 0.000997 val_loss: 0.543227 acc_train: 0.717647 acc_val: 0.711111
epoch 12 train_time: 0.006521 loss: 0.556270 val_time: 0.001000 val_loss: 0.538794 acc_train: 0.729412 acc_val: 0.711111
epoch 13 train_time: 0.006516 loss: 0.551529 val_time: 0.001503 val_loss: 0.538478 acc_train: 0.745098 acc_val: 0.711111
epoch 14 train_time: 0.006579 loss: 0.546595 val_time: 0.001001 val_loss: 0.537497 acc_train: 0.749020 acc_val: 0.733333
epoch 15 train_time: 0.006550 loss: 0.543805 val_time: 0.000991 val_loss: 0.533154 acc_train: 0.752941 acc_val: 0.733333
epoch 16 train_time: 0.006036 loss: 0.537000 val_time: 0.000999 val_loss: 0.530011 acc_train: 0.752941 acc_val: 0.733333
epoch 17 train_time: 0.004514 loss: 0.533244 val_time: 0.001003 val_loss: 0.526669 acc_train: 0.760784 acc_val: 0.733333
epoch 18 train_time: 0.005025 loss: 0.529177 val_time: 0.001003 val_loss: 0.525024 acc_train: 0.764706 acc_val: 0.733333
epoch 19 train_time: 0.006732 loss: 0.525873 val_time: 0.001000 val_loss: 0.523801 acc_train: 0.776471 acc_val: 0.733333
epoch 20 train_time: 0.008019 loss: 0.521512 val_time: 0.001125 val_loss: 0.522176 acc_train: 0.780392 acc_val: 0.755556
epoch 21 train_time: 0.006521 loss: 0.518520 val_time: 0.000000 val_loss: 0.519269 acc_train: 0.780392 acc_val: 0.755556
epoch 22 train_time: 0.006311 loss: 0.515101 val_time: 0.001008 val_loss: 0.515778 acc_train: 0.776471 acc_val: 0.733333
epoch 23 train_time: 0.006001 loss: 0.513203 val_time: 0.001000 val_loss: 0.519432 acc_train: 0.784314 acc_val: 0.755556
epoch 24 train_time: 0.005550 loss: 0.509660 val_time: 0.000999 val_loss: 0.515220 acc_train: 0.784314 acc_val: 0.777778
epoch 25 train_time: 0.004506 loss: 0.506943 val_time: 0.001003 val_loss: 0.512260 acc_train: 0.788235 acc_val: 0.777778
epoch 26 train_time: 0.005409 loss: 0.505380 val_time: 0.001000 val_loss: 0.513752 acc_train: 0.780392 acc_val: 0.755556
epoch 27 train_time: 0.007043 loss: 0.501728 val_time: 0.000000 val_loss: 0.513422 acc_train: 0.780392 acc_val: 0.755556
epoch 28 train_time: 0.004558 loss: 0.500234 val_time: 0.000000 val_loss: 0.508129 acc_train: 0.780392 acc_val: 0.755556
epoch 29 train_time: 0.005512 loss: 0.497392 val_time: 0.001000 val_loss: 0.510038 acc_train: 0.784314 acc_val: 0.755556
epoch 30 train_time: 0.004509 loss: 0.495293 val_time: 0.001000 val_loss: 0.506375 acc_train: 0.784314 acc_val: 0.755556
epoch 31 train_time: 0.006531 loss: 0.493891 val_time: 0.001010 val_loss: 0.508324 acc_train: 0.784314 acc_val: 0.755556
epoch 32 train_time: 0.006678 loss: 0.490733 val_time: 0.001011 val_loss: 0.502308 acc_train: 0.784314 acc_val: 0.755556
epoch 33 train_time: 0.006102 loss: 0.489358 val_time: 0.001507 val_loss: 0.504019 acc_train: 0.788235 acc_val: 0.755556
epoch 34 train_time: 0.006013 loss: 0.486646 val_time: 0.001000 val_loss: 0.504261 acc_train: 0.784314 acc_val: 0.755556
epoch 35 train_time: 0.005564 loss: 0.485487 val_time: 0.000976 val_loss: 0.502926 acc_train: 0.784314 acc_val: 0.755556
epoch 36 train_time: 0.005718 loss: 0.483562 val_time: 0.001007 val_loss: 0.504548 acc_train: 0.792157 acc_val: 0.755556
epoch 37 train_time: 0.006582 loss: 0.481137 val_time: 0.001072 val_loss: 0.502033 acc_train: 0.792157 acc_val: 0.755556
epoch 38 train_time: 0.005511 loss: 0.479781 val_time: 0.001003 val_loss: 0.501860 acc_train: 0.796078 acc_val: 0.755556
epoch 39 train_time: 0.006415 loss: 0.478170 val_time: 0.001165 val_loss: 0.500380 acc_train: 0.800000 acc_val: 0.755556
epoch 40 train_time: 0.006022 loss: 0.476871 val_time: 0.001000 val_loss: 0.498162 acc_train: 0.796078 acc_val: 0.755556
epoch 41 train_time: 0.005652 loss: 0.475505 val_time: 0.001009 val_loss: 0.499792 acc_train: 0.796078 acc_val: 0.777778
epoch 42 train_time: 0.006061 loss: 0.473835 val_time: 0.001005 val_loss: 0.498708 acc_train: 0.796078 acc_val: 0.777778
epoch 43 train_time: 0.006543 loss: 0.472051 val_time: 0.001000 val_loss: 0.501097 acc_train: 0.796078 acc_val: 0.777778
epoch 44 train_time: 0.007028 loss: 0.470318 val_time: 0.001007 val_loss: 0.497102 acc_train: 0.800000 acc_val: 0.777778
epoch 45 train_time: 0.007029 loss: 0.468182 val_time: 0.000000 val_loss: 0.494473 acc_train: 0.800000 acc_val: 0.777778
epoch 46 train_time: 0.006011 loss: 0.467393 val_time: 0.001154 val_loss: 0.495783 acc_train: 0.796078 acc_val: 0.777778
epoch 47 train_time: 0.005069 loss: 0.465967 val_time: 0.000514 val_loss: 0.495563 acc_train: 0.796078 acc_val: 0.777778
epoch 48 train_time: 0.006578 loss: 0.465339 val_time: 0.001000 val_loss: 0.495331 acc_train: 0.796078 acc_val: 0.777778
epoch 49 train_time: 0.005827 loss: 0.463593 val_time: 0.001001 val_loss: 0.493717 acc_train: 0.796078 acc_val: 0.777778
epoch 50 train_time: 0.007028 loss: 0.462370 val_time: 0.001006 val_loss: 0.492275 acc_train: 0.800000 acc_val: 0.777778
epoch 51 train_time: 0.007019 loss: 0.461523 val_time: 0.000518 val_loss: 0.493138 acc_train: 0.796078 acc_val: 0.800000
epoch 52 train_time: 0.005528 loss: 0.460417 val_time: 0.001007 val_loss: 0.494454 acc_train: 0.800000 acc_val: 0.777778
epoch 53 train_time: 0.006044 loss: 0.459334 val_time: 0.001998 val_loss: 0.492033 acc_train: 0.796078 acc_val: 0.800000
epoch 54 train_time: 0.005609 loss: 0.457327 val_time: 0.001000 val_loss: 0.487873 acc_train: 0.803922 acc_val: 0.777778
epoch 55 train_time: 0.006538 loss: 0.456415 val_time: 0.001002 val_loss: 0.487303 acc_train: 0.803922 acc_val: 0.777778
epoch 56 train_time: 0.005528 loss: 0.455030 val_time: 0.000999 val_loss: 0.486743 acc_train: 0.803922 acc_val: 0.777778
epoch 57 train_time: 0.004508 loss: 0.454154 val_time: 0.001000 val_loss: 0.487643 acc_train: 0.803922 acc_val: 0.800000
epoch 58 train_time: 0.004515 loss: 0.453704 val_time: 0.000999 val_loss: 0.483372 acc_train: 0.807843 acc_val: 0.777778
epoch 59 train_time: 0.004568 loss: 0.451267 val_time: 0.001021 val_loss: 0.484756 acc_train: 0.807843 acc_val: 0.800000
epoch 60 train_time: 0.006513 loss: 0.451604 val_time: 0.000000 val_loss: 0.486329 acc_train: 0.803922 acc_val: 0.777778
epoch 61 train_time: 0.005353 loss: 0.449811 val_time: 0.001000 val_loss: 0.484585 acc_train: 0.803922 acc_val: 0.777778
epoch 62 train_time: 0.005524 loss: 0.449721 val_time: 0.000998 val_loss: 0.482820 acc_train: 0.807843 acc_val: 0.800000
epoch 63 train_time: 0.005516 loss: 0.447913 val_time: 0.001507 val_loss: 0.482491 acc_train: 0.803922 acc_val: 0.777778
epoch 64 train_time: 0.007041 loss: 0.448823 val_time: 0.001000 val_loss: 0.482010 acc_train: 0.803922 acc_val: 0.777778
epoch 65 train_time: 0.005012 loss: 0.446369 val_time: 0.001503 val_loss: 0.483129 acc_train: 0.800000 acc_val: 0.755556
epoch 66 train_time: 0.006522 loss: 0.445447 val_time: 0.002009 val_loss: 0.480999 acc_train: 0.800000 acc_val: 0.777778
epoch 67 train_time: 0.005015 loss: 0.445404 val_time: 0.001000 val_loss: 0.486650 acc_train: 0.796078 acc_val: 0.777778
epoch 68 train_time: 0.005522 loss: 0.444460 val_time: 0.001000 val_loss: 0.487049 acc_train: 0.796078 acc_val: 0.777778
epoch 69 train_time: 0.006112 loss: 0.442527 val_time: 0.001505 val_loss: 0.482234 acc_train: 0.800000 acc_val: 0.777778
epoch 70 train_time: 0.006519 loss: 0.442984 val_time: 0.001000 val_loss: 0.482030 acc_train: 0.800000 acc_val: 0.777778
epoch 71 train_time: 0.004506 loss: 0.440611 val_time: 0.001512 val_loss: 0.480820 acc_train: 0.803922 acc_val: 0.777778
epoch 72 train_time: 0.006124 loss: 0.440156 val_time: 0.000894 val_loss: 0.480842 acc_train: 0.800000 acc_val: 0.777778
epoch 73 train_time: 0.005010 loss: 0.438593 val_time: 0.001000 val_loss: 0.479417 acc_train: 0.803922 acc_val: 0.777778
epoch 74 train_time: 0.006529 loss: 0.438962 val_time: 0.001007 val_loss: 0.479688 acc_train: 0.803922 acc_val: 0.777778
epoch 75 train_time: 0.006538 loss: 0.437273 val_time: 0.000999 val_loss: 0.480948 acc_train: 0.800000 acc_val: 0.777778
epoch 76 train_time: 0.008034 loss: 0.436836 val_time: 0.000504 val_loss: 0.478903 acc_train: 0.803922 acc_val: 0.777778
epoch 77 train_time: 0.007015 loss: 0.436190 val_time: 0.000000 val_loss: 0.481736 acc_train: 0.796078 acc_val: 0.777778
epoch 78 train_time: 0.006032 loss: 0.436840 val_time: 0.001045 val_loss: 0.481888 acc_train: 0.796078 acc_val: 0.777778
epoch 79 train_time: 0.005521 loss: 0.433567 val_time: 0.001003 val_loss: 0.478438 acc_train: 0.803922 acc_val: 0.777778
epoch 80 train_time: 0.006090 loss: 0.433801 val_time: 0.001002 val_loss: 0.479732 acc_train: 0.800000 acc_val: 0.777778
epoch 81 train_time: 0.005529 loss: 0.433351 val_time: 0.001000 val_loss: 0.476616 acc_train: 0.800000 acc_val: 0.777778
epoch 82 train_time: 0.006518 loss: 0.432495 val_time: 0.001505 val_loss: 0.475177 acc_train: 0.800000 acc_val: 0.777778
epoch 83 train_time: 0.006561 loss: 0.431201 val_time: 0.001000 val_loss: 0.477437 acc_train: 0.800000 acc_val: 0.777778
epoch 84 train_time: 0.005056 loss: 0.430826 val_time: 0.001011 val_loss: 0.475507 acc_train: 0.800000 acc_val: 0.777778
epoch 85 train_time: 0.005032 loss: 0.431591 val_time: 0.001000 val_loss: 0.477952 acc_train: 0.800000 acc_val: 0.777778
epoch 86 train_time: 0.005018 loss: 0.428879 val_time: 0.001003 val_loss: 0.475768 acc_train: 0.800000 acc_val: 0.777778
epoch 87 train_time: 0.006157 loss: 0.429469 val_time: 0.000864 val_loss: 0.479029 acc_train: 0.796078 acc_val: 0.777778
epoch 88 train_time: 0.006541 loss: 0.427537 val_time: 0.002007 val_loss: 0.475638 acc_train: 0.807843 acc_val: 0.777778
epoch 89 train_time: 0.007041 loss: 0.426961 val_time: 0.001001 val_loss: 0.474318 acc_train: 0.807843 acc_val: 0.777778
epoch 90 train_time: 0.007595 loss: 0.425945 val_time: 0.000936 val_loss: 0.473740 acc_train: 0.807843 acc_val: 0.777778
epoch 91 train_time: 0.007540 loss: 0.426258 val_time: 0.001009 val_loss: 0.474088 acc_train: 0.807843 acc_val: 0.777778
epoch 92 train_time: 0.006417 loss: 0.425213 val_time: 0.001503 val_loss: 0.473682 acc_train: 0.807843 acc_val: 0.777778
epoch 93 train_time: 0.006039 loss: 0.424503 val_time: 0.001009 val_loss: 0.473505 acc_train: 0.807843 acc_val: 0.777778
epoch 94 train_time: 0.004512 loss: 0.425619 val_time: 0.001010 val_loss: 0.473489 acc_train: 0.807843 acc_val: 0.777778
epoch 95 train_time: 0.006037 loss: 0.423174 val_time: 0.001014 val_loss: 0.471925 acc_train: 0.807843 acc_val: 0.777778
epoch 96 train_time: 0.008095 loss: 0.423395 val_time: 0.000000 val_loss: 0.473106 acc_train: 0.803922 acc_val: 0.777778
epoch 97 train_time: 0.004768 loss: 0.422295 val_time: 0.003011 val_loss: 0.470060 acc_train: 0.807843 acc_val: 0.777778
epoch 98 train_time: 0.005531 loss: 0.421285 val_time: 0.001005 val_loss: 0.468067 acc_train: 0.803922 acc_val: 0.777778
epoch 99 train_time: 0.006522 loss: 0.422199 val_time: 0.000999 val_loss: 0.468618 acc_train: 0.807843 acc_val: 0.777778
epoch 100 train_time: 0.007038 loss: 0.420444 val_time: 0.001007 val_loss: 0.467997 acc_train: 0.803922 acc_val: 0.777778</code></pre>
</div>
</div>
</section>
</section>
<section id="pytorch-crash-course" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="pytorch-crash-course"><span class="header-section-number">6.4</span> Pytorch crash course</h2>
<section id="tensor" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="tensor"><span class="header-section-number">6.4.1</span> Tensor</h3>
<p>This is the basic data structure. It is very similar to <code>numpy.ndarray</code>, but with many more features. There are a few things that we need to mention at the beginning.</p>
<ol type="1">
<li>A tensor with only one item is mathematically equal to a number. In Pytorch, you may use <code>.item()</code> to extract the number from a tensor with only one item.</li>
</ol>
<div id="8377f470" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> torch</span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a>a <span class="op">=</span> torch.tensor([<span class="dv">1</span>])</span>
<span id="cb12-4"><a href="#cb12-4"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>tensor([1])</code></pre>
</div>
</div>
<div id="1ea3db61" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>a.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>1</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>It is type sensitive. Pytorch expect you to assign the exact data type to each tensor, and it won’t automatically guess it in most cases. You may specify data type when you create a tensor.</li>
</ol>
<div id="91cb594a" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>b <span class="op">=</span> torch.tensor([<span class="dv">1</span>], dtype<span class="op">=</span>torch.float64)</span>
<span id="cb16-2"><a href="#cb16-2"></a>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([1.], dtype=torch.float64)</code></pre>
</div>
</div>
<p>If you want to convert data type, you could use <code>.to()</code>.</p>
<div id="f0ec9d1f" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>b <span class="op">=</span> torch.tensor([<span class="dv">1</span>], dtype<span class="op">=</span>torch.float64)</span>
<span id="cb18-2"><a href="#cb18-2"></a>b <span class="op">=</span> b.to(torch.<span class="bu">int</span>)</span>
<span id="cb18-3"><a href="#cb18-3"></a>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([1], dtype=torch.int32)</code></pre>
</div>
</div>
<p>Tensor data structure has many other features that will be introduced later.</p>
</section>
<section id="gradient-descent-1" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="gradient-descent-1"><span class="header-section-number">6.4.2</span> Gradient descent</h3>
<p>To implement the gradient descent algorithm for the neural network, there would be a series of computations:</p>
<ol type="1">
<li>From the input, feedforward the network to get the output <code>y_pred</code>.</li>
<li>Based on the real output <code>y_true</code>, compute the loss function <code>loss = loss_fn(y_true, y_pred)</code>.</li>
<li>Compute the gradient based on the information provided. For this step many data are needed. You may look up the gradient descent formula (backprop).</li>
<li>Based on the gradient computed in Step 3, weights are updated, according to the optimizer we choose.</li>
</ol>
<p>In Pytorch, the above steps are implemented as follows.</p>
<ol type="1">
<li>You have to define a <code>model</code> function to indicate how to feedforward the network to get an output. Here for a lot of reasons, the typical way is to define a <code>model</code> class, which contains a <code>forward</code> method that can compute the output of the model. Let us consider the following example: the dataset is as follows:</li>
</ol>
<div id="76f90d8b" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>x <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">0</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb20-2"><a href="#cb20-2"></a>y <span class="op">=</span> torch.tensor([[<span class="dv">3</span>], [<span class="dv">7</span>], [<span class="dv">1</span>]], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The model is defined as follows.</p>
<div id="982b564d" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-2"><a href="#cb21-2"></a></span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="kw">class</span> MyModel(nn.Module):</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb21-5"><a href="#cb21-5"></a>        <span class="bu">super</span>(MyModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb21-6"><a href="#cb21-6"></a></span>
<span id="cb21-7"><a href="#cb21-7"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-8"><a href="#cb21-8"></a>    </span>
<span id="cb21-9"><a href="#cb21-9"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-10"><a href="#cb21-10"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb21-11"><a href="#cb21-11"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this example, we define a 2-input linear regression model. Pytorch doesn’t need the class to work. Actually the minimal working example of the above code is as follows. To put things into a class can make it easier in larger models.</p>
<div id="eab3b3ab" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="kw">def</span> model(x):</span>
<span id="cb22-2"><a href="#cb22-2"></a>    <span class="cf">return</span> nn.Linear(in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">1</span>)(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The reason the model can be written in a very simple way is because the information about computing gradients is recorded in the parameter tensors, on the level of tensors, instead of on the level of the model class. Therefore it is important to get access to the parameters of the model.</p>
<div id="d0daeed7" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>model <span class="op">=</span> MyModel()</span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>[Parameter containing:
 tensor([[-0.3573, -0.2246]], requires_grad=True),
 Parameter containing:
 tensor([-0.2558], requires_grad=True)]</code></pre>
</div>
</div>
<p>Note that the parameters we get here is a iterator. So to look at it we need to convert it inot a list. In this example, there are two sets of tensors: the first is the coefficients, and the second is the bias term. This bias term can be turned on/off by setting the argument <code>bias=True</code> or <code>False</code> when using <code>nn.Linear()</code> to create fully connected layers. The default is <code>True</code>.</p>
<p>To evaluate the model, we just directly apply the model to the input tensor.</p>
<div id="3c8d189e" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>y_pred <span class="op">=</span> model(x)</span>
<span id="cb25-2"><a href="#cb25-2"></a>y_pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor([[-1.0623],
        [-2.2262],
        [-0.4804]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>You may use the coefficients provided above to validate the resutl.</p>
<p>Note that, although we define the <code>.forward()</code> method, we don’t use it explicitly. The reason is that <code>model(x)</code> will not only excute <code>.forward(x)</code> method, but many other operations, like recording many intermediate results that can be used for debugging, visualization and modifying gradients.</p>
<ol start="2" type="1">
<li>We may define the loss function. We mannually define the MSE loss function.</li>
</ol>
<div id="0e1d2122" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="kw">def</span> loss_fn(y_true, y_pred):</span>
<span id="cb27-2"><a href="#cb27-2"></a>    <span class="cf">return</span> ((y_true<span class="op">-</span>y_pred)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb27-3"><a href="#cb27-3"></a></span>
<span id="cb27-4"><a href="#cb27-4"></a>loss <span class="op">=</span> loss_fn(y, y_pred)</span>
<span id="cb27-5"><a href="#cb27-5"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor(34.6054, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Now we need to do gradient descent. The manual way to <code>loss.backward()</code>. What it does is to</li>
</ol>
<div id="1af1c833" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="bu">print</span>(<span class="bu">list</span>(model.parameters()))</span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="bu">print</span>(<span class="bu">list</span>(model.parameters())[<span class="dv">0</span>].grad)</span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="bu">print</span>(<span class="bu">list</span>(model.parameters())[<span class="dv">1</span>].grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[Parameter containing:
tensor([[-0.3573, -0.2246]], requires_grad=True), Parameter containing:
tensor([-0.2558], requires_grad=True)]
None
None</code></pre>
</div>
</div>
<div id="95e8fa90" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb31-2"><a href="#cb31-2"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1e-2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bd2b8f69" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>loss.backward()</span>
<span id="cb32-2"><a href="#cb32-2"></a>optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c50b8836" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb33-2"><a href="#cb33-2"></a>    optimizer.zero_grad()</span>
<span id="cb33-3"><a href="#cb33-3"></a>    <span class="co"># print(optimizer.param_groups)</span></span>
<span id="cb33-4"><a href="#cb33-4"></a></span>
<span id="cb33-5"><a href="#cb33-5"></a>    y_pred <span class="op">=</span> model(x)</span>
<span id="cb33-6"><a href="#cb33-6"></a>    loss <span class="op">=</span> loss_fn(y_pred, y)</span>
<span id="cb33-7"><a href="#cb33-7"></a></span>
<span id="cb33-8"><a href="#cb33-8"></a>    <span class="co"># print(optimizer.param_groups)</span></span>
<span id="cb33-9"><a href="#cb33-9"></a></span>
<span id="cb33-10"><a href="#cb33-10"></a>    loss.backward()</span>
<span id="cb33-11"><a href="#cb33-11"></a>    optimizer.step()</span>
<span id="cb33-12"><a href="#cb33-12"></a></span>
<span id="cb33-13"><a href="#cb33-13"></a>    <span class="co"># print(optimizer.param_groups)</span></span>
<span id="cb33-14"><a href="#cb33-14"></a></span>
<span id="cb33-15"><a href="#cb33-15"></a>    <span class="co"># print(list(model.parameters()))</span></span>
<span id="cb33-16"><a href="#cb33-16"></a>    <span class="co"># print(list(model.parameters())[0].grad)</span></span>
<span id="cb33-17"><a href="#cb33-17"></a>    <span class="co"># print(list(model.parameters())[1].grad)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Update the parameters by <code>optim</code> or manually done.</li>
</ol>
</section>
<section id="mini-batch" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="mini-batch"><span class="header-section-number">6.4.3</span> Mini-batch</h3>
<!-- {{< include multi.qmd >}} -->
</section>
<section id="codes" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="codes"><span class="header-section-number">6.4.4</span> Codes</h3>
<p>We will only talk about using packages. <code>sklearn</code> provides two methods to implement the Logistic regression. The API interface is very similar to other models. Later we will use <code>PyTorch</code> and our</p>
<p>Note that Logistic regression is very sensitive to the scale of features. Therefore we need to normalize the features before throwing them into the model.</p>
<p>Let’s still take <code>iris</code> as an example.</p>
<div id="01b41729" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb34-3"><a href="#cb34-3"></a></span>
<span id="cb34-4"><a href="#cb34-4"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb34-5"><a href="#cb34-5"></a>X <span class="op">=</span> iris.data</span>
<span id="cb34-6"><a href="#cb34-6"></a>y <span class="op">=</span> iris.target</span>
<span id="cb34-7"><a href="#cb34-7"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first method is <code>sklearn.linear_model.LogisticRegression</code>.</p>
<div id="1e8b3aa6" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb35-3"><a href="#cb35-3"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb35-4"><a href="#cb35-4"></a></span>
<span id="cb35-5"><a href="#cb35-5"></a>steps <span class="op">=</span> [(<span class="st">'normalize'</span>, MinMaxScaler()),</span>
<span id="cb35-6"><a href="#cb35-6"></a>         (<span class="st">'log'</span>, LogisticRegression())]</span>
<span id="cb35-7"><a href="#cb35-7"></a></span>
<span id="cb35-8"><a href="#cb35-8"></a>log_reg <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb35-9"><a href="#cb35-9"></a>log_reg.fit(X_train, y_train)</span>
<span id="cb35-10"><a href="#cb35-10"></a>log_reg.score(X_test, y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>0.9565217391304348</code></pre>
</div>
</div>
<p>Note that this method has an option <code>solver</code> that will set the way to solve the Logistic regression problem, and there is no “stochastic gradient descent” provided. The default solver for this <code>LogsiticRegression</code> is <code>lbfgs</code> which will NOT be discussed in lectures.</p>
<p>The second method is <code>sklearn.linear_model.SGDClassifier</code>.</p>
<div id="dfd51551" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDClassifier</span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb37-4"><a href="#cb37-4"></a></span>
<span id="cb37-5"><a href="#cb37-5"></a>steps <span class="op">=</span> [(<span class="st">'normalize'</span>, MinMaxScaler()),</span>
<span id="cb37-6"><a href="#cb37-6"></a>         (<span class="st">'log'</span>, SGDClassifier(loss<span class="op">=</span><span class="st">'log_loss'</span>, max_iter<span class="op">=</span><span class="dv">100</span>))]</span>
<span id="cb37-7"><a href="#cb37-7"></a></span>
<span id="cb37-8"><a href="#cb37-8"></a>sgd_clf <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb37-9"><a href="#cb37-9"></a>sgd_clf.fit(X_train, y_train)</span>
<span id="cb37-10"><a href="#cb37-10"></a>sgd_clf.score(X_test, y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>0.9565217391304348</code></pre>
</div>
</div>
<p>This method is the one we discussed in lectures. The <code>log_loss</code> loss function is the binary entropy function we mentioned in lectures. If you change to other loss functions it will become other models.</p>
<p>From the above example, you may notice that <code>SGDClassifier</code> doesn’t perform as well as <code>LogisticRegression</code>. This is due to the algorithm. To make <code>SGDClassifier</code> better you need to tune the hyperparameters, like <code>max_iter</code>, <code>learning_rate</code>/<code>alpha</code>, <code>penalty</code>, etc..</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The argument <code>warm_start</code> is used to set whether you want to use your previous model. When set to <code>True</code>, it will reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. The default is <code>False</code>.</p>
<p>Repeatedly calling <code>fit</code> when <code>warm_start</code> is <code>True</code> can result in a different solution than when calling <code>fit</code> a single time because of the way the data is shuffled.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that for both methods, regularization (which will be discussed later) is applied by default.</p>
</div>
</div>
</section>
<section id="several-important-side-topics" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="several-important-side-topics"><span class="header-section-number">6.4.5</span> Several important side topics</h3>
<section id="epochs" class="level4" data-number="6.4.5.1">
<h4 data-number="6.4.5.1" class="anchored" data-anchor-id="epochs"><span class="header-section-number">6.4.5.1</span> Epochs</h4>
<p>We use epoch to describe feeding data into the model. One <em>Epoch</em> is when an entire dataset is passed through the model once. When using gradient descent, we tend to run several epochs. The number of maximal epochs is one important hyperparameter of this model.</p>
<p>The general idea is that more epochs are better for the score of the model, but it will definitely be slower. In addition, sometimes due to many other factors, after a few epochs, the model becomes stall. To train for more epochs cannot improve the model. In this case you have to turn to other methods.</p>
</section>
<section id="batch-gradient-descent-vs-sgd-vs-minibatch" class="level4" data-number="6.4.5.2">
<h4 data-number="6.4.5.2" class="anchored" data-anchor-id="batch-gradient-descent-vs-sgd-vs-minibatch"><span class="header-section-number">6.4.5.2</span> Batch Gradient Descent vs SGD vs Minibatch</h4>
<p>Recall the Formula <a href="#eq-nablaJ" class="quarto-xref">Equation&nbsp;<span>6.1</span></a>:</p>
<p><span class="math display">\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\]</span> We could rewrite this formula:</p>
<p><span class="math display">\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}=\frac1m\sum_{i=1}^m\left[(p^{(i)}-y^{(i)})\hat{x}^{(i)}\right].
\]</span> This new formula can be understood in the following way: For every data point, we could get one gradient direction. Then <span class="math inline">\(\nabla J\)</span> is the average of all gradient directions. So this algorithm can be expressed as that compute the gradient for every data points and then take the average, and finally update the parameters once. This algorithm is called <em>batch gradient descent</em>.</p>
<p>Following the idea, there is another way to update the model. For every data point, we could compute one gradient direction, and we could use the gradient direction to update the parameters of the model. This algorithm is called <em>stochastic gradient descent</em>.</p>
<p>Then there is an algrothm living in the middle, called <em>mini-batch gradient descent</em>. In this case, we will group the data set into a collection of subsets of a fiexed number of training examples. Each subset is called a <em>mini-batch</em>, and the fixed number of elements of each mini-batch is called the <em>batch size</em>. Using this method, we will just go through mini-batches one at a time, compute the average of the gradient for these data, and then update the parameters of the model after we finish one mini-batch. Assume that the total number of the dataset is <code>N</code>, the mini-batch size is <code>m</code>. Then there are <code>N/m</code> mini-batches, and during one epoch we will update the model <code>N/m</code> times.</p>
<p>Mini-batch size is one important hyperparameters of this model. Usually the larger the batch size is, the less variance the model has. Then it tends to behave more smoothly, but it will also be slower, and might be stuck to a local minimal. The smaller batch size is more chaotic. It might go faster, but it tends not to converge.</p>
</section>
</section>
</section>
<section id="exercises-and-projects" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="exercises-and-projects"><span class="header-section-number">6.5</span> Exercises and Projects</h2>
<div id="exr-" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.1</strong></span> Please hand write a report about the details of the math formulas for Logistic regression.</p>
</div>
<div id="exr-" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.2</strong></span> CHOOSE ONE: Please use <code>sklearn</code> to apply the LogisticRegression to one of the following datasets. You may either use <code>LogisticRegression</code> or <code>SGDClassifier</code>.</p>
<ul>
<li>the <code>iris</code> dataset.</li>
<li>the dating dataset.</li>
<li>the <code>titanic</code> dataset.</li>
</ul>
<p>Please in addition answer the following questions.</p>
<ol type="1">
<li>What is your accuracy score?</li>
<li>How many epochs do you use?</li>
<li>Plot the learning curve (accuracy vs training sizes).</li>
</ol>
</div>
<div id="exr-" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.3</strong></span> CHOOSE ONE: Please use <code>keras</code> to apply the LogisticRegression to one of the following datasets.</p>
<ul>
<li>the <code>iris</code> dataset.</li>
<li>the dating dataset.</li>
<li>the <code>titanic</code> dataset.</li>
</ul>
<p>Please in addition answer the following questions.</p>
<ol type="1">
<li>What is your accuracy score?</li>
<li>How many epochs do you use?</li>
<li>What is the batch size do you use?</li>
<li>Plot the learning curve (loss vs epochs, accuracy vs epochs).</li>
<li>Analyze the bias / variance status.</li>
</ol>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/intro2pt/intro.html" class="pagination-link" aria-label="Intro to Pytorch">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/6/intro.html" class="pagination-link" aria-label="Netural networks">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Netural networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>