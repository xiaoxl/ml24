[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Fall 2024",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 4803/5803 Machine Learning Fall 2024 at ATU. If you have any comments/suggetions/concers about the notes please contact us at xxiao@atu.edu.\n\n\nReferences\n\n\n[1] Godoy, D. V.\n(2022). Deep learning with\nPyTorch step-by-step: A beginner’s guide.\nhttps://leanpub.com/pytorch.\n\n\n[2] Chollet, F.\n(2021). Deep\nlearning with python, second edition. MANNING PUBN.\n\n\n[3] Géron, A.\n(2019). Hands-on machine learning with scikit-learn, keras, and\nTensorFlow concepts, tools, and techniques to build intelligent systems:\nConcepts, tools, and techniques to build intelligent systems.\nO’Reilly Media.\n\n\n[4] Harrington, P.\n(2012). Machine\nlearning in action. Manning Publications.\n\n\n[5] Klosterman, S.\n(2021). Data\nscience projects with python: A case study approach to gaining valuable\ninsights from real data with machine learning. Packt\nPublishing, Limited.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/1/intro.html",
    "href": "contents/1/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Machine Learning?\nMachine Learning is the science (and art) of programming computers so they can learn from data [1].\nHere is a slightly more general definition:\nThis “without being explicitly programmed to do so” is the essential difference between Machine Learning and usual computing tasks. The usual way to make a computer do useful work is to have a human programmer write down rules — a computer program — to be followed to turn input data into appropriate answers. Machine Learning turns this around: the machine looks at the input data and the expected task outcome, and figures out what the rules should be. A Machine Learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task [2].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#what-is-machine-learning",
    "href": "contents/1/intro.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "",
    "text": "[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n                                                   -- Arthur Samuel, 1959",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#types-of-machine-learning-systems",
    "href": "contents/1/intro.html#types-of-machine-learning-systems",
    "title": "1  Introduction",
    "section": "1.2 Types of Machine Learning Systems",
    "text": "1.2 Types of Machine Learning Systems\nThere are many different types of Machine Learning systems that it is useful to classify them in braod categories, based on different criteria. These criteria are not exclusive, and you can combine them in any way you like.\nThe most popular criterion for Machine Learning classification is the amount and type of supervision they get during training. In this case there are four major types.\nSupervised Learning The training set you feed to the algorithm includes the desired solutions. The machines learn from the data to alter the model to get the desired output. The main task for Supervised Learning is classification and regression.\nUnsupervised Learning In Unsupervised Learning, the data provided doesn’t have class information or desired solutions. We just want to dig some information directly from those data themselves. Usually Unsupervised Learning is used for clustering and dimension reduction.\nReinforcement Learning In Reinforcement Learning, there is a reward system to measure how well the machine performs the task, and the machine is learning to find the strategy to maximize the rewards. Typical examples here include gaming AI and walking robots.\nSemisupervised Learning This is actually a combination of Supervised Learning and Unsupervised Learning, that it is usually used to deal with data that are half labelled.\n\n1.2.1 Tasks for Supervised Learning\nAs mentioned above, for Supervised Learning, there are two typical types of tasks:\nClassification It is the task of predicting a discrete class labels. A typical classification problem is to see an handwritten digit image and recognize it.\nRegression It is the task of predicting a continuous quantity. A typical regression problem is to predict the house price based on various features of the house.\nThere are a lot of other tasks that are not directly covered by these two, but these two are the most classical Supervised Learning tasks.\n\n\n\n\n\n\nNote\n\n\n\nIn this course we will mainly focus on Supervised Classification problems.\n\n\n\n\n1.2.2 Classification based on complexity\nAlong with the popularity boost of deep neural network, there comes another classificaiton: shallow learning vs. deep learning. Basically all but deep neural network belongs to shallow learning. Although deep learning can do a lot of fancy stuffs, shallow learning is still very good in many cases. When the performance of a shallow learning model is good enough comparing to that of a deep learning model, people tend to use the shallow learning since it is usually faster, easier to understand and easier to modify.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "href": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "title": "1  Introduction",
    "section": "1.3 Basic setting for Machine learning problems",
    "text": "1.3 Basic setting for Machine learning problems\n\n\n\n\n\n\nNote\n\n\n\nWe by default assume that we are dealing with a Supervised Classification problem.\n\n\n\n1.3.1 Input and output data structure\nSince we are dealing with Supervised Classification problems, the desired solutions are given. These desired solutions in Classification problems are also called labels. The properties that the data are used to describe are called features. Both features and labels are usually organized as row vectors.\n\nExample 1.1 The example is extracted from [3]. There are some sample data shown in the following table. We would like to use these information to classify bird species.\n\n\n\n\nTable 1.1: Bird species classification based on four features\n\n\n\n\n\n\n\n\nWeight (g)\nWingspan (cm)\nWebbed feet?\nBack color\nSpecies\n\n\n\n\n1000.100000\n125.000000\nNo\nBrown\nButeo jamaicensis\n\n\n3000.700000\n200.000000\nNo\nGray\nSagittarius serpentarius\n\n\n3300.000000\n220.300000\nNo\nGray\nSagittarius serpentarius\n\n\n4100.000000\n136.000000\nYes\nBlack\nGavia immer\n\n\n3.000000\n11.000000\nNo\nGreen\nCalothorax lucifer\n\n\n570.000000\n75.000000\nNo\nBlack\nCampephilus principalis\n\n\n\n\n\n\n\n\nThe first four columns are features, and the last column is the label. The first two features are numeric and can take on decimal values. The third feature is binary that can only be \\(1\\) (Yes) or \\(0\\) (No). The fourth feature is an enumeration over the color palette. You may either treat it as categorical data or numeric data, depending on how you want to build the model and what you want to get out of the data. In this example we will use it as categorical data that we only choose it from a list of colors (\\(1\\) — Brown, \\(2\\) — Gray, \\(3\\) — Black, \\(4\\) — Green).\nThen we are able to transform the above data into the following form:\n\n\n\nTable 1.2: Vectorized Bird species data\n\n\n\n\n\nFeatures\nLabels\n\n\n\n\n\\(\\begin{bmatrix}1001.1 & 125.0 & 0 & 1 \\end{bmatrix}\\)\n\\(1\\)\n\n\n\\(\\begin{bmatrix}3000.7 & 200.0 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}3300.0 & 220.3 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}4100.0 & 136.0 & 1 & 3 \\end{bmatrix}\\)\n\\(3\\)\n\n\n\\(\\begin{bmatrix}3.0 & 11.0 & 0 & 4 \\end{bmatrix}\\)\n\\(4\\)\n\n\n\\(\\begin{bmatrix}570.0 & 75.0 & 0 & 3 \\end{bmatrix}\\)\n\\(5\\)\n\n\n\n\n\n\nThen the Supervised Learning problem is stated as follows: Given the features and the labels, we would like to find a model that can classify future data.\n\n\n\n1.3.2 Parameters and hyperparameters\nA model parameter is internal to the model and its value is learned from the data.\nA model hyperparameter is external to the model and its value is set by people.\nFor example, assume that we would like to use Logistic regression to fit the data. We set the learning rate is 0.1 and the maximal iteration is 100. After the computations are done, we get a the model\n\\[\ny = \\sigma(0.8+0.7x).\n\\] The two cofficients \\(0.8\\) and \\(0.7\\) are the parameters of the model. The model Logistic regression, the learning rate 0.1 and the maximal iteration 100 are all hyperparametrs. If we change to a different set of hyperparameters, we may get a different model, with a different set of parameters.\nThe details of Logistic regression will be discussed later.\n\n\n1.3.3 Evaluate a Machine Learning model\nOnce the model is built, how do we know that it is good or not? The naive idea is to test the model on some brand new data and check whether it is able to get the desired results. The usual way to achieve it is to split the input dataset into three pieces: training set, validation set and test set.\nThe model is initially fit on the training set, with some arbitrary selections of hyperparameters. Then hyperparameters will be changed, and new model is fitted over the training set. Which set of hyperparameters is better? We then test their performance over the validation set. We could run through a lot of different combinations of hyperparameters, and find the best performance over the validation set. After we get the best hyperparameters, the model is selcted, and we fit it over the training set to get our model to use.\nTo compare our model with our models, either our own model using other algorithms, or models built by others, we need some new data. We can no longer use the training set and the validation set since all data in them are used, either for training or for hyperparameters tuning. We need to use the test set to evaluate the “real performance” of our data.\nTo summarize:\n\nTraining set: used to fit the model;\nValidation set: used to tune the hyperparameters;\nTest set: used to check the overall performance of the model.\n\nThe validation set is not always required. If we use cross-validation technique for hyperparameters tuning, like sklearn.model_selection.GridSearchCV(), we don’t need a separated validation set. In this case, we will only need the training set and the test set, and run GridSearchCV over the training set. The cross-validation will be discussed in {numref}Section %s&lt;section-cross-validation&gt;.\nThe sizes and strategies for dataset division depends on the problem and data available. It is often recommanded that more training data should be used. The typical distribution of training, validation and test is \\((6:3:1)\\), \\((7:2:1)\\) or \\((8:1:1)\\). Sometimes validation set is discarded and only training set and test set are used. In this case the distribution of training and test set is usually \\((7:3)\\), \\((8:2)\\) or \\((9:1)\\).\n\n\n1.3.4 Workflow in developing a machine learning application\nThe workflow described below is from [3].\n\nCollect data.\nPrepare the input data.\nAnalyze the input data.\nTrain the algorithm.\nTest the algorithm.\nUse it.\n\nIn this course, we will mainly focus on Step 4 as well Step 5. These two steps are where the “core” algorithms lie, depending on the algorithm. We will start from the next Chapter to talk about various Machine Learning algorithms and examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#python-quick-guide",
    "href": "contents/1/intro.html#python-quick-guide",
    "title": "1  Introduction",
    "section": "1.4 Python quick guide",
    "text": "1.4 Python quick guide\n\n1.4.1 Python Notebook\nWe mainly use Python Notebook (.ipynb) to write documents for this course. Currently all main stream Python IDE support Python Notebook. All of them are not entirely identical but the differences are not huge and you may choose any you like.\nOne of the easiest ways to use Python Notebook is through JupyterLab. The best part about it is that you don’t need to worry about installation and configuration in the first place, and you can directly start to code.\nClick the above link and choose JupyterLab. Then you will see the following page.\n\nThe webapp you just started is called JupyterLite. This is a demo version. The full JupyterLab installation instruction can also be found from the link.\nThere is a small button + under the tab bar. This is the place where you click to start a new cell. You may type codes or markdown documents or raw texts in the cell according to your needs. The drag-down menu at the end of the row which is named Code or Markdown or Raw can help you make the switch. Markdown is a very simple light wighted language to write documents. In most cases it behaves very similar to plain texts. Codes are just regular Python codes (while some other languages are supported). You may either use the triangle button in the menu to execute the codes, or hit shift + enter.\n\nJupyterLite contains a few popular packages. Therefore it is totally ok if you would like to play with some simple things. However since it is an online evironment, it has many limitations. Therefore it is still recommended to set up a local environment once you get familiar with Python Notebook. Please check the following links for some popular choices for notebooks and Python installations in general, either local and online.\n\nJupyter Notebook / JupyterLab\nVS Code\nPyCharm\nGoogle Colab\nAnaconda\n\n\n\n1.4.2 Python fundamentals\nWe will put some very basic Python commands here for you to warm up. More advanced Python knowledge will be covered during the rest of the semester. The main reference for this part is [3]. Another referenece is My notes.\n\n1.4.2.1 Indentation\nPython is using indentation to denote code blocks. It is not convienent to write in the first place, but it forces you to write clean, readable code.\nBy the way, the if and for block are actually straightforward.\nif jj &lt; 3:\n    jj = jj \n    print(\"It is smaller than 3.\")\nif jj &lt; 3:\n    jj = jj\nprint(\"It is smaller than 3.\")\nfor i in range(3):\n    i = i + 1\n    print(i)\nfor i in range(3):\n    i = i + 1\nprint(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease tell the differences between the above codes.\n\n\n1.4.2.2 list and dict\nHere are some very basic usage of lists of dictionaries in Python.\n\nnewlist = list()\nnewlist.append(1)\nnewlist.append('hello')\nnewlist\n\n[1, 'hello']\n\n\n\nnewlisttwo = [1, 'hello']\nnewlisttwo\n\n[1, 'hello']\n\n\n\nnewdict = dict()\nnewdict['one'] = 'good'\nnewdict[1] = 'yes'\nnewdict\n\n{'one': 'good', 1: 'yes'}\n\n\n\nnewdicttwo = {'one': 'good', 1: 'yes'}\nnewdicttwo\n\n{'one': 'good', 1: 'yes'}\n\n\n\n\n1.4.2.3 Loop through lists\nWhen creating for loops we may let Python directly loop through lists. Here is an example. The code is almost self-explained.\n\nalist = ['one', 2, 'three', 4]\n\nfor item in alist:\n    print(item)\n\none\n2\nthree\n4\n\n\n\n\n1.4.2.4 Reading files\nThere are a lot of functions that can read files. The basic one is to read any files as a big string. After we get the string, we may parse it based on the structure of the data.\nThe above process sounds complicated. That’s why we have so many different functions reading files. Usually they focus on a certain types of files (e.g. spreadsheets, images, etc..), parse the data into a particular data structure for us to use later.\nI will mention a few examples.\n\ncsv files and excel files Both of them are spreadsheets format. Usually we use pandas.read_csv and pandas.read_excel both of which are from the package pandas to read these two types of files.\nimages Images can be treated as matrices, that each entry represents one pixel. If the image is black/white, it is represented by one matrix where each entry represents the gray value. If the image is colored, it is represented by three matrices where each entry represents one color. To use which three colors depends on the color map. rgb is a popular choice.\nIn this course when we need to read images, we usually use matplotlib.pyplot.imread from the package matplotlib or cv.imread from the package opencv.\n.json files .json is a file format to store dictionary type of data. To read a json file and parse it as a dictionary, we need json.load from the package json.\n\n\n\n1.4.2.5 Writing files\n\npandas.DataFrame.to_csv\npandas.DataFrame.to_excel\nmatplotlib.pyplot.imsave\ncv.imwrite\njson.dump\n\n\n\n1.4.2.6 Relative paths\nIn this course, when reading and writing files, please keep all the files using relative paths. That is, only write the path starting from the working directory.\n\nExample 1.2 Consider the following tasks:\n\nYour working directory is C:/Users/Xinli/projects/.\nWant to read a file D:/Files/example.csv.\nWant to generate a file whose name is result.csv and put it in a subfoler named foldername.\n\nTo do the tasks, don’t directly run the code pd.read_csv('D:/Files/example.csv'). Instead you should first copy the file to your working directory C:/Users/Xinli/projects/, and then run the following code.\n\nimport pandas as pd\n\ndf = pd.read_csv('example.csv')\ndf.to_csv('foldername/result.csv')\n\nPlease pay attention to how the paths are written.\n\n\n\n1.4.2.7 .\n\nclass and packages.\nGet access to attributes and methods\nChaining dots.\n\n\n\n\n1.4.3 Some additional topics\nYou may read about these parts from the appendices of My notes.\n\n1.4.3.1 Package management and Virtual environment\n\nconda\n\nconda create\n\nconda create --name myenv\nconda create --name myenv python=3.9\nconda create --name myenv --file spec-file.txt\n\nconda install\n\nconda install -c conda-forge numpy\n\nconda activate myenv\nconda list\n\nconda list numpy\nconda list --explicit &gt; spec-file.txt\n\nconda env list\n\npip / venv\n\npython -m venv newenv\nnewenv\\Scripts\\activate\npip install\npip freeze &gt; requirements.txt\npip install -r /path/to/requirements.txt\ndeactivate\n\n\n\n\n1.4.3.2 Version Control\n\nGit\n\nInstall\ngit config --list\ngit config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n\nGitHub",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#exercises",
    "href": "contents/1/intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nThese exercises are from [4], [1] and [3].\n\n1.5.1 Python Notebook\n\nExercise 1.1 (Hello World!) Please set up a Python Notebook environment and type print('Hello World!').\n\n\nExercise 1.2 Please set up a Python Notebook and start a new virtual environment and type print('Hello World!').\n\n\n\n1.5.2 Basic Python\n\nExercise 1.3 (Play with lists) Please complete the following tasks.\n\nWrite a for loop to print values from 0 to 4.\nCombine two lists ['apple', 'orange'] and ['banana'] using +.\nSort the list ['apple', 'orange', 'banana'] using sorted().\n\n\n\n\nExercise 1.4 (Play with list, dict and pandas.) Please complete the following tasks.\n\nCreate a new dictionary people with two keys name and age. The values are all empty list.\nAdd Tony to the name list in people.\nAdd Harry to the name list in people.\nAdd number 100 to the age list in people.\nAdd number 10 to the age list in people.\nFind all the keys of people and save them into a list namelist.\nConvert the dictionary people to a Pandas DataFrame df.\n\n\n\n\nExercise 1.5 (The dataset iris)  \n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nPlease explore this dataset.\n\nPlease get the features for iris and save it into X as an numpy array.\nWhat is the meaning of these features?\nPlease get the labels for iris and save it into y as an numpy array.\nWhat is the meaning of labels?\n\n\n\n\nExercise 1.6 (Play with Pandas) Please download the Titanic data file from here. Then follow the instructions to perform the required tasks.\n\nUse pandas.read_csv to read the dataset and save it as a dataframe object df.\nChange the values of the Sex column that male is 0 and female is 1.\nPick the columns Pclass, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare and transform them into a 2-dimensional numpy.ndarray, and save it as X.\nPick the column Survived and transform it into a 1-dimensional numpy.ndarray and save it as y.\n\n\n\n\n\n\n\n[1] Géron, A. (2019). Hands-on machine learning with scikit-learn, keras, and TensorFlow concepts, tools, and techniques to build intelligent systems: Concepts, tools, and techniques to build intelligent systems. O’Reilly Media.\n\n\n[2] Chollet, F. (2021). Deep learning with python, second edition. MANNING PUBN.\n\n\n[3] Harrington, P. (2012). Machine learning in action. Manning Publications.\n\n\n[4] Klosterman, S. (2021). Data science projects with python: A case study approach to gaining valuable insights from real data with machine learning. Packt Publishing, Limited.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html",
    "href": "contents/2/intro.html",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1 k-Nearest Neighbors Algorithm (k-NN)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "href": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1.1 Ideas\nAssume that we have a set of labeled data \\(\\{(X_i, y_i)\\}\\) where \\(y_i\\) denotes the label. Given a new data \\(X\\), how do we determine the label of it?\nk-NN algorithm starts from a very straightforward idea. We use the distances from the new data point \\(X\\) to the known data points to identify the label. If \\(X\\) is closer to \\(y_i\\) points, then we will label \\(X\\) as \\(y_i\\).\nLet us take cities and countries as an example. New York and Los Angeles are U.S cities, and Beijing and Shanghai are Chinese cities. Now we would like to consider Tianjin and Russellville. Do they belong to China or U.S? We calculate the distances from Tianjin (resp. Russellville) to all four known cities. Since Tianjin is closer to Beijing and Shanghai comparing to New York and Los Angeles, we classify Tianjin as a Chinese city. Similarly, since Russellville is closer to New York and Los Angeles comparing to Beijing and Shanghai, we classify it as a U.S. city.\n\n\n\n\n\n\n\nG\n\n\n\nBeijing\n\nBeijing\n\n\n\nShanghai\n\nShanghai\n\n\n\nTianjin\n\nTianjin\n\n\n\nTianjin-&gt;Beijing\n\n\ncloser\n\n\n\nTianjin-&gt;Shanghai\n\n\ncloser   \n\n\n\nNew York\n\nNew York\n\n\n\nTianjin-&gt;New York\n\n\n far away\n\n\n\nLos Angelis\n\nLos Angelis\n\n\n\nTianjin-&gt;Los Angelis\n\n\n far away\n\n\n\nRussellville\n\nRussellville\n\n\n\nRussellville-&gt;Beijing\n\n\nfar away \n\n\n\nRussellville-&gt;Shanghai\n\n\nfar away\n\n\n\nRussellville-&gt;New York\n\n\ncloser  \n\n\n\nRussellville-&gt;Los Angelis\n\n\ncloser\n\n\n\n\n\n\n\n\nThis naive example explains the idea of k-NN. Here is a more detailed description of the algorithm.\n\n\n2.1.2 The Algorithm\n\n\n\n\n\n\nk-NN Classifier\n\n\n\nInputs: Given the training data set \\(\\{(X_i, y_i)\\}\\) where \\(X_i=(x_i^1,x_i^2,\\ldots,x_i^n)\\) represents \\(n\\) features and \\(y_i\\) represents labels. Given a new data point \\(\\tilde{X}=(\\tilde{x}^1,\\tilde{x}^2,\\ldots,\\tilde{x}^n)\\).\nOutputs: Want to find the best label for \\(\\tilde{X}\\).\n\nCompute the distance from \\(\\tilde{X}\\) to each \\(X_i\\).\nSort all these distances from the nearest to the furthest.\nFind the nearest \\(k\\) data points.\nDetermine the labels for each of these \\(k\\) nearest points, and compute the frenqucy of each labels.\nThe most frequent label is considered to be the label of \\(\\tilde{X}\\).\n\n\n\n\n\n2.1.3 Details\n\nThe distance between two data points are defined by the Euclidean distance:\n\n\\[\ndist\\left((x^j_i)_{j=1}^n, (\\tilde{x}^j)_{j=1}^n\\right)=\\sqrt{\\sum_{j=1}^n(x^j_i-\\tilde{x}^j)^2}.\n\\]\n\nUsing linear algebra notations:\n\n\\[\ndist(X_i,\\tilde{X})=\\sqrt{(X_i-\\tilde{X})\\cdot(X_i-\\tilde{X})}.\n\\]\n\nAll the distances are stored in a \\(1\\)-dim numpy array, and we will combine it together with another \\(1\\)-dim array that store the labels of each point.\n\n\n\n2.1.4 The codes\n\nargsort\nunique\nargmax\n\n\nimport numpy as np\n\ndef classify_kNN(inX, X, y, k=5):\n    # compute the distance between each row of X and Xmat\n    Dmat = np.sqrt(((inX - X)**2).sum(axis=1))\n    # sort by distance\n    k = min(k, Dmat.shape[0])\n    argsorted = Dmat.argsort()[:k]\n    relatedy = y[argsorted]\n    # count the freq. of the first k labels\n    labelcounts = np.unique(relatedy, return_counts=True)\n    # find the label with the most counts\n    label = labelcounts[0][labelcounts[1].argmax()]\n    return label\n\n\n\n2.1.5 sklearn packages\nYou may also directly use the kNN function KNeighborsClassifier packaged in sklearn.neighbors. You may check the description of the function online from here.\nThere are many ways to modify the kNN algorithm. What we just mentioned is the simplest idea. It is correspondent to the argument weights='uniform', algorithm='brute and metric='euclidean'. However due to the implementation details, the results we got from sklearn are still a little bit different from the results produced by our naive codes.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=10, weights='uniform',\n                           algorithm='brute', metric='euclidean')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\n2.1.6 Normalization\nDifferent features may have different scales. It might be unfair for those features that have small scales. Therefore usually it is better to rescale all the features to make them have similar scales. After examining all the data, we find the minimal value minVal and the range ranges for each column. The normalization formula is:\n\\[\nX_{norm} = \\frac{X_{original}-minVal}{ranges}.\n\\]\nWe could also convert the normalized number back to the original value by\n\\[\nX_{original} = X_{norm} \\times ranges + minVal.\n\\]\nThe sample codes are listed below.\n\nimport numpy as np\n\ndef encodeNorm(X, parameters=None):\n    # parameters contains minVals and ranges\n    if parameters is None:\n        minVals = np.min(X, axis=0)\n        maxVals = np.max(X, axis=0)\n        ranges = np.maximum(maxVals - minVals, np.ones(minVals.size))\n        parameters = {'ranges': ranges, 'minVals': minVals}\n    else:\n        minVals = parameters['minVals']\n        ranges = parameters['ranges']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xnorm = (X - Nmat)/ranges\n    return (Xnorm, parameters)\n\n\ndef decodeNorm(X, parameters):\n    # parameters contains minVals and ranges\n    ranges = parameters['ranges']\n    minVals = parameters['minVals']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xoriginal = X * ranges + Nmat\n    return Xoriginal\n\nIf you use sklearn you could use MinMaxScaler from sklearn.preprocessing to achive the same goal. The related codes will be discussed later in projects. I keep our handwritten codes here for Python practicing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "href": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.2 k-NN Project 1: iris Classification",
    "text": "2.2 k-NN Project 1: iris Classification\nThis data is from sklearn.datasets. This dataset consists of 3 different types of irises’ petal / sepal length / width, stored in a \\(150\\times4\\) numpy.ndarray. We already explored the dataset briefly in the previous chapter. This time we will try to use the feature provided to predict the type of the irises. For the purpose of plotting, we will only use the first two features: sepal length and sepal width.\n\n2.2.1 Explore the dataset\nWe first load the dataset.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nThen we would like to split the dataset into trainning data and test data. Here we are going to use sklearn.model_selection.train_test_split function. Besides the dataset, we should also provide the propotion of the test set comparing to the whole dataset. We will choose test_size=0.1 here, which means that the size of the test set is 0.1 times the size of the whole dataset. stratify=y means that when split the dataset we want to split respects the distribution of labels in y.\nThe split will be randomly. You may set the argument random_state to be a certain number to control the random process. If you set a random_state, the result of the random process will stay the same. This is for reproducible output across multiple function calls.\nAfter we get the training set, we should also normalize it. All our normalization should be based on the training set. When we want to use our model on some new data points, we will use the same normalization parameters to normalize the data points in interests right before we apply the model. Here since we mainly care about the test set, we could normalize the test set at this stage.\nNote that in the following code, the function encodeNorm defined in the previous section is used. \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\nBefore we start to play with k-NN, let us look at the data first. Since we only choose two features, it is able to plot these data points on a 2D plane, with different colors representing different classes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the scatter plot.\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n# Generate legends.\nlabels = ['setosa', 'versicolor', 'virginica']\n_ = fig.legend(handles=scatter.legend_elements()[0], labels=labels,\n               loc=\"right\", title=\"Labels\")\n\n\n\n\n\n\n\n\n\n\n2.2.2 Apply our k-NN model\nNow let us apply k-NN to this dataset. We first use our codes. The poential code is\n\ny_pred = classify_kNN(X_test, X_train, y_train, k=10)\n\nHowever the above code is actually wrong. The issue is that our function classify_kNN can only classify one row of data. To classify many rows, we need to use a for loop.\n\nn_neighbors = 10\ny_pred = list()\nfor row in X_test_norm:\n    row_pred = classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n    y_pred.append(row_pred)\ny_pred = np.array(y_pred)\n\nWe could use list comprehension to simply the above codes.\n\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nThis y_pred is the result we got for the test set. We may compare it with the real answer y_test, and calcuate the accuracy.\n\nacc = np.mean(y_pred == y_test)\nacc\n\n0.7333333333333333\n\n\n\n\n2.2.3 Apply k-NN model from sklearn\nNow we would like to use sklearn to reproduce this result. Since our data is prepared, what we need to do is directly call the functions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 10\nclf = KNeighborsClassifier(n_neighbors, weights=\"uniform\", metric=\"euclidean\",\n                           algorithm='brute')\nclf.fit(X_train_norm, y_train)\ny_pred_sk = clf.predict(X_test_norm)\n\nacc = np.mean(y_pred_sk == y_test)\nacc\n\n0.7333333333333333\n\n\n\n\n2.2.4 Using data pipeline\nWe may organize the above process in a neater way. After we get a data, the usual process is to apply several transforms to the data before we really get to the model part. Using terminolgies from sklearn, the former are called transforms, and the latter is called an estimator. In this example, we have exactly one tranform which is the normalization. The estimator here we use is the k-NN classifier.\nsklearn provides a standard way to write these codes, which is called pipeline. We may chain the transforms and estimators in a sequence and let the data go through the pipeline. In this example, the pipeline contains two steps: 1. The normalization transform sklearn.preprocessing.MinMaxScaler. When we directly apply it the parameters ranges and minVals and will be recorded automatically, and we don’t need to worry about it when we want to use the same parameters to normalize other data. 2. The k-NN classifier sklearn.neighbors.KNeighborsClassifier. This is the same one as we use previously.\nThe code is as follows. It is a straightforward code. Note that the () after the class in each step of steps is very important. The codes cannot run if you miss it.\nAfter we setup the pipeline, we may use it as other estimators since it is an estimator. Here we may also use the accuracy function provided by sklearn to perform the computation. It is essentially the same as our acc computation.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.7333333333333333\n\n\n\n\n2.2.5 Visualize the Decision boundary [Optional]\nUsing the classifier we get above, we are able to classify every points on the plane. This enables us to draw the following plot, which is called the Decision boundary. It helps us to visualize the relations between features and the classes.\nWe use DecisionBoundaryDisplay from sklearn.inspection to plot the decision boundary. The function requires us to have a fitted classifier. We may use the classifier pipe we got above. Note that this classifier should have some build-in structures that our classify_kNN function doesn’t have. We may rewrite our codes to make it work, but this goes out of the scope of this section. This is supposed to be Python programming exercise. We will talk about it in the future if we have enough time.\nWe first plot the dicision boundary using DecisionBoundaryDisplay.from_estimator. Then we plot the points from X_test. From the plot it is very clear which points are misclassified.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n            pipe, \n            X_train,\n            response_method=\"predict\",\n            plot_method=\"pcolormesh\",\n            xlabel=iris.feature_names[0],\n            ylabel=iris.feature_names[1],\n            alpha=0.5)\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\")\ndisp.figure_.set_size_inches((10,7))\n\n\n\n\n\n\n\n\n\n\n2.2.6 k-Fold Cross-Validation\nPreviously we perform a random split and test our model in this case. What would happen if we fit our model on another split? We might get a different accuracy score. So in order to evaluate the performance of our model, it is natual to consider several different split and compute the accuracy socre for each case, and combine all these socres together to generate an index to indicate whehter our model is good or bad. This naive idea is called k-Fold Cross-Validation.\nThe algorithm is described as follows. We first randomly split the dataset into k groups. We use one of them as the test set, and the rest together forming the training set, and use this setting to get an accuracy score. We did this for each group to be chosen as the test set. Then the final score is the mean.\nsklearn provides a function sklearn.model_selection.cross_val_score to perform the above computation. The usage is straightforward, as follows.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(pipe, X, y, cv=5)\ncv_scores\n\narray([0.66666667, 0.8       , 0.63333333, 0.8       , 0.7       ])\n\n\n\nnp.mean(cv_scores)\n\n0.7200000000000001\n\n\n\n\n2.2.7 Choosing a k value\nIn the previous example we choose k to be 10 as an example. To choose a k value we usually run some test by trying different k and choose the one with the best performance. In this case, best performance means the highest cross-validation score.\nsklearn.model_selection.GridSearchCV provides a way to do this directly. We only need to setup the esitimator, the metric (which is the cross-validation score in this case), and the hyperparameters to be searched through, and GridSearchCV will run the search automatically.\nWe let k go from 1 to 100. The code is as follows.\nNote that parameters is where we set the search space. It is a dictionary. The key is the name of the estimator plus double _ and then plus the name of the parameter.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n35\n\n\nAfter we fit the data, the best_estimator_.get_params() can be printed. It tells us that it is best to use 31 neibhours for our model. We can directly use the best estimator by calling clf.best_estimator_.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nnp.mean(cv_scores)\n\n0.82\n\n\nThe cross-validation score using k=31 is calculated. This serves as a benchmark score and we may come back to dataset using other methods and compare the scores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "href": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.3 k-NN Project 2: Dating Classification",
    "text": "2.3 k-NN Project 2: Dating Classification\nThe data can be downloaded from here.\n\n2.3.1 Background\nHelen dated several people and rated them using a three-point scale: 3 is best and 1 is worst. She also collected data from all her dates and recorded them in the file attached. These data contains 3 features:\n\nNumber of frequent flyer miles earned per year\nPercentage of time spent playing video games\nLiters of ice cream consumed per week\n\nWe would like to predict her ratings of new dates when we are given the three features.\nThe data contains four columns, while the first column refers to Mileage, the second Gamingtime, the third Icecream and the fourth Rating.\n\n\n2.3.2 Look at Data\nWe first load the data and store it into a DataFrame.\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('datingTestSet2.txt', sep='\\t', header=None)\ndf.head()\n\nTo make it easier to read, we would like to change the name of the columns.\n\ndf = df.rename(columns={0: \"Mileage\", 1: \"Gamingtime\", 2: 'Icecream', 3: 'Rating'})\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nGamingtime\nIcecream\nRating\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\nSince now we have more than 2 features, it is not suitable to directly draw scatter plots. We use seaborn.pairplot to look at the pairplot. From the below plots, before we apply any tricks, it seems that Milegae and Gamingtime are better than Icecream to classify the data points.\n\nimport seaborn as sns\nsns.pairplot(data=df, hue='Rating')\n\n\n\n\n\n\n\n\n\n\n2.3.3 Applying kNN\nSimilar to the previous example, we will apply both methods for comparisons.\n\nfrom sklearn.model_selection import train_test_split\nX = np.array(df[['Mileage', 'Gamingtime', 'Icecream']])\ny = np.array(df['Rating'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\n\nUsing our codes.\n\n\n# Using our codes.\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nacc = np.mean(y_pred == y_test)\nacc\n\n0.93\n\n\n\nUsing sklearn.\n\n\n# Using sklearn.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.93\n\n\n\n\n2.3.4 Choosing k Value\nSimilar to the previous section, we can run tests on k value to choose one to be used in our model using GridSearchCV.\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n4\n\n\nFrom this result, in this case the best k is 4. The corresponding cross-validation score is computed below.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nnp.mean(cv_scores)\n\n0.952",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "href": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.4 k-NN Project 3: Handwritten recognition",
    "text": "2.4 k-NN Project 3: Handwritten recognition\nWe would like to let the machine recognize handwritten digits. The dataset comes from the UCI dataset repository. Now we apply kNN algrotithm to it.\n\n2.4.1 Dataset description\nEvery digit is stored as a \\(8\\times8\\) picture. This is a \\(8\\times8\\) matrix. Every entry represents a gray value of the corresponding pixel, whose value is from 0 to 16. The label of each matrix is the digit it represents. Note that the dataset provided is already splitted into a training set and a test set.\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nX = datasets.load_digits().images\ny = datasets.load_digits().target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15)\n\nLet us play with these data first.\n\nX_train.shape\n\n(1527, 8, 8)\n\n\n\ny_train.shape\n\n(1527,)\n\n\n\nX_test.shape\n\n(270, 8, 8)\n\n\n\ny_test.shape\n\n(270,)\n\n\n\ntype(X_train)\n\nnumpy.ndarray\n\n\nFrom these information we can see that the training set contains 1527 digits and the test set contains 270 digits. Each digit is represented by a \\(8\\times8\\) numpy array. Let us load one and display the digit by matplotlib.pyplot.imshow.\n\ndigit = X_train[0]\n\nimport matplotlib.pyplot as plt\nplt.imshow(digit, cmap='gray')\n\n\n\n\n\n\n\n\nThis image represents a handwritten digit. Could you recognize it? We could check our guess by looking at the label. The following shows that it is a 0.\n\ny_train[0]\n\n0\n\n\nNow we need to reshape these digits from \\(8\\times8\\) numpy arraies to \\(64\\) numpy arraies. Similar to previous examples, we will also normalize the dataset.\n\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\n\n\n2.4.2 Apply k-NN\nLike the previous two examples, we now try to apply the k-NN algorithm to classify these handwritten digits.\n\nimport numpy as np\n\nn_neighbors = 10\nX_test_sample = X_test_norm\ny_test_sample = y_test\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_sample])\n\nacc = np.mean(y_pred == y_test_sample)\nacc\n\n0.9814814814814815\n\n\nNow let us try to apply sklearn package. Note that we could run the code over the whole test set (which contains 10000 digits) and the speed is much faster comparing to our codes. To save time we won’t grid search k here. The code is the same anyway.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.9814814814814815",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#exercises-and-projects",
    "href": "contents/2/intro.html#exercises-and-projects",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.5 Exercises and Projects",
    "text": "2.5 Exercises and Projects\n\nExercise 2.1 Handwritten example :label: ex2handwritten Consider the 1-dimensional data set shown below.\n\n\n\n\n\n\n\nx\n1.5\n2.5\n3.5\n4.5\n5.0\n5.5\n5.75\n6.5\n7.5\n10.5\n\n\n\n\ny\n+\n+\n-\n-\n-\n+\n+\n-\n+\n+\n\n\n\n\n\nPlease use the data to compute the class of \\(x=5.5\\) according to \\(k=1\\), \\(3\\), \\(6\\) and \\(9\\). Please compute everything by hand.\n\n\nExercise 2.2 (Titanic) Please download the titanic dataset from here. This is the same dataset from what you dealt with in Chapter 1 Exercises. Therefore you may use the same way to prepare the data.\nPlease analyze the dataset and build a k-NN model to predict whether someone is survived or not. Note that you have to pick k at the end.\n\n\n\n\n\n[1] Harrington, P. (2012). Machine learning in action. Manning Publications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html",
    "href": "contents/3/intro.html",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1 Gini impurity\nTo split a dataset, we need a metric to tell whether the split is good or not. The two most popular metrics that are used are Gini impurity and Entropy. The two metrics don’t have essential differences, that the results obtained by applying each metric are very similar to each other. Therefore we will only focus on Gini impurity since it is slightly easier to compute and slightly easier to explain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#gini-impurity",
    "href": "contents/3/intro.html#gini-impurity",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1.1 Motivation and Definition\nAssume that we have a dataset of totally \\(n\\) objects, and these objects are divided into \\(k\\) classes. The \\(i\\)-th class has \\(n_i\\) objects. Then if we randomly pick an object, the probability to get an object belonging to the \\(i\\)-th class is\n\\[\np_i=\\frac{n_i}{n}\n\\]\nIf we then guess the class of the object purely based on the distribution of each class, the probability that our guess is incorrect is\n\\[\n1-p_i = 1-\\frac{n_i}{n}.\n\\]\nTherefore, if we randomly pick an object that belongs to the \\(i\\)-th class and randomly guess its class purely based on the distribution but our guess is wrong, the probability that such a thing happens is\n\\[\np_i(1-p_i).\n\\]\nConsider all classes. The probability at which any object of the dataset will be mislabelled when it is randomly labeled is the sum of the probability described above:\n\\[\n\\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2.\n\\]\nThis is the definition formula for the Gini impurity.\n\nDefinition 3.1 The Gini impurity is calculated using the following formula\n\\[\nGini = \\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2,\n\\] where \\(p_i\\) is the probability of class \\(i\\).\n\nThe way to understand Gini impurity is to consider some extreme examples.\n\nExample 3.1 Assume that we only have one class. Therefore \\(k=1\\), and \\(p_1=1\\). Then the Gini impurity is\n\\[\nGini = 1-1^2=0.\n\\] This is the minimum possible Gini impurity. It means that the dataset is pure: all the objects contained are of one unique class. In this case, we won’t make any mistakes if we randomly guess the label.\n\n\nExample 3.2 Assume that we have two classes. Therefore \\(k=2\\). Consider the distribution \\(p_1\\) and \\(p_2\\). We know that \\(p_1+p_2=1\\). Therefore \\(p_2=1-p_1\\). Then the Gini impurity is\n\\[\nGini(p_1) = 1-p_1^2-p_2^2=1-p_1^2-(1-p_1)^2=2p_1-2p_1^2.\n\\] When \\(0\\leq p_1\\leq 1\\), this function \\(Gini(p_1)\\) is between \\(0\\) and \\(0.5\\). - It gets \\(0\\) when \\(p_1=0\\) or \\(1\\). In these two cases, the dataset is still a one-class set since the size of one class is \\(0\\). - It gets \\(0.5\\) when \\(p_1=0.5\\). This means that the Gini impurity is maximized when the size of different classes are balanced.\n\n\n\n3.1.2 Algorithm\n\n\n\n\n\n\nAlgorithm: Gini impurity\n\n\n\nInputs A dataset \\(S=\\{data=[features, label]\\}\\) with labels.\nOutputs The Gini impurity of the dataset.\n\nGet the size \\(n\\) of the dataset.\nGo through the label list, and find all unique labels: \\(uniqueLabelList\\).\nGo through each label \\(l\\) in \\(uniqueLabelList\\) and count how many elements belonging to the label, and record them as \\(n_l\\).\nUse the formula to compute the Gini impurity:\n\\[\nGini = 1-\\sum_{l\\in uniqueLabelList}\\left(\\frac{n_l}{n}\\right)^2.\n\\]\n\n\n\nThe sample codes are listed below:\n\nimport pandas as pd\ndef gini(S):\n    N = len(S)\n    y = S[:, -1].reshape(N)\n    gini = 1 - ((pd.Series(y).value_counts()/N)**2).sum()\n    return gini",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#cart-algorithms",
    "href": "contents/3/intro.html#cart-algorithms",
    "title": "3  Decision Trees",
    "section": "3.2 CART Algorithms",
    "text": "3.2 CART Algorithms\n\n3.2.1 Ideas\nConsider a labeled dataset \\(S\\) with totally \\(m\\) elements. We use a feature \\(k\\) and a threshold \\(t_k\\) to split it into two subsets: \\(S_l\\) with \\(m_l\\) elements and \\(S_r\\) with \\(m_r\\) elements. Then the cost function of this split is\n\\[\nJ(k, t_k)=\\frac{m_l}mGini(S_l)+\\frac{m_r}{m}Gini(S_r).\n\\] It is not hard to see that the more pure the two subsets are the lower the cost function is. Therefore our goal is find a split that can minimize the cost function.\n\n\n\n\n\n\nAlgorithm: Split the Dataset\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\).\nOutputs A best split \\((k, t_k)\\).\n\nFor each feature \\(k\\):\n\nFor each value \\(t\\) of the feature:\n\nSplit the dataset \\(S\\) into two subsets, one with \\(k\\leq t\\) and one with \\(k&gt;t\\).\nCompute the cost function \\(J(k,t)\\).\nCompare it with the current smallest cost. If this split has smaller cost, replace the current smallest cost and pair with \\((k, t)\\).\n\n\nReturn the pair \\((k,t_k)\\) that has the smallest cost function.\n\n\n\nWe then use this split algorithm recursively to get the decision tree.\n\n\n\n\n\n\nClassification and Regression Tree, CART\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\) and a maximal depth max_depth.\nOutputs A decision tree.\n\nStarting from the original dataset \\(S\\). Set the working dataset \\(G=S\\).\nConsider a dataset \\(G\\). If \\(Gini(G)\\neq0\\), split \\(G\\) into \\(G_l\\) and \\(G_r\\) to minimize the cost function. Record the split pair \\((k, t_k)\\).\nNow set the working dataset \\(G=G_l\\) and \\(G=G_r\\) respectively, and apply the above two steps to each of them.\nRepeat the above steps, until max_depth is reached.\n\n\n\nHere are the sample codes.\n\ndef split(G):\n    m = G.shape[0]\n    gmini = gini(G)\n    pair = None\n    if gini(G) != 0:\n        numOffeatures = G.shape[1] - 1\n        for k in range(numOffeatures):\n            for t in range(m):\n                Gl = G[G[:, k] &lt;= G[t, k]]\n                Gr = G[G[:, k] &gt; G[t, k]]\n                gl = gini(Gl)\n                gr = gini(Gr)\n                ml = Gl.shape[0]\n                mr = Gr.shape[0]\n                g = gl*ml/m + gr*mr/m\n                if g &lt; gmini:\n                    gmini = g\n                    pair = (k, G[t, k])\n                    Glm = Gl\n                    Grm = Gr\n        res = {'split': True,\n               'pair': pair,\n               'sets': (Glm, Grm)}\n    else:\n        res = {'split': False,\n               'pair': pair,\n               'sets': G}\n    return res\n\nFor the purpose of counting labels, we also write a code to do so.\n\nimport pandas as pd\ndef countlabels(S):\n    y = S[:, -1].reshape(S.shape[0])\n    labelCount = dict(pd.Series(y).value_counts())\n    return labelCount",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "href": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "title": "3  Decision Trees",
    "section": "3.3 Decision Tree Project 1: the iris dataset",
    "text": "3.3 Decision Tree Project 1: the iris dataset\nWe are going to use the Decision Tree model to study the iris dataset. This dataset has already studied previously using k-NN. Again we will only use the first two features for visualization purpose.\n\n3.3.1 Initial setup\nSince the dataset will be splitted, we will put X and y together as a single variable S. In this case when we split the dataset by selecting rows, the features and the labels are still paired correctly.\nWe also print the labels and the feature names for our convenience.\n\nfrom sklearn.datasets import load_iris\nimport numpy as np\nfrom assests.codes.dt import gini, split, countlabels\n\niris = load_iris()\nX = iris.data[:, 2:]\ny = iris.target\ny = y.reshape((y.shape[0],1))\nS = np.concatenate([X,y], axis=1)\n\nprint(iris.target_names)\nprint(iris.feature_names)\n\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\n\n\n3.3.2 Apply CART manually\nWe apply split to the dataset S.\n\nr = split(S)\nif r['split'] is True:\n    Gl, Gr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Gl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gr)),\n          ' and its label counts is {d}'.format(d=countlabels(Gr)))\n\n(0, 1.9)\nThe left subset's Gini impurity is 0.00,  and its label counts is {0.0: 50}\nThe right subset's Gini impurity is 0.50,  and its label counts is {1.0: 50, 2.0: 50}\n\n\nThe results shows that S is splitted into two subsets based on the 0-th feature and the split value is 1.9.\nThe left subset is already pure since its Gini impurity is 0. All elements in the left subset is label 0 (which is setosa). The right one is mixed since its Gini impurity is 0.5. Therefore we need to apply split again to the right subset.\n\nr = split(Gr)\nif r['split'] is True:\n    Grl, Grr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Grl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grr)),\n          ' and its label counts is {d}'.format(d=countlabels(Grr)))\n\n(1, 1.7)\nThe left subset's Gini impurity is 0.17,  and its label counts is {1.0: 49, 2.0: 5}\nThe right subset's Gini impurity is 0.04,  and its label counts is {2.0: 45, 1.0: 1}\n\n\nThis time the subset is splitted into two more subsets based on the 1-st feature and the split value is 1.7. The total Gini impurity is minimized using this split.\nThe decision we created so far can be described as follows:\n\nCheck the first feature sepal length (cm) to see whether it is smaller or equal to 1.9.\n\nIf it is, classify it as lable 0 which is setosa.\nIf not, continue to the next stage.\n\nCheck the second feature sepal width (cm) to see whether it is smaller or equal to 1.7.\n\nIf it is, classify it as label 1 which is versicolor.\nIf not, classify it as label 2 which is virginica.\n\n\n\n\n3.3.3 Use package sklearn\nNow we would like to use the decision tree package provided by sklearn. The process is straightforward. The parameter random_state=40 will be discussed {ref}later&lt;note-random_state&gt;, and it is not necessary in most cases.\n\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth=2, random_state=40)\nclf.fit(X, y)\n\nDecisionTreeClassifier(max_depth=2, random_state=40)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=2, random_state=40)\n\n\nsklearn provide a way to automatically generate the tree view of the decision tree. The code is as follows.\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(2, 2), dpi=200)\ntree.plot_tree(clf, filled=True, impurity=True)\n\n[Text(0.4, 0.8333333333333334, 'x[1] &lt;= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.2, 0.5, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.6, 0.5, 'x[1] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.4, 0.16666666666666666, 'gini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.8, 0.16666666666666666, 'gini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]')]\n\n\n\n\n\n\n\n\n\nSimilar to k-NN, we may use sklearn.inspection.DecisionBoundaryDisplay to visualize the decision boundary of this decision tree.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap='coolwarm',\n    response_method=\"predict\",\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n)\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=15)\n\n\n\n\n\n\n\n\n\n\n3.3.4 Analyze the differences between the two methods\nThe tree generated by sklearn and the tree we got manually is a little bit different. Let us explore the differences here.\nTo make it easier to split the set, we could convert the numpy.ndarray to pandas.DataFrame.\n\nimport pandas as pd\n\ndf = pd.DataFrame(X)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.4\n0.2\n\n\n1\n1.4\n0.2\n\n\n2\n1.3\n0.2\n\n\n3\n1.5\n0.2\n\n\n4\n1.4\n0.2\n\n\n\n\n\n\n\nNow based on our tree, we would like to get all data points that the first feature (which is marked as 0) is smaller or equal to 1.9. We save it as df1. Similarly based on the tree gotten from sklearn, we would like to get all data points taht the second feature (which is marked as 1) is smaller or equal to 0.8 and save it to df2.\n\ndf1 = df[df[0]&lt;=1.9]\ndf2 = df[df[1]&lt;=0.8]\n\nThen we would like to compare these two dataframes. What we want is to see whether they are the same regardless the order. One way to do this is to sort the two dataframes and then compare them directly.\nTo sort the dataframe we use the method DataFrame.sort_values. The details can be found here. Note that after sort_values we apply reset_index to reset the index just in case the index is massed by the sort operation.\nThen we use DataFrame.equals to check whether they are the same.\n\ndf1sorted = df1.sort_values(by=df1.columns.tolist()).reset_index(drop=True)\ndf2sorted = df2.sort_values(by=df2.columns.tolist()).reset_index(drop=True)\nprint(df1sorted.equals(df2sorted))\n\nTrue\n\n\nSo these two sets are really the same. The reason this happens can be seen from the following two graphs.\n\n\n\n\n\n\n\n\n\nFrom our code\n\n\n\n\n\n\n\nFrom sklearn\n\n\n\n\n\nSo you can see that either way can give us the same classification. This means that given one dataset the construction of the decision tree might be random at some points.\n\n\n\n\n\n\nnote-random_state\n\n\n\nSince the split is random, when using sklearn.DecisionTreeClassifier to construct decision trees, sometimes we might get the same tree as what we get from our naive codes.\nTo illustrate this phenomenaon I need to set the random state in case it will generate the same tree as ours when I need it to generate a different tree. The parameter random_state=40 mentioned before is for this purpose.\n\n\nAnother difference is the split value of the second branch. In our case it is 1.7 and in sklearn case it is 1.75. So after we get the right subset from the first split (which is called dfr), we would split it into two sets based on whether the second feature is above or below 1.7.\n\ndfr = df[df[0]&gt;1.9]\ndf2a = dfr[dfr[1]&gt;1.7]\ndf2b = dfr[dfr[1]&lt;=1.7]\nprint(df2b[1].max())\nprint(df2a[1].min())\n\n1.7\n1.8\n\n\nNow you can see where the split number comes from. In our code, when we found a split, we will directly use that number as the cut. In this case it is 1.7.\nIn sklearn, when it finds a split, the algorithm will go for the middle of the gap as the cut. In this case it is (1.7+1.8)/2=1.75.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-2-make_moons-dataset",
    "href": "contents/3/intro.html#decision-tree-project-2-make_moons-dataset",
    "title": "3  Decision Trees",
    "section": "3.4 Decision Tree Project 2: make_moons dataset",
    "text": "3.4 Decision Tree Project 2: make_moons dataset\nsklearn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. We are going to use make_moons in this section. More details can be found here.\nmake_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. make_moons produces two interleaving half circles. It is useful for visualization.\nLet us explorer the dataset first.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\n\n\n\n\n\n\n\n\nNow we are applying sklearn.DecisionTreeClassifier to construct the decision tree. The steps are as follows.\n\nSplit the dataset into training data and test data.\nConstruct the pipeline. Since we won’t apply any transformers there for this problem, we may just use the classifier sklearn.DecisionTreeClassifier directly without really construct the pipeline object.\nConsider the hyperparameter space for grid search. For this problme we choose min_samples_split and max_leaf_nodes as the hyperparameters we need. We will let min_samples_split run through 2 to 5, and max_leaf_nodes run through 2 to 50. We will use grid_search_cv to find the best hyperparameter for our model. For cross-validation, the number of split is set to be 3 which means that we will run trainning 3 times for each pair of hyperparameters.\nRun grid_search_cv. Find the best hyperparameters and the best estimator. Test it on the test set to get the accuracy score.\n\n\n# Step 1\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Step 3\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nparams = {'min_samples_split': list(range(2, 5)),\n          'max_leaf_nodes': list(range(2, 50))}\ngrid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), \n                              params, verbose=1, cv=3)\ngrid_search_cv.fit(X_train, y_train)\n\nFitting 3 folds for each of 144 candidates, totalling 432 fits\n\n\nGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)estimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=42)DecisionTreeClassifierDecisionTreeClassifier(random_state=42)\n\n\n\n# Step 4\nfrom sklearn.metrics import accuracy_score\n\nclf = grid_search_cv.best_estimator_\nprint(grid_search_cv.best_params_)\ny_pred = clf.predict(X_test)\naccuracy_score(y_pred, y_test)\n\n{'max_leaf_nodes': 17, 'min_samples_split': 2}\n\n\n0.8695\n\n\nNow you can see that for this make_moons dataset, the best decision tree should have at most 17 leaf nodes and the minimum number of samples required to be at a leaft node is 2. The fitted decision tree can get 86.95% accuracy on the test set.\nNow we can plot the decision tree and the decision surface.\n\nfrom sklearn import tree\nplt.figure(figsize=(15, 15), dpi=300)\ntree.plot_tree(clf, filled=True)\n\n[Text(0.5340909090909091, 0.9375, 'x[1] &lt;= 0.296\\ngini = 0.5\\nsamples = 8000\\nvalue = [3987, 4013]'),\n Text(0.25, 0.8125, 'x[0] &lt;= -0.476\\ngini = 0.367\\nsamples = 4275\\nvalue = [1036, 3239]'),\n Text(0.09090909090909091, 0.6875, 'x[0] &lt;= -0.764\\ngini = 0.183\\nsamples = 472\\nvalue = [424, 48]'),\n Text(0.045454545454545456, 0.5625, 'gini = 0.035\\nsamples = 333\\nvalue = [327, 6]'),\n Text(0.13636363636363635, 0.5625, 'x[1] &lt;= 0.047\\ngini = 0.422\\nsamples = 139\\nvalue = [97, 42]'),\n Text(0.09090909090909091, 0.4375, 'gini = 0.496\\nsamples = 70\\nvalue = [38, 32]'),\n Text(0.18181818181818182, 0.4375, 'gini = 0.248\\nsamples = 69\\nvalue = [59, 10]'),\n Text(0.4090909090909091, 0.6875, 'x[1] &lt;= -0.062\\ngini = 0.27\\nsamples = 3803\\nvalue = [612, 3191]'),\n Text(0.3181818181818182, 0.5625, 'x[1] &lt;= -0.371\\ngini = 0.147\\nsamples = 2426\\nvalue = [194, 2232]'),\n Text(0.2727272727272727, 0.4375, 'gini = 0.079\\nsamples = 1336\\nvalue = [55, 1281]'),\n Text(0.36363636363636365, 0.4375, 'gini = 0.223\\nsamples = 1090\\nvalue = [139, 951]'),\n Text(0.5, 0.5625, 'x[0] &lt;= 1.508\\ngini = 0.423\\nsamples = 1377\\nvalue = [418, 959]'),\n Text(0.45454545454545453, 0.4375, 'x[0] &lt;= 0.503\\ngini = 0.48\\nsamples = 1013\\nvalue = [404, 609]'),\n Text(0.36363636363636365, 0.3125, 'x[0] &lt;= -0.162\\ngini = 0.417\\nsamples = 469\\nvalue = [139, 330]'),\n Text(0.3181818181818182, 0.1875, 'gini = 0.5\\nsamples = 120\\nvalue = [61, 59]'),\n Text(0.4090909090909091, 0.1875, 'gini = 0.347\\nsamples = 349\\nvalue = [78, 271]'),\n Text(0.5454545454545454, 0.3125, 'x[0] &lt;= 1.1\\ngini = 0.5\\nsamples = 544\\nvalue = [265, 279]'),\n Text(0.5, 0.1875, 'x[1] &lt;= 0.129\\ngini = 0.49\\nsamples = 339\\nvalue = [193, 146]'),\n Text(0.45454545454545453, 0.0625, 'gini = 0.498\\nsamples = 178\\nvalue = [84, 94]'),\n Text(0.5454545454545454, 0.0625, 'gini = 0.437\\nsamples = 161\\nvalue = [109, 52]'),\n Text(0.5909090909090909, 0.1875, 'gini = 0.456\\nsamples = 205\\nvalue = [72, 133]'),\n Text(0.5454545454545454, 0.4375, 'gini = 0.074\\nsamples = 364\\nvalue = [14, 350]'),\n Text(0.8181818181818182, 0.8125, 'x[0] &lt;= 1.452\\ngini = 0.329\\nsamples = 3725\\nvalue = [2951, 774]'),\n Text(0.7272727272727273, 0.6875, 'x[1] &lt;= 0.757\\ngini = 0.232\\nsamples = 3355\\nvalue = [2905, 450]'),\n Text(0.6818181818181818, 0.5625, 'x[0] &lt;= -0.588\\ngini = 0.349\\nsamples = 1629\\nvalue = [1262, 367]'),\n Text(0.6363636363636364, 0.4375, 'gini = 0.07\\nsamples = 384\\nvalue = [370, 14]'),\n Text(0.7272727272727273, 0.4375, 'x[1] &lt;= 0.439\\ngini = 0.406\\nsamples = 1245\\nvalue = [892, 353]'),\n Text(0.6818181818181818, 0.3125, 'gini = 0.477\\nsamples = 420\\nvalue = [255, 165]'),\n Text(0.7727272727272727, 0.3125, 'gini = 0.352\\nsamples = 825\\nvalue = [637, 188]'),\n Text(0.7727272727272727, 0.5625, 'gini = 0.092\\nsamples = 1726\\nvalue = [1643, 83]'),\n Text(0.9090909090909091, 0.6875, 'x[0] &lt;= 1.782\\ngini = 0.218\\nsamples = 370\\nvalue = [46, 324]'),\n Text(0.8636363636363636, 0.5625, 'gini = 0.416\\nsamples = 132\\nvalue = [39, 93]'),\n Text(0.9545454545454546, 0.5625, 'gini = 0.057\\nsamples = 238\\nvalue = [7, 231]')]\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap=plt.cm.RdYlBu,\n    response_method=\"predict\"\n)\nplt.scatter(\n    X[:, 0],\n    X[:, 1],\n    c=y,\n    cmap='gray',\n    edgecolor=\"black\",\n    s=15,\n    alpha=.15)\n\n\n\n\n\n\n\n\nSince it is not very clear what the boundary looks like, I will draw the decision surface individually below.\n\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap=plt.cm.RdYlBu,\n    response_method=\"predict\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#exercises-and-projects",
    "href": "contents/3/intro.html#exercises-and-projects",
    "title": "3  Decision Trees",
    "section": "3.5 Exercises and Projects",
    "text": "3.5 Exercises and Projects\n\nExercise 3.1 The dataset and its scattering plot is given below.\n\nPlease calculate the Gini impurity of the whole set by hand.\nPlease apply CART to create the decision tree by hand.\nPlease use the tree you created to classify the following points:\n\n\\((0.4, 1.0)\\)\n\\((0.6, 1.0)\\)\n\\((0.6, 0)\\)\n\n\nThe following code is for ploting. You may also get the precise data points by reading the code. You don’t need to write codes to solve the problem.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.2 CHOOSE ONE: Please apply the Decision Tree to one of the following datasets.\n\ndating dataset (in Chpater 2).\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the accuracy (or cross-validation score) of your model and compare it with the models you learned before (kNN).\nPlease find the two most important features and explane your reason.\n(Optional) Use the two most important features to draw the Decision Boundary if possible.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html",
    "href": "contents/4/intro.html",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1 Bootstrap aggregating",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#bootstrap-aggregating",
    "href": "contents/4/intro.html#bootstrap-aggregating",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1.1 Basic bagging\nOne approach to get many estimators is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.\nConsider the following example. The dataset is the one we used in Chpater 3: make_moon. We split the dataset into training and test sets.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\n\n\n\n\n\n\n\nWe would like to sample from the dataset to get some smaller minisets. We will use sklearn.model_selection.ShuffleSplit to perform the action.\nThe output of ShuffleSplit is a generator. To get the index out of it we need a for loop. You may check out the following code.\nNote that ShuffleSplit is originally used to shuffle data into training and test sets. We would only use the shuffle function out of it, so we will set test_size to be 1 and use _ later in the for loop since we won’t use that part of the information.\nWhat we finally get is a generator rs that produces indexes of subsets of X_train and y_train.\n\nfrom sklearn.model_selection import ShuffleSplit\nn_trees = 1000\nn_instances = 100\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\n\nNow we would like to generate a list of Decision Trees. We could use the hyperparameters we get from Chapter 3. We train each tree over a certain mini set, and then evaluate the trained model over the test set. The average accuracy is around 80%.\nNote that rs is a generator. We put it in a for loop, and during each loop it will produce a list of indexes which gives a subset. We will directly train our model over the subset and use it to predict the test set. The result of each tree is put in the list y_pred_list and the accuracy is stored in the list acc_list. The mean of the accuracy is then computed by np.mean(acc_list).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(min_samples_split=2, max_leaf_nodes=17)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\n\nnp.mean(acc_list)\n\n0.7982400000000001\n\n\nNow for each test data, we actually have n_trees=1000 predicted results. We can treat it as the options from 1000 exports and would like to use the majority as our result. For this purpose we would like to use mode() which will find the most frequent entry.\n\nfrom scipy.stats import mode\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\n\nSince the output of mode is a tuple where the first entry is a 2D array, we need to reshape y_pred_mode. This is the result using this voting system. Then we are able to compute the accuracy, and find that it is increased from the previous prediction.\n\naccuracy_score(y_pred_mode, y_test)\n\n0.8646666666666667\n\n\n\n\n4.1.2 Some rough analysis\nThe point of Bagging is to let every classifier study part of the data, and then gather the opinions from everyone. If the performance are almost the same between individual classifers and the Bagging classifiers, this means that the majority of the individual classifiers have the same opinions. One possible reason is that the randomized subsets already catch the main features of the dataset that every individual classifiers behave similar.\n\n4.1.2.1 Case 1\nLet us continue with the previous dataset. We start from using Decision Tree with max_depth=1. In other words each tree only split once.\n\nn_trees = 500\nn_instances = 1000\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.7704213333333334\nThe accuracy of the bagging classifier: 0.772\n\n\nThe two accuracy has some differences, but not much. This is due to the fact that the sample size of the subset is too large: 1000 can already help the individual classifers to capture the major ideas of the datasets. Let us see the first 1000 data points. The scattering plot is very similar to that of the whole dataset shown above.\n\nNpiece = 1000\nplt.scatter(x=X[:Npiece, 0], y=X[:Npiece, 1], c=y[:Npiece])\n\n\n\n\n\n\n\n\n\n\n4.1.2.2 Case 2\nIf we reduce the sample size to be very small, for example, 20, the sampled subset will lose a lot of information and it will be much harder to capture the idea of the original dataset. See the scattering plot of the first 20 data points.\n\nNpiece = 20\nplt.scatter(x=X[:Npiece, 0], y=X[:Npiece, 1], c=y[:Npiece])\n\n\n\n\n\n\n\n\nIn this case, let us see the performance comparison between multiple decision trees and the bagging classifier.\n\nn_trees = 500\nn_instances = 20\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.7273333333333334\nThe accuracy of the bagging classifier: 0.814\n\n\nThis time you may see a significant increase in the performance.\n\n\n\n4.1.3 Using sklearn\nsklearn provides BaggingClassifier to directly perform bagging or pasting. The code is as follows.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(),\n                            n_estimators=1000,\n                            max_samples=100,\n                            bootstrap=True)\n\nIn the above code, bag_clf is a bagging classifier, made of 500 DecisionTreeClassifers, and is trained over subsets of size 100. The option bootstrap=True means that it is bagging. If you would like to use pasting, the option is bootstrap=False.\nThis bag_clf also has .fit() and .predict() methods. It is used the same as our previous classifiers. Let us try the make_moon dataset.\n\nbag_clf.fit(X_train, y_train)\ny_pred_bag = bag_clf.predict(X_test)\naccuracy_score(y_pred_bag, y_test)\n\n0.8606666666666667\n\n\n\n\n4.1.4 OOB score\nWhen we use bagging, it is possible that some of the training data are not used. In this case, we could record which data are not used, and just use them as the test set, instead of providing extra data for test. The data that are not used is called out-of-bag instances, or oob for short. The accuracy over the oob data is called the oob score.\nWe could set oob_score=True to enable the function when creating a BaggingClassifier, and use .oob_score_ to get the oob score after training.\n\nbag_clf_oob = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=1000,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True)\nbag_clf_oob.fit(X_train, y_train)\nbag_clf_oob.oob_score_\n\n0.8649411764705882\n\n\n\n\n4.1.5 Random Forests\nWhen the classifiers used in a bagging classifier are all Decision Trees, the bagging classifier is called a random forest. sklearn provide RandomForestClassifier class. It is almost the same as BaggingClassifier + DecisionTreeClassifer.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\ny_pred_rnd = rnd_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.8613333333333333\n\n\nWhen we use the Decision Tree as our base estimators, the class RandomForestClassifier provides more control over growing the random forest, with a certain optimizations. If you would like to use other estimators, then BaggingClassifier should be used.\n\n\n4.1.6 Extra-trees\nWhen growing a Decision Tree, our method is to search through all possible ways to find the best split point that get the lowest Gini impurity. Anohter method is to use a random split. Of course a random tree performs much worse, but if we use it to form a random forest, the voting system can help to increase the accuracy. On the other hand, random split is much faster than a regular Decision Tree.\nThis type of forest is called Extremely Randomized Trees, or Extra-Trees for short. We could modify the above random forest classifier code to implement the extra-tree algorithm. The key point is that we don’t apply the Decision Tree algorithm to X_subset. Instead we perform a random split.\n\nn_trees = 500\nn_instances = 20\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n# random split\n    i = np.random.randint(0, X_subset.shape[0])\n    j = np.random.randint(0, X_subset.shape[1])\n    split_threshold = X_subset[i, j]\n    lsetindex = np.where(X_subset[:, j]&lt;split_threshold)[0]\n\n    if len(lsetindex) == 0:\n        rsetindex = np.where(X_subset[:, j]&gt;=split_threshold)\n        rmode, _ = mode(y_subset[rsetindex], keepdims=True)\n        rmode = rmode[0]\n        lmode = 1 - rmode\n    else:\n        lmode, _ = mode(y_subset[lsetindex], keepdims=True)\n        lmode = lmode[0]\n        rmode = 1 - lmode\n    y_pred = np.where(X_test[:, j] &lt; split_threshold, lmode, rmode).reshape(-1)\n# The above code is used to use the random split to classify the data points\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.6313213333333333\nThe accuracy of the bagging classifier: 0.8273333333333334\n\n\nFrom the above example, you may find a significant increase in the performace from the mean individual accuracy to the Extra-tree classifier accuracy. The accuracy of the Extra-tree classifier is also very close to what we get from the original data points, although its base classifier is much simpler.\nIn sklearn there is an ExtraTreesClassifier to create such a classifier. It is hard to say which random forest is better beforehand. What we can do is to test and calculate the cross-validation scores (with grid search for hyperparameters tuning).\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\ny_pred_rnd = ext_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.858\n\n\nIn the above example, RandomForestClassifier and ExtraTreesClassifier get similar accuracy. However from the code below, you will see that in this example ExtraTreesClassifier is much faster than RandomForestClassifier.\n\nfrom time import time\nt0 = time()\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\nt1 = time()\nprint('Random Frorest: {}'.format(t1 - t0))\n\nt0 = time()\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\nt1 = time()\nprint('Extremely Randomized Trees: {}'.format(t1 - t0))\n\nRandom Frorest: 9.64048957824707\nExtremely Randomized Trees: 1.4413414001464844\n\n\n\n\n4.1.7 Gini importance\nAfter training a Decision Tree, we could look at each node. Each split is against a feature, which decrease the Gini impurity the most. In other words, we could say that the feature is the most important during the split.\nUsing the average Gini impurity decreased as a metric, we could measure the importance of each feature. This is called Gini importance. If the feature is useful, it tends to split mixed labeled nodes into pure single class nodes.\nIn the case of random forest, since there are many trees, we might compute the weighted average of the Gini importance across all trees. The weight depends on how many times the feature is used in a specific node.\nUsing RandomForestClassifier, we can directly get access to the Gini importance of each feature by .feature_importance_. Please see the following example.\n\nrnd_clf.fit(X_train, y_train)\nrnd_clf.feature_importances_\n\narray([0.43823588, 0.56176412])\n\n\nIn this example, you may see that the two features are relavely equally important, where the second feature is slightly more important since on average it decrease the Gini impurity a little bit more.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#voting-machine",
    "href": "contents/4/intro.html#voting-machine",
    "title": "4  Ensemble methods",
    "section": "4.2 Voting machine",
    "text": "4.2 Voting machine\n\n4.2.1 Voting classifier\nAssume that we have several trained classifiers. The easiest way to make a better classifer out of what we already have is to build a voting system. That is, each classifier give its own prediction, and it will be considered as a vote, and finally the highest vote will be the prediction of the system.\nIn sklearn, you may use VotingClassifier. It works as follows.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclfs = [('knn', KNeighborsClassifier(n_neighbors=5)),\n        ('dt', DecisionTreeClassifier(max_depth=2))]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\n\nAll classifiers are stored in the list clfs, whose elements are tuples. The syntax is very similar to Pipeline. What the classifier does is to train all listed classifiers and use the majority vote to predict the class of given test data. If each classifier has one vote, the voting method is hard. There is also a soft voting method. In this case, every classifiers not only can predict the classes of the given data, but also estimiate the probability of the given data that belongs to certain classes. On coding level, each classifier should have the predict_proba() method. In this case, the weight of each vote is determined by the probability computed. In our course we mainly use hard voting.\nLet us use make_moon as an example. We first load the dataset.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nWe would like to apply kNN model. As before, we build a data pipeline pipe to first apply MinMaxScaler and then KNeighborsClassifier.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline(steps=[('scalar', MinMaxScaler()),\n                       ('knn', KNeighborsClassifier())])\nparameters = {'knn__n_neighbors': list(range(1, 51))}\ngs_knn = GridSearchCV(pipe, param_grid=parameters) \ngs_knn.fit(X_train, y_train)\nclf_knn = gs_knn.best_estimator_\nclf_knn.score(X_test, y_test)\n\n0.8633333333333333\n\n\nThe resulted accuracy is shown above.\nWe then try it with the Decision Tree.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ngs_dt = GridSearchCV(DecisionTreeClassifier(), param_grid={'max_depth': list(range(1, 11)), 'max_leaf_nodes': list(range(10, 30))})\ngs_dt.fit(X_train, y_train)\nclf_dt = gs_dt.best_estimator_\nclf_dt.score(X_test, y_test)\n\n0.8606666666666667\n\n\nWe would also want to try Logistic regression method. This will be covered in the next Chapter. At current stage we just use the default setting without changing any hyperparameters.\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(X_train, y_train)\nclf_lr.score(X_test, y_test)\n\n0.8266666666666667\n\n\nNow we use a voting classifier to combine the results.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = [('knn', KNeighborsClassifier()),\n        ('dt', DecisionTreeClassifier()),\n        ('lr', LogisticRegression())]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n0.8346666666666667\n\n\nYou may compare the results of all these four classifiers. The voting classifier is not guaranteed to be better. It is just a way to form a model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#adaboost",
    "href": "contents/4/intro.html#adaboost",
    "title": "4  Ensemble methods",
    "section": "4.3 AdaBoost",
    "text": "4.3 AdaBoost\nThis is the first algorithm that successfully implements the boosting idea. AdaBoost is short for Adaptive Boosting.\n\n4.3.1 Weighted dataset\nWe firstly talk about training a Decision Tree on a weighted dataset. The idea is very simple. When building a Decision Tree, we use some method to determine the split. In this course the Gini impurity is used. There are at least two other methods: cross-entropy and misclassified rate. For all three, the count of the elemnts in some classes is the essnetial part. To train the model over the weighted dataset, we just need to upgrade the count of the elements by the weighted count.\n\nExample 4.1 Consider the following data:\n\n\n\n\n\n\n\nx0\nx1\ny\nWeight\n\n\n\n\n1.0\n2.1\n+\n0.5\n\n\n1.0\n1.1\n+\n0.125\n\n\n1.3\n1.0\n-\n0.125\n\n\n1.0\n1.0\n-\n0.125\n\n\n2.0\n1.0\n+\n0.125\n\n\n\n\n\nThe weighted Gini impurity is\n\\[\n\\text{WeightedGini}=1-(0.5+0.125+0.125)^2-(0.125+0.125)^2=0.375.\n\\]\nYou may see that the original Gini impurity is just the weighted Gini impurity with equal weights. Therefore the first tree we get from AdaBoost (see below) is the same tree we get from the Decision Tree model in Chpater 3.\n\n\n\n4.3.2 General process\nHere is the rough description of AdaBoost.\n\nAssign weights to each data point. At the begining we could assign weights equally.\nTrain a classifier based on the weighted dataset, and use it to predict on the training set. Find out all wrong answers.\nAdjust the weights, by inceasing the weights of data points that are done wrongly in the previous generation.\nTrain a new classifier using the new weighted dataset. Predict on the training set and record the wrong answers.\nRepeat the above process to get many classifiers. The training stops either by hitting \\(0\\) error rate, or after a specific number of rounds.\nThe final results is based on the weighted total votes from all classifiers we trained.\n\nNow let us talk about the details. Assume there are \\(N\\) data points. Then the inital weights are set to be \\(\\dfrac1N\\). There are 2 sets of weights. Let \\(w^{(i)}\\) be weights of the \\(i\\)th data points. Let \\(\\alpha_j\\) be the weights of the \\(j\\)th classifier. After training the \\(j\\)th classifier, the error rate is denoted by \\(e_j\\). Then we have\n\\[\ne_j=\\frac{\\text{the total weights of data points that are misclassified by the $j$th classifier}}{\\text{the total weights of data points}}\n\\]\n\\[\n\\alpha_j=\\eta\\ln\\left(\\dfrac{1-e_j}{e_j}\\right).\n\\]\n\\[\nw^{(i)}_{\\text{new}}\\leftarrow\\text{normalization} \\leftarrow w^{(i)}\\leftarrow\\begin{cases}w^{(i)}&\\text{if the $i$th data is correctly classified,}\\\\w^{(i)}\\exp(\\alpha_j)&\\text{if the $i$th data is misclassified.}\\end{cases}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe first tree is the same tree we get from the regular Decision Tree model. In the rest of the training process, more weights are put on the data that we are wrong in the previous iteration. Therefore the process is the mimic of “learning from mistakes”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe \\(\\eta\\) in computing \\(\\alpha_j\\) is called the learning rate. It is a hyperparameter that will be specified mannually. It does exactly what it appears to do: alter the weights of each classifier. The default is 1.0. When the number is very small (which is recommended although it can be any positive number), more iterations will be expected.\n\n\n\n\n4.3.3 Example 1: the iris dataset\nSimilar to all previous models, sklearn provides AdaBoostClassifier. The way to use it is similar to previous models. Note that although we are able to use any classifiers for AdaBoost, the most popular choice is Decision Tree with max_depth=1. This type of Decision Trees are also called Decision Stumps.\nIn the following examples, we initialize an AdaBoostClassifier with 500 Decision Stumps and learning_rate=0.5.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=1000,\n                             learning_rate=.5)\n\nWe will use the iris dataset for illustration. The cross_val_score is calculated as follows.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nscores = cross_val_score(ada_clf, X, y, cv=5)\nscores.mean()\n\n0.9533333333333334\n\n\n\n\n4.3.4 Example 2: the Horse Colic dataset\nThis dataset is from UCI Machine Learning Repository. The data is about whether horses survive if they get a disease called Colic. The dataset is preprocessed as follows. Note that there are a few missing values inside, and we replace them with 0.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\ndf = df.fillna(0)\nX = df.iloc[:, 1:].to_numpy().astype(float)\ny = df[0].to_numpy().astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nclf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.2)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.6222222222222222",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#exercises",
    "href": "contents/4/intro.html#exercises",
    "title": "4  Ensemble methods",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\nExercise 4.1 CHOOSE ONE: Please apply the random forest to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the cross-validation score and the OOB score of your model and compare it with the models you learned before (kNN, Decision Trees).\nPlease find some typical features (using the Gini importance) and draw the Decision Boundary against the features you choose.\n\n\n\nExercise 4.2 Please use the following code to get the mgq dataset.\n\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300,\n                                 n_features=2, n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, -y2 + 1))\n\nPlease build an AdaBoost model.\n\n\nExercise 4.3 Please use RandomForestClassifier, ExtraTreesClassifier and KNeighbourClassifier to form a voting classifier, and apply to the MNIST dataset.\n\n\n\n\n\n\n\nMNIST\n\n\n\nThis dataset can be loaded using the following code.\n\nimport numpy as np\nimport requests\nfrom io import BytesIO\nr = requests.get('https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz', stream = True) \ndata = np.load(BytesIO(r.raw.read()))\nX_train = data['x_train']\nX_test = data['x_test']\ny_train = data['y_train']\ny_test = data['y_test']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html",
    "href": "contents/5/intro.html",
    "title": "5  Logistic regression",
    "section": "",
    "text": "5.1 Basic idea\nThe Logsitic regression is used to predict the probability of a data point belonging to a specific class. It is based on linear regression. The major difference is that logistic regreesion will have an activation function \\(\\sigma\\) at the final stage to change the predicted values of the linear regression to the values that indicate classes. In the case of binary classification, the outcome of \\(\\sigma\\) will be between \\(0\\) and \\(1\\), which is related to the two classes respectively. In this case, the number is interepted as the probability of the data to be in one of the specific class.\nThe model for Logistic regression is as follows:\n\\[\np=\\sigma(L(x))=\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\sigma\\left(\\Theta \\hat{x}^T\\right).\n\\]\nIn most cases, this activation function is chosen to be the Sigmoid funciton.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#basic-idea",
    "href": "contents/5/intro.html#basic-idea",
    "title": "5  Logistic regression",
    "section": "",
    "text": "5.1.1 Sigmoid function\nThe Sigmoid function is defined as follows:\n\\[\n\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n\\] The graph of the function is shown below.\n\n\n\n\n\n\n\n\n\nThe main properties of \\(\\sigma\\) are listed below as a Lemma.\n\nLemma 5.1 The Sigmoid function \\(\\sigma(z)\\) satisfies the following properties.\n\n\\(\\sigma(z)\\rightarrow \\infty\\) when \\(z\\mapsto \\infty\\).\n\\(\\sigma(z)\\rightarrow -\\infty\\) when \\(z\\mapsto -\\infty\\).\n\\(\\sigma(0)=0.5\\).\n\\(\\sigma(z)\\) is always increasing.\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\).\n\n\n\nSolution. We will only look at the last one.\n\\[\n\\begin{split}\n\\sigma'(z)&=-\\frac{(1+\\mathrm e^{-z})'}{(1+\\mathrm e^{-z})^2}=\\frac{\\mathrm e^{-z}}{(1+\\mathrm e^{-z})^2}=\\frac{1}{1+\\mathrm e^{-z}}\\frac{\\mathrm e^{-z}}{1+\\mathrm e^{-z}}\\\\\n&=\\sigma(z)\\left(\\frac{1+\\mathrm e^{-z}}{1+\\mathrm e^{-z}}-\\frac{1}{1+\\mathrm e^{-z}}\\right)=\\sigma(z)(1-\\sigma(z)).\n\\end{split}\n\\]\n\n\n\n5.1.2 Gradient descent\nAssume that we would like to minimize a function \\(J(\\Theta)\\), where this \\(\\Theta\\) is an \\(N\\)-dim vector. Geometricly, we could treat \\(J\\) as a height function, and it tells us the height of the mountain. Then to minimize \\(J\\) is the same thing as to find the lowest point. One idea is to move towards the lowest point step by step. During each step we only need to lower our current height. After several steps we will be around the lowest point.\nThe geometric meaning of \\(\\nabla J\\) is the direction that \\(J\\) increase the most. Therefore the opposite direction is the one we want to move in. The formula to update \\(x\\) is\n\\[\n\\Theta_{\\text{new}} = \\Theta_{\\text{old}}-\\alpha \\nabla J(\\Theta_{\\text{old}}),\n\\] where \\(\\alpha\\) is called the learning rate which controls how fast you want to learn. Usually if \\(\\alpha\\) is small, the learning tends to be slow and stble, and when \\(\\alpha\\) is big, the learning tends to be fast and unstable.\nIn machine learning, in most cases we would like to formulate the problem in terms of finding the lowest point of a cost function \\(J(\\Theta)\\). Then we could start to use Logistic regression to solve it. For binary classification problem, the cost function is defined to be\n\\[\nJ(\\Theta)=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\log(p^{(i)})+(1-y^{(i)})\\log(1-p^{(i)})\\right].\n\\] Here \\(m\\) is the number of data points, \\(y^{(i)}\\) is the labelled result (which is either \\(0\\) or \\(1\\)), \\(p^{(i)}\\) is the predicted value (which is between \\(0\\) and \\(1\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe algorithm gets its name since we are using the gradient to find a direction to lower our height.\n\n\n\n\n5.1.3 The Formulas\n\nTheorem 5.1 The gradient of \\(J\\) is computed by\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\tag{5.1}\\]\n\n\n\nClick for details.\n\n\nProof. The formula is an application of the chain rule for the multivariable functions.\n\\[\n\\begin{split}\n\\dfrac{\\partial p}{\\partial \\theta_k}&=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma(L(\\Theta))\\\\\n&=\\sigma(L)(1-\\sigma(L))\\dfrac{\\partial}{\\partial \\theta_k}\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)\\\\\n&=\\begin{cases}\np(1-p)&\\text{ if }k=0,\\\\\np(1-p)x_k&\\text{ otherwise}.\n\\end{cases}\n\\end{split}\n\\] Then\n\\[\n\\nabla p = \\left(\\frac{\\partial p}{\\partial\\theta_0},\\ldots,\\frac{\\partial p}{\\partial\\theta_n}\\right) = p(1-p)\\hat{x}.\n\\]\nThen\n\\[\n\\nabla \\log(p) = \\frac{\\nabla p}p =\\frac{p(1-p)\\hat{x}}{p}=(1-p)\\hat{x}.\n\\]\n\\[\n\\nabla \\log(1-p) = \\frac{-\\nabla p}{1-p} =-\\frac{p(1-p)\\hat{x}}{1-p}=-p\\hat{x}.\n\\]\nThen\n\\[\n\\begin{split}\n\\nabla J& = -\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\nabla \\log(p^{(i)})+(1-y^{(i)})\\nabla \\log(1-p^{(i)})\\right]\\\\\n&=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}(1-p^{(i)})\\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\\hat{x}^{(i)})\\right]\\\\\n&=-\\frac1m\\sum_{i=1}^m\\left[(y^{(i)}-p^{(i)})\\hat{x}^{(i)}\\right].\n\\end{split}\n\\]\nWe write \\(\\hat{x}^{(i)}\\) as row vectors, and stack all these row vectors vertically. What we get is a matrix \\(\\hat{\\textbf X}\\) of the size \\(m\\times (1+n)\\). We stack all \\(y^{(i)}\\) (resp. \\(p^{(i)}\\)) vectically to get the \\(m\\)-dim column vector \\(\\textbf y\\) (resp. \\(\\textbf p\\)).\nUsing this notation, the previous formula becomes\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\]\nAfter the gradient can be computed, we can start to use the gradient descent method. Note that, although \\(\\Theta\\) are not explicitly presented in the formula of \\(\\nabla J\\), this is used to modify \\(\\Theta\\):\n\\[\n\\Theta_{s+1} = \\Theta_s - \\alpha\\nabla J.\n\\]\n\n\n\n\n5.1.4 Codes\nWe will only talk about using packages. sklearn provides two methods to implement the Logistic regression. The API interface is very similar to other models.\nNote that Logistic regression is very sensitive to the scale of features. Therefore we need to normalize the features before throwing them into the model.\nLet’s still take iris as an example.\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nThe first method is sklearn.linear_model.LogisticRegression.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nsteps = [('normalize', MinMaxScaler()),\n         ('log', LogisticRegression())]\n\nlog_reg = Pipeline(steps=steps)\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_test, y_test)\n\n0.9130434782608695\n\n\nNote that this method has an option solver that will set the way to solve the Logistic regression problem, and there is no “stochastic gradient descent” provided. The default solver for this LogsiticRegression is lbfgs which will NOT be discussed in lectures.\nThe second method is sklearn.linear_model.SGDClassifier.\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nsteps = [('normalize', MinMaxScaler()),\n         ('log', SGDClassifier(loss='log_loss', max_iter=100))]\n\nsgd_clf = Pipeline(steps=steps)\nsgd_clf.fit(X_train, y_train)\nsgd_clf.score(X_test, y_test)\n\n0.8695652173913043\n\n\nThis method is the one we discussed in lectures. The log_loss loss function is the binary entropy function we mentioned in lectures. If you change to other loss functions it will become other models.\nFrom the above example, you may notice that SGDClassifier doesn’t perform as well as LogisticRegression. This is due to the algorithm. To make SGDClassifier better you need to tune the hyperparameters, like max_iter, learning_rate/alpha, penalty, etc..\n\n\n\n\n\n\nNote\n\n\n\nThe argument warm_start is used to set whether you want to use your previous model. When set to True, it will reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. The default is False.\nRepeatedly calling fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that for both methods, regularization (which will be discussed later) is applied by default.\n\n\n\n\n5.1.5 Several important side topics\n\n5.1.5.1 Epochs\nWe use epoch to describe feeding data into the model. One Epoch is when an entire dataset is passed through the model once. When using gradient descent, we tend to run several epochs. The number of maximal epochs is one important hyperparameter of this model.\nThe general idea is that more epochs are better for the score of the model, but it will definitely be slower. In addition, sometimes due to many other factors, after a few epochs, the model becomes stall. To train for more epochs cannot improve the model. In this case you have to turn to other methods.\n\n\n5.1.5.2 Batch Gradient Descent vs SGD vs Minibatch\nRecall the Formula Equation 5.1:\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\] We could rewrite this formula:\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}=\\frac1m\\sum_{i=1}^m\\left[(p^{(i)}-y^{(i)})\\hat{x}^{(i)}\\right].\n\\] This new formula can be understood in the following way: For every data point, we could get one gradient direction. Then \\(\\nabla J\\) is the average of all gradient directions. So this algorithm can be expressed as that compute the gradient for every data points and then take the average, and finally update the parameters once. This algorithm is called batch gradient descent.\nFollowing the idea, there is another way to update the model. For every data point, we could compute one gradient direction, and we could use the gradient direction to update the parameters of the model. This algorithm is called stochastic gradient descent.\nThen there is an algrothm living in the middle, called mini-batch gradient descent. In this case, we will group the data set into a collection of subsets of a fiexed number of training examples. Each subset is called a mini-batch, and the fixed number of elements of each mini-batch is called the batch size. Using this method, we will just go through mini-batches one at a time, compute the average of the gradient for these data, and then update the parameters of the model after we finish one mini-batch. Assume that the total number of the dataset is N, the mini-batch size is m. Then there are N/m mini-batches, and during one epoch we will update the model N/m times.\nMini-batch size is one important hyperparameters of this model. Usually the larger the batch size is, the less variance the model has. Then it tends to behave more smoothly, but it will also be slower, and might be stuck to a local minimal. The smaller batch size is more chaotic. It might go faster, but it tends not to converge.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#regularization",
    "href": "contents/5/intro.html#regularization",
    "title": "5  Logistic regression",
    "section": "5.2 Regularization",
    "text": "5.2 Regularization\n\n5.2.1 Three types of errors\nEvery estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.\n\n\n5.2.2 Underfit vs Overfit\nWhen fit a model to data, it is highly possible that the model is underfit or overfit.\nRoughly speaking, underfit means the model is not sufficient to fit the training samples, and overfit means that the models learns too many noise from the data. In many cases, high bias is related to underfit, and high variance is related to overfit.\nThe following example is from the sklearn guide. Although it is a polynomial regression example, it grasps the key idea of underfit and overfit.\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Learning curves (accuracy vs training size)\nA learning curve shows the validation and training score of an estimator for varying a key hyperparameter. In most cases the key hyperparameter is the training size or the number of epochs. It is a tool to find out how much we benefit from altering the hyperparameter by training more data or training for more epochs, and whether the estimator suffers more from a variance error or a bias error.\nsklearn provides sklearn.model_selection.learning_curve() to generate the values that are required to plot such a learning curve. However this function is just related to the sample size. If we would like to talk about epochs, we need other packages.\nLet us first look at the learning curve about sample size. The official document page is here. The function takes input estimator, dataset X, y, and an arry-like argument train_sizes. The dataset (X, y) will be split into pieces using the cross-validation technique. The number of pieces is set by the argument cv. The default value is cv=5. For details about cross-validation please see Section 2.2.6.\nThen the model is trained over a random sample of the training set, and evaluate the score over the test set. The size of the sample of the training set is set by the argument train_sizes. This argument is array-like. Therefore the process will be repeated several times, and we can see the impact of increasing the training size.\nThe output contains three pieces. The first is train_sizes_abs which is the number of elements in each training set. This output is mainly for reference. The difference between the output and the input train_sizes is that the input can be float which represents the percentagy. The output is always the exact number of elements.\nThe second output is train_scores and the third is test_scores, both of which are the scores we get from the training and testing process. Note that both are 2D numpy arrays, of the size (number of different sizes, cv). Each row is a 1D numpy array representing the cross-validation scores, which is corresponding to a train size. If we want the mean as the cross-validation score, we could use train_scores.mean(axis=1).\nAfter understanding the input and output, we could plot the learning curve. We still use the horse colic as the example. The details about the dataset can be found here.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nWe use the model LogisticRegression. The following code plot the learning curve for this model.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nclf = LogisticRegression(max_iter=1000)\nsteps = [('scalar', MinMaxScaler()),\n         ('log', clf)]\npipe = Pipeline(steps=steps)\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\ntrain_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n                                                        train_sizes=np.linspace(0.1, 1, 20))\n\nimport matplotlib.pyplot as plt\nplt.plot(train_sizes, train_scores.mean(axis=1), label='train')\nplt.plot(train_sizes, test_scores.mean(axis=1), label='test')\nplt.legend()\n\n\n\n\n\n\n\n\nThe learning curve is a primary tool for us to study the bias and variance. Usually\n\nIf the two training curve and the testing curve are very close to each other, this means that the variance is low. Otherwise the variance is high, and this means that the model probabily suffer from overfitting.\nIf the absolute training curve score is high, this means that the bias is low. Otherwise the bias is high, and this means that the model probabily suffer from underfitting.\n\nIn the above example, although regularization is applied by default, you may still notice some overfitting there.\n\n\n5.2.4 Regularization\nRegularization is a technique to deal with overfitting. Here we only talk about the simplest method: ridge regression, also known as Tikhonov regularizaiton. Because of the formula given below, it is also called \\(L_2\\) regularization. The idea is to add an additional term \\(\\dfrac{\\alpha}{2m}\\sum_{i=1}^m\\theta_i^2\\) to the original cost function. When training with the new cost function, this additional term will force the parameters in the original term to be as small as possible. After finishing training, the additional term will be dropped, and we use the original cost function for validation and testing. Note that in the additional term \\(\\theta_0\\) is not presented.\nThe hyperparameter \\(\\alpha\\) is the regularization strength. If \\(\\alpha=0\\), the new cost function becomes the original one; If \\(\\alpha\\) is very large, the additional term dominates, and it will force all parameters to be almost \\(0\\). In different context, the regularization strength is also given by \\(C=\\dfrac{1}{2\\alpha}\\), called inverse of regularization strength.\n\n5.2.4.1 The math of regularization\n\nTheorem 5.2 The gradient of the ridge regression cost function is\n\\[\n\\nabla J=\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}+\\frac{\\alpha}{m}\\Theta.\n\\]\nNote that \\(\\Theta\\) doesn’t contain \\(\\theta_0\\), or you may treat \\(\\theta_0=0\\).\n\nThe computation is straightforward.\n\n\n5.2.4.2 The code\nRegularization is directly provided by the logistic regression functions.\n\nIn LogisticRegression, the regularization is given by the argument penalty and C. penalty specifies the regularizaiton method. It is l2 by default, which is the method above. C is the inverse of regularization strength, whose default value is 1.\nIn SGDClassifier, the regularization is given by the argument penalty and alpha. penalty is the same as that in LogisticRegression, and alpha is the regularization strength, whose default value is 0.0001.\n\nLet us see the above example.\n\nclf = LogisticRegression(max_iter=1000, C=0.1)\nsteps = [('scalar', MinMaxScaler()),\n         ('log', clf)]\npipe = Pipeline(steps=steps)\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\ntrain_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n                                                        train_sizes=np.linspace(0.1, 1, 20))\n\nimport matplotlib.pyplot as plt\nplt.plot(train_sizes, train_scores.mean(axis=1), label='train')\nplt.plot(train_sizes, test_scores.mean(axis=1), label='test')\nplt.legend()\n\n\n\n\n\n\n\n\nAfter we reduce C from 1 to 0.1, the regularization strength is increased. Then you may find that the gap between the two curves are reduced. However the overall performace is also reduced, from 85%~90% in C=1 case to around 80% in C=0.1 case. This means that the model doesn’t fit the data well as the previous one. Therefore this is a trade-off: decrease the variance but increase the bias.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#neural-network-implement-of-logistic-regression",
    "href": "contents/5/intro.html#neural-network-implement-of-logistic-regression",
    "title": "5  Logistic regression",
    "section": "5.3 Neural network implement of Logistic regression",
    "text": "5.3 Neural network implement of Logistic regression\nIn the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. But we didn’t implement them. In fact sklearn doesn’t provide a very good tool to do all these computations. Hence we turn to another package for this model. We are going to use keras to build a Logistic regression model, and plot the “loss vs epochs” learning curves.\nkeras is high level Neural network library. It is now in the phase of transition from single backend tensorflow to multi-backend. The old version is installed along with tensorflow. You may use the following command to install it.\npip install tensorflow\nIf you want to use new version, besides installing tensorflow, you should also install keras-core. Currently we still use tensorflow as the backend. This part may be updated after the stable version is released.\npip install keras-core\nYou may follow the instructions for tensforflow and keras for more details.\nTo use keras to implement logistic regression, we need the following modules: a Sequential model, a Dense layer. The model is organized as follows.\nWe still use the horse colic dataset as an example.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nNote that we need to perform normalization before throwing the data into the model. Here we use the MinMaxScaler() from sklearn package. The normalization layer in keras is a little bit more complicated and doesn’t fit into situation.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\nmms.fit(X_train)\nX_train = mms.transform(X_train)\nX_test = mms.transform(X_test)\n\nIn the following code, we first set up the model, and then add one Dense layer. This Dense layer means that we would perform a linear transformation to the input, by the formula \\(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\ldots+\\theta_nx_n\\). Then there are three arguments:\n\n1: means that there is only output.\nactivation='sigmoid': means that we will apply the sigmoid function after the linear transformation.\ninput_dim: means the dimension of the input. Note that this dimension is the dimension of one individual data point. You don’t take the size of the training set into consideration.\n\nAfter building the basic architectal of the model, we need to speicify a few more arguments. In the model.compile() step, we have to input the optimizer, the loss function (which is the binary_crossentropy in our case) and the metrics to test the performance of the model (which is accuracy in our case).\nThe optimizer is how the parameters are updated. The best choice in general is adam. The default setting is RMSprop and the optimizer discussed in our lecture is sgd. We will use adam here, since the learning curve it produces looks better (for illustration).\nFinally we could train the model. The argument is straightforward.\n\n# import keras_core as keras\nfrom keras import models, layers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(1, activation='sigmoid', input_dim=X_train.shape[1]))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=400, batch_size=30, validation_data=(X_test, y_test))\n\nNote that we assign the output of model.fit() to a variable hist. The infomation about this training process is recorded inside. We will extract those information.\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\nWe now could plot the learning curve (loss vs epochs) and the learning curve (accuracy vs epochs).\n\nimport matplotlib.pyplot as plt\nplt.plot(loss_train, label='train_loss')\nplt.plot(loss_val, label='val_loss')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(acc_train, label='train_acc')\nplt.plot(acc_val, label='val_acc')\nplt.legend()\n\n\n\n\n\n\n\n\n\n5.3.1 Regularization\nTo apply regularization, we just need to modify the layer we added to the model. The argument is kernel_regularizer. We would like to set it to be keras.regularizers.L2(alpha), where alpha is the regularization strength.\n\nfrom keras import models, layers, regularizers\n\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(1, activation='sigmoid', input_dim=X_train.shape[1],\n                       kernel_regularizer=regularizers.L2(0.5)))\n\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=400, batch_size=30,\n                 validation_data=(X_test, y_test))\n\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\nplt.plot(loss_train, label='train_loss')\nplt.plot(loss_val, label='val_loss')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(acc_train, label='train_acc')\nplt.plot(acc_val, label='val_acc')\nplt.legend()\n\n\n\n\n\n\n\n\nYou may compare what we get here with the codes we get before.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#pytorch-crash-course",
    "href": "contents/5/intro.html#pytorch-crash-course",
    "title": "5  Logistic regression",
    "section": "5.4 Pytorch crash course",
    "text": "5.4 Pytorch crash course\n\n5.4.1 Tensor\nThis is the basic data structure. It is very similar to numpy.ndarray, but with many more features. There are a few things that we need to mention at the beginning.\n\nA tensor with only one item is mathematically equal to a number. In Pytorch, you may use .item() to extract the number from a tensor with only one item.\n\n\nimport torch\n\na = torch.tensor([1])\na\n\ntensor([1])\n\n\n\na.item()\n\n1\n\n\n\nIt is type sensitive. Pytorch expect you to assign the exact data type to each tensor, and it won’t automatically guess it in most cases. You may specify data type when you create a tensor.\n\n\nb = torch.tensor([1], dtype=torch.float64)\nb\n\ntensor([1.], dtype=torch.float64)\n\n\nIf you want to convert data type, you could use .to().\n\nb = torch.tensor([1], dtype=torch.float64)\nb = b.to(torch.int)\nb\n\ntensor([1], dtype=torch.int32)\n\n\nTensor data structure has many other features that will be introduced later.\n\n\n5.4.2 Gradient descent\nTo implement the gradient descent algorithm for the neural network, there would be a series of computations:\n\nFrom the input, feedforward the network to get the output y_pred.\nBased on the real output y_true, compute the loss function loss = loss_fn(y_true, y_pred).\nCompute the gradient based on the information provided. For this step many data are needed. You may look up the gradient descent formula (backprop).\nBased on the gradient computed in Step 3, weights are updated, according to the optimizer we choose.\n\nIn Pytorch, the above steps are implemented as follows.\n\nYou have to define a model function to indicate how to feedforward the network to get an output. Here for a lot of reasons, the typical way is to define a model class, which contains a forward method that can compute the output of the model. Let us consider the following example: the dataset is as follows:\n\n\nx = torch.tensor([[1, 2], [3, 4], [0, 1]], dtype=torch.float)\ny = torch.tensor([[3], [7], [1]], dtype=torch.float)\n\nThe model is defined as follows.\n\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n\n        self.fc = nn.Linear(in_features=2, out_features=1)\n    \n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\nIn this example, we define a 2-input linear regression model. Pytorch doesn’t need the class to work. Actually the minimal working example of the above code is as follows. To put things into a class can make it easier in larger models.\n\ndef model(x):\n    return nn.Linear(in_features=2, out_features=1)(x)\n\nThe reason the model can be written in a very simple way is because the information about computing gradients is recorded in the parameter tensors, on the level of tensors, instead of on the level of the model class. Therefore it is important to get access to the parameters of the model.\n\nmodel = MyModel()\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[-0.7063,  0.5948]], requires_grad=True),\n Parameter containing:\n tensor([-0.6278], requires_grad=True)]\n\n\nNote that the parameters we get here is a iterator. So to look at it we need to convert it inot a list. In this example, there are two sets of tensors: the first is the coefficients, and the second is the bias term. This bias term can be turned on/off by setting the argument bias=True or False when using nn.Linear() to create fully connected layers. The default is True.\nTo evaluate the model, we just directly apply the model to the input tensor.\n\ny_pred = model(x)\ny_pred\n\ntensor([[-0.1445],\n        [-0.3675],\n        [-0.0330]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nYou may use the coefficients provided above to validate the resutl.\nNote that, although we define the .forward() method, we don’t use it explicitly. The reason is that model(x) will not only excute .forward(x) method, but many other operations, like recording many intermediate results that can be used for debugging, visualization and modifying gradients.\n\nWe may define the loss function. We mannually define the MSE loss function.\n\n\ndef loss_fn(y_true, y_pred):\n    return ((y_true-y_pred)**2).mean()\n\nloss = loss_fn(y, y_pred)\nloss\n\ntensor(21.7448, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nNow we need to do gradient descent. The manual way to loss.backward(). What it does is to\n\n\nprint(list(model.parameters()))\nprint(list(model.parameters())[0].grad)\nprint(list(model.parameters())[1].grad)\n\n[Parameter containing:\ntensor([[-0.7063,  0.5948]], requires_grad=True), Parameter containing:\ntensor([-0.6278], requires_grad=True)]\nNone\nNone\n\n\n\nimport torch.optim as optim\noptimizer = optim.SGD(model.parameters(), lr=1e-2)\n\n\nloss.backward()\noptimizer.step()\n\n\nfor i in range(100):\n    optimizer.zero_grad()\n    # print(optimizer.param_groups)\n\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n\n    # print(optimizer.param_groups)\n\n    loss.backward()\n    optimizer.step()\n\n    # print(optimizer.param_groups)\n\n    # print(list(model.parameters()))\n    # print(list(model.parameters())[0].grad)\n    # print(list(model.parameters())[1].grad)\n\n\nUpdate the parameters by optim or manually done.\n\n\n\n5.4.3 Mini-batch",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#multi-class-case",
    "href": "contents/5/intro.html#multi-class-case",
    "title": "5  Logistic regression",
    "section": "5.5 Multi class case",
    "text": "5.5 Multi class case\n\n5.5.1 Naive idea (one-vs-all)\nAssume that there are \\(N\\) classes. The naive idea is to decompose this \\(N\\)-class classification problem into \\(N\\) binary classification problems. The model will contains \\(N\\) classifiers. The \\(i\\)th classifer is used to classify whehter the given features has the label i or not.\nFor example, asusme we are dealing with the iris dataset. There are 3 classes. By the naive idea, we will modify the labels into Setosa and not Setosa, and use it to train the first classifier. Similarly we can have two more classifiers to tell Versicolour/not Versicolour and Virginica/not Virginica. Then we combine all three classifiers to get a final classifier to put the data into one of the three classes.\n\n\n5.5.2 Softmax function\nA better method is mimic the sigmoid function. Recall that in binary classification problem, after\n\\[\nz=L(x)=\\theta_0+\\sum_{i=1}^n\\theta_ix_i,\n\\] the sigmoid function is applied \\(p=\\sigma(z)\\). This \\(p\\) is interepreted as the probability for the data belonging to class \\(1\\). For \\(N\\)-class problem, we could generalize the sigmoid function to softmax function, whose value is a \\(N\\)-dimensional vector \\(p=[p_k]_{i=1}^N\\). Here \\(p_k\\) represents the probability for the data belonging to class \\(k\\). Then after we get the vector \\(p\\), we then find the highest probability and that indicates the class of the data point.\nThe softmax function is defined in the following way:\n\\[\np_k=\\sigma(z)=\\dfrac{\\exp(z_k)}{\\sum_{i=1}^N\\exp(z_i)},\\quad \\text{ for }z=[z_1, z_2,\\ldots,z_N].\n\\] In the model, each \\(z_i=L_i(x)=\\theta^{(i)}_0+\\sum_{i=1}^n\\theta^{(i)}_ix_i,\\) has its own weights.\nThe related cost function is also updated:\n\\[\nJ(\\Theta)=-\\sum_{i=1}^Ny_k\\ln(p_i).\n\\] Therefore the same gradient descent algorithm can be applied.\n\n\n\n\n\n\nNote\n\n\n\nNote that sigmoid function and the binary crossentropy cost functions are the special case of softmax function.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe labels are not recorded as labels, but as vectors. This is called dummy variables, or one-hot encodings.\n\n\n\n\n5.5.3 Codes\n\nBoth LogisticRegression() and SGDClassifier() by default uses the one-vs-all naive idea.\nUsing kears, softmax can be implemented. The key configuration is the loss function loss='categorical_crossentropy' and the activation function softmax. Note that in this case the labels should be translated into one-hot vectors.\n\nWe use make_classification as an example. To save time we won’t carefully tune the hyperparameters here.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=2, n_repeated=2, n_classes=3, n_clusters_per_class=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nAlthough there are totally 10 features, the dataset can be visualized using the informative features. By description, the informative features are the first two.\n\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.8466666666666667\n\n\n\nfrom sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.8266666666666667\n\n\nTo apply keras package, we should first change y into one-hot vectors. Here we use the function provided by keras.\n\n# import keras_core as keras\nfrom keras.utils import to_categorical\nfrom keras import models, layers\n\nvy_train = to_categorical(y_train, num_classes=3)\nvy_test = to_categorical(y_test, num_classes=3)\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(3, activation='softmax', input_dim=10))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(X_train, vy_train, epochs=50, batch_size=50, verbose=0)\n_ = model.evaluate(X_test, vy_test)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#exercises-and-projects",
    "href": "contents/5/intro.html#exercises-and-projects",
    "title": "5  Logistic regression",
    "section": "5.6 Exercises and Projects",
    "text": "5.6 Exercises and Projects\n\nExercise 5.1 Please hand write a report about the details of the math formulas for Logistic regression.\n\n\nExercise 5.2 CHOOSE ONE: Please use sklearn to apply the LogisticRegression to one of the following datasets. You may either use LogisticRegression or SGDClassifier.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nPlot the learning curve (accuracy vs training sizes).\n\n\n\nExercise 5.3 CHOOSE ONE: Please use keras to apply the LogisticRegression to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nWhat is the batch size do you use?\nPlot the learning curve (loss vs epochs, accuracy vs epochs).\nAnalyze the bias / variance status.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html",
    "href": "contents/6/intro.html",
    "title": "6  Netural networks",
    "section": "",
    "text": "6.1 Neural network: Back propagation\nTo train a MLP model, we still use gradient descent. Therefore it is very important to know how to compute the gradient. Actually the idea is the same as logistic regreesion. The only issue is that now the model is more complicated. The gradient computation is summrized as an algorithm called back propagation. It is described as follows.\nHere is an example of a Neural network with one hidden layer.\n\\(\\Theta\\) is the coefficients of the whole Neural network.\nThe dependency is as follows:\nEach layer is represented by the following diagram:\nThe diagram says:\n\\[\nz^{(k+1)}=b^{(k)}+\\Theta^{(k)}a^{(k)},\\quad z^{(k+1)}_j=b^{(k)}_j+\\sum \\Theta^{(k)}_{jl}a^{(k)}_l,\\quad a^{(k)}_j=\\sigma(z^{(k)}_j).\n\\]\nAssume \\(r,j\\geq1\\). Then\n\\[\n\\begin{aligned}\n\\diffp{z^{(k+1)}_i}{a^{(k)}_r}&=\\diffp*{\\left(b^{(k)}_i+\\sum\\Theta^{(k)}_{il}a^{(k)}_l\\right)}{a^{(k)}_r}=\\Theta_{ir}^{(k)},\\\\\n% \\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{ij}}&=\\diffp*{\\qty(a^{(k)}_0+\\sum\\Theta^{(k)}_{il}a^{(k)}_l)}{\\Theta^{(k)}_{ij}}=a^{(k)}_j,\\\\\n\\diffp{z^{(k+1)}_i}{z^{(k)}_j}&=\\sum_r \\diffp{z^{(k+1)}_i}{a^{k}_r}\\diffp{a^{(k)}_r}{z^{(k)}_j}+\\sum_{p,g}\\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{pq}}\\diffp{\\Theta^{(k)}_{pq}}{z^{(k)}_j}+\\sum_r \\diffp{z^{(k+1)}_i}{b^{k}_r}\\diffp{b^{(k)}_r}{z^{(k)}_j}\\\\\n&=\\sum_r \\Theta^{(k)}_{ir}\\diffp{a^{(k)}_r}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\diffp{a^{(k)}_j}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\sigma'(z^{(k)}_j),\\\\\n\\diffp{J}{z^{(k)}_j}&=\\sum_r \\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{z^{(k)}_j}=\\sum_r\\diffp{J}{z^{(k+1)}_r}\\Theta^{(k)}_{rj}\\sigma'(z^{(k)}_j).\n\\end{aligned}\n\\]\nWe set\nThen we have the following formula. Note that there are ``\\(z_0\\)’’ terms.\n\\[\n    \\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k).\n\\]\n\\[\n\\begin{aligned}\n\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}&=\\diffp*{\\left(b^{(k)}_r+\\sum_l\\Theta^{(k)}_{rl}a^{(k)}_l\\right)}{\\Theta^{(k)}_{pq}}=\\begin{cases}\n0&\\text{ for }r\\neq q,\\\\\na^{(k)}_q&\\text{ for }r=q,\n\\end{cases}\\\\\n\\diffp{J}{\\Theta^{(k)}_{pq}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}=\\diffp{J}{z^{(k+1)}_p}\\diffp{z^{(k+1)}_p}{\\Theta^{(k)}_{pq}}=\\delta^{k+1}_pa^{k}_q,\\\\\n\\diffp{J}{b^{(k)}_{j}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}\\diffp{z^{(k+1)}_j}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}=\\delta^{k+1}_j.\n\\end{aligned}\n\\]\nExtend \\(\\hat{\\Theta}=\\left[b^{(k)},\\Theta^{(k)}\\right]\\), and \\(\\partial^k J=\\left[\\diffp{J}{\\hat{\\Theta}^{(k)}_{ij}}\\right]\\). Then \\[\n    \\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right].\n\\] Then the algorithm is as follows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#neural-network-back-propagation",
    "href": "contents/6/intro.html#neural-network-back-propagation",
    "title": "6  Netural networks",
    "section": "",
    "text": "\\[\n\\newcommand\\diffp[2]{\\dfrac{\\partial #1}{\\partial #2}}\n\\]\n\n\n\n\n\n\n\\(a^{(1)}=\\hat{\\textbf{x}}\\) is the input. \\(a_0^{(1)}\\) is added. This is an \\((n+1)\\)-dimension column vector.\n\\(\\Theta^{(1)}\\) is the coefficient matrix from the input layer to the hidden layer, of size \\(k\\times(n+1)\\).\n\\(z^{(2)}=\\Theta^{(1)}a^{(1)}\\).\n\\(a^{(2)}=\\sigma(z^{(2)})\\), and then add \\(a^{(2)}_0\\). This is an \\((k+1)\\)-dimension column vector.\n\\(\\Theta^{(2)}\\) is the coefficient matrix from the hidden layer to the output layer, of size \\(r\\times(k+1)\\).\n\\(z^{(3)}=\\Theta^{(2)}a^{(2)}\\).\n\\(a^{(3)}=\\sigma(z^{(3)})\\). Since this is the output layer, \\(a^{(3)}_0\\) won’t be added. %\nThese \\(a^{(3)}\\) are \\(h_{\\Theta}(\\textbf{x})\\).\n\n\n\n\\(J\\) depends on \\(z^{(3)}\\) and \\(a^{(3)}\\).\n\\(z^{(3)}\\) and \\(a^{(3)}\\) depends on \\(\\Theta^{(2)}\\) and \\(a^{(2)}\\).\n\\(z^{(2)}\\) and \\(a^{(2)}\\) depends on \\(\\Theta^{(1)}\\) and \\(a^{(1)}\\).\n\\(J\\) depends on \\(\\Theta^{(1)}\\), \\(\\Theta^{(2)}\\) and \\(a^{(1)}\\).\n\n\n\n\n\n\n\n\n\n\\(\\delta^k_j=\\diffp{J}{z^{(k)}_j}\\), \\(\\delta^k=\\left[\\delta^k_1,\\delta_2^k,\\ldots\\right]^T\\).\n\\(\\mathbf{z}^k=\\left[z^{(k)}_1,z^{(k)}_2,\\ldots\\right]^T\\), \\(\\mathbf{a}^k=\\left[a^{(k)}_1,a^{(k)}_2,\\ldots\\right]^T\\), \\(\\hat{\\mathbf{a}}^k=\\left[a^{(k)}_0,a^{(k)}_1,\\ldots\\right]^T\\).\n\\(\\Theta^{k}=\\left[\\Theta^{(k)}_{ij}\\right]\\).\n\n\n\n\n\n\nStarting from \\(x\\), \\(y\\) and some random \\(\\Theta\\).\nForward computation: compute \\(z^{(k)}\\) and \\(a^{(k)}\\). The last \\(a^{(n)}\\) is \\(h\\).\nCompute \\(\\delta^n=\\nabla J\\circ\\sigma'(z^{(n)})\\). In the case of \\(J=\\frac12||{h-y}||^2\\), \\(\\nabla J=(a^{(n)}-y)\\), and then \\(\\delta^n=(a^{(n)}-y)\\circ\\sigma'(z^{(n)})\\).\nBackwards: \\(\\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k)\\), and \\(\\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right]\\) .\n\n\nExample 6.1 Consider there are 3 layers: input, hidden and output. There are \\(m+1\\) nodes in the input layer, \\(n+1\\) nodes in the hidden layer and \\(k\\) in the output layer. Therefore\n\n\\(a^{(1)}\\) and \\(\\delta^1\\) are \\(m\\)-dim column vectors.\n\\(z^{(2)}\\), \\(a^{(2)}\\) and \\(\\delta^2\\) are \\(n\\)-dim column vectors.\n\\(z^{(3)}\\), \\(a^{(3)}\\) and \\(\\delta^3\\) are \\(k\\)-dim column vectors.\n\\(\\hat{\\Theta}^1\\) is \\(n\\times(m+1)\\), \\(\\hat{\\Theta}^2\\) is \\(k\\times(n+1)\\).\n\\(z^{(2)}=b^{(1)}+\\Theta^{(1)}a^{(1)}=\\hat{\\Theta}^{(1)}\\hat{a}^{(1)}\\), \\(z^{(3)}=b^{(2)}+\\Theta^{(2)}a^{(2)}=\\hat{\\Theta}^{(2)}\\hat{a}^{(2)}\\).\n\\(\\delta^3=\\nabla_aJ\\circ\\sigma'(z^{(3)})\\). This is a \\(k\\)-dim column vector.\n\\(\\partial^2 J=\\left[\\delta^3,\\delta^3(a^{(2)})^T\\right]\\).\n\\(\\delta^2=\\left[(\\Theta^2)^T\\delta^3\\right]\\circ \\sigma'(z^{(2)})\\), where \\((\\hat{\\Theta^2})^T\\delta^3=(\\hat{\\Theta^2})^T\\delta^3\\) and then remove the first row.\n\\(\\delta^1=\\begin{bmatrix}(\\Theta^1)^T\\delta^2\\end{bmatrix}\\circ \\sigma'(z^{(1)})\\), where \\((\\hat{\\Theta^1})^T\\delta^2=(\\hat{\\Theta^1})^T\\delta^2\\) and then remove the first row.\n\\(\\partial^1 J=\\left[\\delta^2,\\delta^2(a^{(1)})^T\\right]\\).\nWhen \\(J=-\\frac1m\\sum y\\ln a+(1-y)\\ln(1-a)\\), \\(\\delta^3=\\frac1m(\\sum a^{(3)}-\\sum y)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#example",
    "href": "contents/6/intro.html#example",
    "title": "6  Netural networks",
    "section": "6.2 Example",
    "text": "6.2 Example\nLet us take some of our old dataset as an example. This is an continuation of the horse colic dataset from Logistic regression.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\nmms.fit(X_train)\nX_train = mms.transform(X_train)\nX_test = mms.transform(X_test)\n\nNow we build a neural network. This is a 2-layer model, with 1 hidden layer with 10 nodes.\n\n# import keras_core as keras\nfrom keras import models, layers, Input\nmodel = models.Sequential()\n\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(layers.Dense(10, activation='sigmoid'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\nAnd the learning curve are shown in the following plots.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nIt seems that our model has overfitting issues. Therefore we need to modifify the architects of our model. The first idea is to add L2 regularization as we talked about it in LogsiticRegression case. Here we use 0.01 as the regularization strenth.\nLet us add the layer to the model and retrain it.\n\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1], kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAnother way to deal with overfitting is to add a Dropout layer. The idea is that when training the model, part of the data will be randomly discarded. Then after fitting, the model tends to reduce the variance, and then reduce the overfitting.\nThe code of a Dropout layer is listed below. Note that the number represents the percentage of the training data that will be dropped.\n\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAfter playing with different hyperparameters, the overfitting issues seem to be better (but not entirely fixed). However, the overall performance is getting worse. This means that the model is moving towards underfitting side. Then we may add more layers to make the model more complicated in order to capture more information.\n\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAs you may see, to build a netural network model it requires many testing. There are many established models. When you build your own architecture, you may start from there and modify it to fit your data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#exercises-and-projects",
    "href": "contents/6/intro.html#exercises-and-projects",
    "title": "6  Netural networks",
    "section": "6.3 Exercises and Projects",
    "text": "6.3 Exercises and Projects\n\nExercise 6.1 Please hand write a report about the details of back propagation.\n\n\nExercise 6.2 CHOOSE ONE: Please use netural network to one of the following datasets. - the iris dataset. - the dating dataset. - the titanic dataset.\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nWhat is the batch size do you use?\nPlot the learning curve (loss vs epochs, accuracy vs epochs).\nAnalyze the bias / variance status.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html",
    "href": "contents/intro2pt/intro.html",
    "title": "5  Intro to Pytorch",
    "section": "",
    "text": "5.1 Linear regression (math)\nWe only consider the simplest case: simple linear regression (SLR). The idea is very simple. The dataset contains two variables (the independent variable \\(x\\) and the response variable \\(y\\).) The goal is to find the relation between \\(x\\) and \\(y\\) with the given dataset. We assume their relation is \\(y=b+wx\\). How do we find \\(b\\) and \\(w\\)?\nLet us first see an example. We would like to find the red line (which is the best fitted curve) shown below.\nThe key here is to understand the idea of “parameter space”. Since we already know that the function we are looking for has a formula \\(y=b+wx\\), we could use the pair \\((b, w)\\) to denote different candidates of our answer. For example, the following plot show some possibilities in green dashed lines, while each possiblity is denoted by \\((b, w)\\).\nNow we would like to find the best pair such that it fit the data best. This “best” is ususall defined",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-pytorch-version",
    "href": "contents/intro2pt/intro.html#linear-regression-pytorch-version",
    "title": "5  Intro to Pytorch",
    "section": "5.3 Linear regression (Pytorch version)",
    "text": "5.3 Linear regression (Pytorch version)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#exercises",
    "href": "contents/intro2pt/intro.html#exercises",
    "title": "5  Intro to Pytorch",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\ndsf\n\n\n\n\n\n[1] Godoy, D. V. (2022). Deep learning with PyTorch step-by-step: A beginner’s guide. https://leanpub.com/pytorch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-numpy-version",
    "href": "contents/intro2pt/intro.html#linear-regression-numpy-version",
    "title": "5  Intro to Pytorch",
    "section": "5.2 Linear regression (numpy version)",
    "text": "5.2 Linear regression (numpy version)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#rewrite-in-classes",
    "href": "contents/intro2pt/intro.html#rewrite-in-classes",
    "title": "5  Intro to Pytorch",
    "section": "5.4 Rewrite in classes",
    "text": "5.4 Rewrite in classes\ns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-numpy",
    "href": "contents/intro2pt/intro.html#linear-regression-numpy",
    "title": "5  Intro to Pytorch",
    "section": "5.2 Linear regression (numpy)",
    "text": "5.2 Linear regression (numpy)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-pytorch",
    "href": "contents/intro2pt/intro.html#linear-regression-pytorch",
    "title": "5  Intro to Pytorch",
    "section": "5.3 Linear regression (PyTorch)",
    "text": "5.3 Linear regression (PyTorch)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  }
]