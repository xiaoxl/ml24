[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Fall 2024",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 4803/5803 Machine Learning Fall 2024 at ATU. If you have any comments/suggetions/concers about the notes please contact us at xxiao@atu.edu.\n\n\nReferences\n\n\n[1] Chollet, F.\n(2021). Deep\nlearning with python, second edition. MANNING PUBN.\n\n\n[2] Géron, A.\n(2019). Hands-on machine learning with scikit-learn, keras, and\nTensorFlow concepts, tools, and techniques to build intelligent systems:\nConcepts, tools, and techniques to build intelligent systems.\nO’Reilly Media.\n\n\n[3] Harrington, P.\n(2012). Machine\nlearning in action. Manning Publications.\n\n\n[4] Klosterman, S.\n(2021). Data\nscience projects with python: A case study approach to gaining valuable\ninsights from real data with machine learning. Packt\nPublishing, Limited.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/1/intro.html",
    "href": "contents/1/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Machine Learning?\nMachine Learning is the science (and art) of programming computers so they can learn from data [1].\nHere is a slightly more general definition:\nThis “without being explicitly programmed to do so” is the essential difference between Machine Learning and usual computing tasks. The usual way to make a computer do useful work is to have a human programmer write down rules — a computer program — to be followed to turn input data into appropriate answers. Machine Learning turns this around: the machine looks at the input data and the expected task outcome, and figures out what the rules should be. A Machine Learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task [2].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#what-is-machine-learning",
    "href": "contents/1/intro.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "",
    "text": "[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n                                                   -- Arthur Samuel, 1959",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#types-of-machine-learning-systems",
    "href": "contents/1/intro.html#types-of-machine-learning-systems",
    "title": "1  Introduction",
    "section": "1.2 Types of Machine Learning Systems",
    "text": "1.2 Types of Machine Learning Systems\nThere are many different types of Machine Learning systems that it is useful to classify them in braod categories, based on different criteria. These criteria are not exclusive, and you can combine them in any way you like.\nThe most popular criterion for Machine Learning classification is the amount and type of supervision they get during training. In this case there are four major types.\nSupervised Learning The training set you feed to the algorithm includes the desired solutions. The machines learn from the data to alter the model to get the desired output. The main task for Supervised Learning is classification and regression.\nUnsupervised Learning In Unsupervised Learning, the data provided doesn’t have class information or desired solutions. We just want to dig some information directly from those data themselves. Usually Unsupervised Learning is used for clustering and dimension reduction.\nReinforcement Learning In Reinforcement Learning, there is a reward system to measure how well the machine performs the task, and the machine is learning to find the strategy to maximize the rewards. Typical examples here include gaming AI and walking robots.\nSemisupervised Learning This is actually a combination of Supervised Learning and Unsupervised Learning, that it is usually used to deal with data that are half labelled.\n\n1.2.1 Tasks for Supervised Learning\nAs mentioned above, for Supervised Learning, there are two typical types of tasks:\nClassification It is the task of predicting a discrete class labels. A typical classification problem is to see an handwritten digit image and recognize it.\nRegression It is the task of predicting a continuous quantity. A typical regression problem is to predict the house price based on various features of the house.\nThere are a lot of other tasks that are not directly covered by these two, but these two are the most classical Supervised Learning tasks.\n\n\n\n\n\n\nNote\n\n\n\nIn this course we will mainly focus on Supervised Classification problems.\n\n\n\n\n1.2.2 Classification based on complexity\nAlong with the popularity boost of deep neural network, there comes another classificaiton: shallow learning vs. deep learning. Basically all but deep neural network belongs to shallow learning. Although deep learning can do a lot of fancy stuffs, shallow learning is still very good in many cases. When the performance of a shallow learning model is good enough comparing to that of a deep learning model, people tend to use the shallow learning since it is usually faster, easier to understand and easier to modify.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "href": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "title": "1  Introduction",
    "section": "1.3 Basic setting for Machine learning problems",
    "text": "1.3 Basic setting for Machine learning problems\n\n\n\n\n\n\nNote\n\n\n\nWe by default assume that we are dealing with a Supervised Classification problem.\n\n\n\n1.3.1 Input and output data structure\nSince we are dealing with Supervised Classification problems, the desired solutions are given. These desired solutions in Classification problems are also called labels. The properties that the data are used to describe are called features. Both features and labels are usually organized as row vectors.\n\nExample 1.1 The example is extracted from [3]. There are some sample data shown in the following table. We would like to use these information to classify bird species.\n\n\n\n\nTable 1.1: Bird species classification based on four features\n\n\n\n\n\n\n\n\nWeight (g)\nWingspan (cm)\nWebbed feet?\nBack color\nSpecies\n\n\n\n\n1000.100000\n125.000000\nNo\nBrown\nButeo jamaicensis\n\n\n3000.700000\n200.000000\nNo\nGray\nSagittarius serpentarius\n\n\n3300.000000\n220.300000\nNo\nGray\nSagittarius serpentarius\n\n\n4100.000000\n136.000000\nYes\nBlack\nGavia immer\n\n\n3.000000\n11.000000\nNo\nGreen\nCalothorax lucifer\n\n\n570.000000\n75.000000\nNo\nBlack\nCampephilus principalis\n\n\n\n\n\n\n\n\nThe first four columns are features, and the last column is the label. The first two features are numeric and can take on decimal values. The third feature is binary that can only be \\(1\\) (Yes) or \\(0\\) (No). The fourth feature is an enumeration over the color palette. You may either treat it as categorical data or numeric data, depending on how you want to build the model and what you want to get out of the data. In this example we will use it as categorical data that we only choose it from a list of colors (\\(1\\) — Brown, \\(2\\) — Gray, \\(3\\) — Black, \\(4\\) — Green).\nThen we are able to transform the above data into the following form:\n\n\n\nTable 1.2: Vectorized Bird species data\n\n\n\n\n\nFeatures\nLabels\n\n\n\n\n\\(\\begin{bmatrix}1001.1 & 125.0 & 0 & 1 \\end{bmatrix}\\)\n\\(1\\)\n\n\n\\(\\begin{bmatrix}3000.7 & 200.0 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}3300.0 & 220.3 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}4100.0 & 136.0 & 1 & 3 \\end{bmatrix}\\)\n\\(3\\)\n\n\n\\(\\begin{bmatrix}3.0 & 11.0 & 0 & 4 \\end{bmatrix}\\)\n\\(4\\)\n\n\n\\(\\begin{bmatrix}570.0 & 75.0 & 0 & 3 \\end{bmatrix}\\)\n\\(5\\)\n\n\n\n\n\n\nThen the Supervised Learning problem is stated as follows: Given the features and the labels, we would like to find a model that can classify future data.\n\n\n\n1.3.2 Parameters and hyperparameters\nA model parameter is internal to the model and its value is learned from the data.\nA model hyperparameter is external to the model and its value is set by people.\nFor example, assume that we would like to use Logistic regression to fit the data. We set the learning rate is 0.1 and the maximal iteration is 100. After the computations are done, we get a the model\n\\[\ny = \\sigma(0.8+0.7x).\n\\] The two cofficients \\(0.8\\) and \\(0.7\\) are the parameters of the model. The model Logistic regression, the learning rate 0.1 and the maximal iteration 100 are all hyperparametrs. If we change to a different set of hyperparameters, we may get a different model, with a different set of parameters.\nThe details of Logistic regression will be discussed later.\n\n\n1.3.3 Evaluate a Machine Learning model\nOnce the model is built, how do we know that it is good or not? The naive idea is to test the model on some brand new data and check whether it is able to get the desired results. The usual way to achieve it is to split the input dataset into three pieces: training set, validation set and test set.\nThe model is initially fit on the training set, with some arbitrary selections of hyperparameters. Then hyperparameters will be changed, and new model is fitted over the training set. Which set of hyperparameters is better? We then test their performance over the validation set. We could run through a lot of different combinations of hyperparameters, and find the best performance over the validation set. After we get the best hyperparameters, the model is selcted, and we fit it over the training set to get our model to use.\nTo compare our model with our models, either our own model using other algorithms, or models built by others, we need some new data. We can no longer use the training set and the validation set since all data in them are used, either for training or for hyperparameters tuning. We need to use the test set to evaluate the “real performance” of our data.\nTo summarize:\n\nTraining set: used to fit the model;\nValidation set: used to tune the hyperparameters;\nTest set: used to check the overall performance of the model.\n\nThe validation set is not always required. If we use cross-validation technique for hyperparameters tuning, like sklearn.model_selection.GridSearchCV(), we don’t need a separated validation set. In this case, we will only need the training set and the test set, and run GridSearchCV over the training set. The cross-validation will be discussed in {numref}Section %s&lt;section-cross-validation&gt;.\nThe sizes and strategies for dataset division depends on the problem and data available. It is often recommanded that more training data should be used. The typical distribution of training, validation and test is \\((6:3:1)\\), \\((7:2:1)\\) or \\((8:1:1)\\). Sometimes validation set is discarded and only training set and test set are used. In this case the distribution of training and test set is usually \\((7:3)\\), \\((8:2)\\) or \\((9:1)\\).\n\n\n1.3.4 Workflow in developing a machine learning application\nThe workflow described below is from [3].\n\nCollect data.\nPrepare the input data.\nAnalyze the input data.\nTrain the algorithm.\nTest the algorithm.\nUse it.\n\nIn this course, we will mainly focus on Step 4 as well Step 5. These two steps are where the “core” algorithms lie, depending on the algorithm. We will start from the next Chapter to talk about various Machine Learning algorithms and examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#python-quick-guide",
    "href": "contents/1/intro.html#python-quick-guide",
    "title": "1  Introduction",
    "section": "1.4 Python quick guide",
    "text": "1.4 Python quick guide\n\n1.4.1 Python Notebook\nWe mainly use Python Notebook (.ipynb) to write documents for this course. Currently all main stream Python IDE support Python Notebook. All of them are not entirely identical but the differences are not huge and you may choose any you like.\nOne of the easiest ways to use Python Notebook is through JupyterLab. The best part about it is that you don’t need to worry about installation and configuration in the first place, and you can directly start to code.\nClick the above link and choose JupyterLab. Then you will see the following page.\n\nThe webapp you just started is called JupyterLite. This is a demo version. The full JupyterLab installation instruction can also be found from the link.\nThere is a small button + under the tab bar. This is the place where you click to start a new cell. You may type codes or markdown documents or raw texts in the cell according to your needs. The drag-down menu at the end of the row which is named Code or Markdown or Raw can help you make the switch. Markdown is a very simple light wighted language to write documents. In most cases it behaves very similar to plain texts. Codes are just regular Python codes (while some other languages are supported). You may either use the triangle button in the menu to execute the codes, or hit shift + enter.\n\nJupyterLite contains a few popular packages. Therefore it is totally ok if you would like to play with some simple things. However since it is an online evironment, it has many limitations. Therefore it is still recommended to set up a local environment once you get familiar with Python Notebook. Please check the following links for some popular choices for notebooks and Python installations in general, either local and online.\n\nJupyter Notebook / JupyterLab\nVS Code\nPyCharm\nGoogle Colab\nAnaconda\n\n\n\n1.4.2 Python fundamentals\nWe will put some very basic Python commands here for you to warm up. More advanced Python knowledge will be covered during the rest of the semester. The main reference for this part is [3]. Another referenece is My notes.\n\n1.4.2.1 Indentation\nPython is using indentation to denote code blocks. It is not convienent to write in the first place, but it forces you to write clean, readable code.\nBy the way, the if and for block are actually straightforward.\nif jj &lt; 3:\n    jj = jj \n    print(\"It is smaller than 3.\")\nif jj &lt; 3:\n    jj = jj\nprint(\"It is smaller than 3.\")\nfor i in range(3):\n    i = i + 1\n    print(i)\nfor i in range(3):\n    i = i + 1\nprint(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease tell the differences between the above codes.\n\n\n1.4.2.2 list and dict\nHere are some very basic usage of lists of dictionaries in Python.\n\nnewlist = list()\nnewlist.append(1)\nnewlist.append('hello')\nnewlist\n\n[1, 'hello']\n\n\n\nnewlisttwo = [1, 'hello']\nnewlisttwo\n\n[1, 'hello']\n\n\n\nnewdict = dict()\nnewdict['one'] = 'good'\nnewdict[1] = 'yes'\nnewdict\n\n{'one': 'good', 1: 'yes'}\n\n\n\nnewdicttwo = {'one': 'good', 1: 'yes'}\nnewdicttwo\n\n{'one': 'good', 1: 'yes'}\n\n\n\n\n1.4.2.3 Loop through lists\nWhen creating for loops we may let Python directly loop through lists. Here is an example. The code is almost self-explained.\n\nalist = ['one', 2, 'three', 4]\n\nfor item in alist:\n    print(item)\n\none\n2\nthree\n4\n\n\n\n\n1.4.2.4 Reading files\nThere are a lot of functions that can read files. The basic one is to read any files as a big string. After we get the string, we may parse it based on the structure of the data.\nThe above process sounds complicated. That’s why we have so many different functions reading files. Usually they focus on a certain types of files (e.g. spreadsheets, images, etc..), parse the data into a particular data structure for us to use later.\nI will mention a few examples.\n\ncsv files and excel files Both of them are spreadsheets format. Usually we use pandas.read_csv and pandas.read_excel both of which are from the package pandas to read these two types of files.\nimages Images can be treated as matrices, that each entry represents one pixel. If the image is black/white, it is represented by one matrix where each entry represents the gray value. If the image is colored, it is represented by three matrices where each entry represents one color. To use which three colors depends on the color map. rgb is a popular choice.\nIn this course when we need to read images, we usually use matplotlib.pyplot.imread from the package matplotlib or cv.imread from the package opencv.\n.json files .json is a file format to store dictionary type of data. To read a json file and parse it as a dictionary, we need json.load from the package json.\n\n\n\n1.4.2.5 Writing files\n\npandas.DataFrame.to_csv\npandas.DataFrame.to_excel\nmatplotlib.pyplot.imsave\ncv.imwrite\njson.dump\n\n\n\n1.4.2.6 Relative paths\nIn this course, when reading and writing files, please keep all the files using relative paths. That is, only write the path starting from the working directory.\n\nExample 1.2 Consider the following tasks:\n\nYour working directory is C:/Users/Xinli/projects/.\nWant to read a file D:/Files/example.csv.\nWant to generate a file whose name is result.csv and put it in a subfoler named foldername.\n\nTo do the tasks, don’t directly run the code pd.read_csv('D:/Files/example.csv'). Instead you should first copy the file to your working directory C:/Users/Xinli/projects/, and then run the following code.\n\nimport pandas as pd\n\ndf = pd.read_csv('example.csv')\ndf.to_csv('foldername/result.csv')\n\nPlease pay attention to how the paths are written.\n\n\n\n1.4.2.7 .\n\nclass and packages.\nGet access to attributes and methods\nChaining dots.\n\n\n\n\n1.4.3 Some additional topics\nYou may read about these parts from the appendices of My notes.\n\n1.4.3.1 Package management and Virtual environment\n\nconda\n\nconda create\n\nconda create --name myenv\nconda create --name myenv python=3.9\nconda create --name myenv --file spec-file.txt\n\nconda install\n\nconda install -c conda-forge numpy\n\nconda activate myenv\nconda list\n\nconda list numpy\nconda list --explicit &gt; spec-file.txt\n\nconda env list\n\npip / venv\n\npython -m venv newenv\nnewenv\\Scripts\\activate\npip install\npip freeze &gt; requirements.txt\npip install -r /path/to/requirements.txt\ndeactivate\n\n\n\n\n1.4.3.2 Version Control\n\nGit\n\nInstall\ngit config --list\ngit config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n\nGitHub",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#exercises",
    "href": "contents/1/intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nThese exercises are from [4], [1] and [3].\n\n1.5.1 Python Notebook\n\nExercise 1.1 (Hello World!) Please set up a Python Notebook environment and type print('Hello World!').\n\n\nExercise 1.2 Please set up a Python Notebook and start a new virtual environment and type print('Hello World!').\n\n\n\n1.5.2 Basic Python\n\nExercise 1.3 (Play with lists) Please complete the following tasks.\n\nWrite a for loop to print values from 0 to 4.\nCombine two lists ['apple', 'orange'] and ['banana'] using +.\nSort the list ['apple', 'orange', 'banana'] using sorted().\n\n\n\n\nExercise 1.4 (Play with list, dict and pandas.) Please complete the following tasks.\n\nCreate a new dictionary people with two keys name and age. The values are all empty list.\nAdd Tony to the name list in people.\nAdd Harry to the name list in people.\nAdd number 100 to the age list in people.\nAdd number 10 to the age list in people.\nFind all the keys of people and save them into a list namelist.\nConvert the dictionary people to a Pandas DataFrame df.\n\n\n\n\nExercise 1.5 (The dataset iris)  \n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nPlease explore this dataset.\n\nPlease get the features for iris and save it into X as an numpy array.\nWhat is the meaning of these features?\nPlease get the labels for iris and save it into y as an numpy array.\nWhat is the meaning of labels?\n\n\n\n\nExercise 1.6 (Play with Pandas) Please download the Titanic data file from here. Then follow the instructions to perform the required tasks.\n\nUse pandas.read_csv to read the dataset and save it as a dataframe object df.\nChange the values of the Sex column that male is 0 and female is 1.\nPick the columns Pclass, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare and transform them into a 2-dimensional numpy.ndarray, and save it as X.\nPick the column Survived and transform it into a 1-dimensional numpy.ndarray and save it as y.\n\n\n\n\n\n\n\n[1] Géron, A. (2019). Hands-on machine learning with scikit-learn, keras, and TensorFlow concepts, tools, and techniques to build intelligent systems: Concepts, tools, and techniques to build intelligent systems. O’Reilly Media.\n\n\n[2] Chollet, F. (2021). Deep learning with python, second edition. MANNING PUBN.\n\n\n[3] Harrington, P. (2012). Machine learning in action. Manning Publications.\n\n\n[4] Klosterman, S. (2021). Data science projects with python: A case study approach to gaining valuable insights from real data with machine learning. Packt Publishing, Limited.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html",
    "href": "contents/2/intro.html",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1 k-Nearest Neighbors Algorithm (k-NN)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "href": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1.1 Ideas\nAssume that we have a set of labeled data \\(\\{(X_i, y_i)\\}\\) where \\(y_i\\) denotes the label. Given a new data \\(X\\), how do we determine the label of it?\nk-NN algorithm starts from a very straightforward idea. We use the distances from the new data point \\(X\\) to the known data points to identify the label. If \\(X\\) is closer to \\(y_i\\) points, then we will label \\(X\\) as \\(y_i\\).\nLet us take cities and countries as an example. New York and Los Angeles are U.S cities, and Beijing and Shanghai are Chinese cities. Now we would like to consider Tianjin and Russellville. Do they belong to China or U.S? We calculate the distances from Tianjin (resp. Russellville) to all four known cities. Since Tianjin is closer to Beijing and Shanghai comparing to New York and Los Angeles, we classify Tianjin as a Chinese city. Similarly, since Russellville is closer to New York and Los Angeles comparing to Beijing and Shanghai, we classify it as a U.S. city.\n\n\n\n\n\n\n\nG\n\n\n\nBeijing\n\nBeijing\n\n\n\nShanghai\n\nShanghai\n\n\n\nTianjin\n\nTianjin\n\n\n\nTianjin-&gt;Beijing\n\n\ncloser\n\n\n\nTianjin-&gt;Shanghai\n\n\ncloser   \n\n\n\nNew York\n\nNew York\n\n\n\nTianjin-&gt;New York\n\n\n far away\n\n\n\nLos Angelis\n\nLos Angelis\n\n\n\nTianjin-&gt;Los Angelis\n\n\n far away\n\n\n\nRussellville\n\nRussellville\n\n\n\nRussellville-&gt;Beijing\n\n\nfar away \n\n\n\nRussellville-&gt;Shanghai\n\n\nfar away\n\n\n\nRussellville-&gt;New York\n\n\ncloser  \n\n\n\nRussellville-&gt;Los Angelis\n\n\ncloser\n\n\n\n\n\n\n\n\nThis naive example explains the idea of k-NN. Here is a more detailed description of the algorithm.\n\n\n2.1.2 The Algorithm\n\n\n\n\n\n\nk-NN Classifier\n\n\n\nInputs: Given the training data set \\(\\{(X_i, y_i)\\}\\) where \\(X_i=(x_i^1,x_i^2,\\ldots,x_i^n)\\) represents \\(n\\) features and \\(y_i\\) represents labels. Given a new data point \\(\\tilde{X}=(\\tilde{x}^1,\\tilde{x}^2,\\ldots,\\tilde{x}^n)\\).\nOutputs: Want to find the best label for \\(\\tilde{X}\\).\n\nCompute the distance from \\(\\tilde{X}\\) to each \\(X_i\\).\nSort all these distances from the nearest to the furthest.\nFind the nearest \\(k\\) data points.\nDetermine the labels for each of these \\(k\\) nearest points, and compute the frenqucy of each labels.\nThe most frequent label is considered to be the label of \\(\\tilde{X}\\).\n\n\n\n\n\n2.1.3 Details\n\nThe distance between two data points are defined by the Euclidean distance:\n\n\\[\ndist\\left((x^j_i)_{j=1}^n, (\\tilde{x}^j)_{j=1}^n\\right)=\\sqrt{\\sum_{j=1}^n(x^j_i-\\tilde{x}^j)^2}.\n\\]\n\nUsing linear algebra notations:\n\n\\[\ndist(X_i,\\tilde{X})=\\sqrt{(X_i-\\tilde{X})\\cdot(X_i-\\tilde{X})}.\n\\]\n\nAll the distances are stored in a \\(1\\)-dim numpy array, and we will combine it together with another \\(1\\)-dim array that store the labels of each point.\n\n\n\n2.1.4 The codes\n\nargsort\nunique\nargmax\n\n\nimport numpy as np\n\ndef classify_kNN(inX, X, y, k=5):\n    # compute the distance between each row of X and Xmat\n    Dmat = np.sqrt(((inX - X)**2).sum(axis=1))\n    # sort by distance\n    k = min(k, Dmat.shape[0])\n    argsorted = Dmat.argsort()[:k]\n    relatedy = y[argsorted]\n    # count the freq. of the first k labels\n    labelcounts = np.unique(relatedy, return_counts=True)\n    # find the label with the most counts\n    label = labelcounts[0][labelcounts[1].argmax()]\n    return label\n\n\n\n2.1.5 sklearn packages\nYou may also directly use the kNN function KNeighborsClassifier packaged in sklearn.neighbors. You may check the description of the function online from here.\nThere are many ways to modify the kNN algorithm. What we just mentioned is the simplest idea. It is correspondent to the argument weights='uniform', algorithm='brute and metric='euclidean'. However due to the implementation details, the results we got from sklearn are still a little bit different from the results produced by our naive codes.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=10, weights='uniform',\n                           algorithm='brute', metric='euclidean')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\n2.1.6 Normalization\nDifferent features may have different scales. It might be unfair for those features that have small scales. Therefore usually it is better to rescale all the features to make them have similar scales. After examining all the data, we find the minimal value minVal and the range ranges for each column. The normalization formula is:\n\\[\nX_{norm} = \\frac{X_{original}-minVal}{ranges}.\n\\]\nWe could also convert the normalized number back to the original value by\n\\[\nX_{original} = X_{norm} \\times ranges + minVal.\n\\]\nThe sample codes are listed below.\n\nimport numpy as np\n\ndef encodeNorm(X, parameters=None):\n    # parameters contains minVals and ranges\n    if parameters is None:\n        minVals = np.min(X, axis=0)\n        maxVals = np.max(X, axis=0)\n        ranges = np.maximum(maxVals - minVals, np.ones(minVals.size))\n        parameters = {'ranges': ranges, 'minVals': minVals}\n    else:\n        minVals = parameters['minVals']\n        ranges = parameters['ranges']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xnorm = (X - Nmat)/ranges\n    return (Xnorm, parameters)\n\n\ndef decodeNorm(X, parameters):\n    # parameters contains minVals and ranges\n    ranges = parameters['ranges']\n    minVals = parameters['minVals']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xoriginal = X * ranges + Nmat\n    return Xoriginal\n\nIf you use sklearn you could use MinMaxScaler from sklearn.preprocessing to achive the same goal. The related codes will be discussed later in projects. I keep our handwritten codes here for Python practicing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "href": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.2 k-NN Project 1: iris Classification",
    "text": "2.2 k-NN Project 1: iris Classification\nThis data is from sklearn.datasets. This dataset consists of 3 different types of irises’ petal / sepal length / width, stored in a \\(150\\times4\\) numpy.ndarray. We already explored the dataset briefly in the previous chapter. This time we will try to use the feature provided to predict the type of the irises. For the purpose of plotting, we will only use the first two features: sepal length and sepal width.\n\n2.2.1 Explore the dataset\nWe first load the dataset.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nThen we would like to split the dataset into trainning data and test data. Here we are going to use sklearn.model_selection.train_test_split function. Besides the dataset, we should also provide the propotion of the test set comparing to the whole dataset. We will choose test_size=0.1 here, which means that the size of the test set is 0.1 times the size of the whole dataset. stratify=y means that when split the dataset we want to split respects the distribution of labels in y.\nThe split will be randomly. You may set the argument random_state to be a certain number to control the random process. If you set a random_state, the result of the random process will stay the same. This is for reproducible output across multiple function calls.\nAfter we get the training set, we should also normalize it. All our normalization should be based on the training set. When we want to use our model on some new data points, we will use the same normalization parameters to normalize the data points in interests right before we apply the model. Here since we mainly care about the test set, we could normalize the test set at this stage.\nNote that in the following code, the function encodeNorm defined in the previous section is used. \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\nBefore we start to play with k-NN, let us look at the data first. Since we only choose two features, it is able to plot these data points on a 2D plane, with different colors representing different classes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the scatter plot.\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n# Generate legends.\nlabels = ['setosa', 'versicolor', 'virginica']\n_ = fig.legend(handles=scatter.legend_elements()[0], labels=labels,\n               loc=\"right\", title=\"Labels\")\n\n\n\n\n\n\n\n\n\n\n2.2.2 Apply our k-NN model\nNow let us apply k-NN to this dataset. We first use our codes. The poential code is\n\ny_pred = classify_kNN(X_test, X_train, y_train, k=10)\n\nHowever the above code is actually wrong. The issue is that our function classify_kNN can only classify one row of data. To classify many rows, we need to use a for loop.\n\nn_neighbors = 10\ny_pred = list()\nfor row in X_test_norm:\n    row_pred = classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n    y_pred.append(row_pred)\ny_pred = np.array(y_pred)\n\nWe could use list comprehension to simply the above codes.\n\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nThis y_pred is the result we got for the test set. We may compare it with the real answer y_test, and calcuate the accuracy.\n\nacc = np.mean(y_pred == y_test)\nacc\n\n0.7333333333333333\n\n\n\n\n2.2.3 Apply k-NN model from sklearn\nNow we would like to use sklearn to reproduce this result. Since our data is prepared, what we need to do is directly call the functions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 10\nclf = KNeighborsClassifier(n_neighbors, weights=\"uniform\", metric=\"euclidean\",\n                           algorithm='brute')\nclf.fit(X_train_norm, y_train)\ny_pred_sk = clf.predict(X_test_norm)\n\nacc = np.mean(y_pred_sk == y_test)\nacc\n\n0.7333333333333333\n\n\n\n\n2.2.4 Using data pipeline\nWe may organize the above process in a neater way. After we get a data, the usual process is to apply several transforms to the data before we really get to the model part. Using terminolgies from sklearn, the former are called transforms, and the latter is called an estimator. In this example, we have exactly one tranform which is the normalization. The estimator here we use is the k-NN classifier.\nsklearn provides a standard way to write these codes, which is called pipeline. We may chain the transforms and estimators in a sequence and let the data go through the pipeline. In this example, the pipeline contains two steps: 1. The normalization transform sklearn.preprocessing.MinMaxScaler. When we directly apply it the parameters ranges and minVals and will be recorded automatically, and we don’t need to worry about it when we want to use the same parameters to normalize other data. 2. The k-NN classifier sklearn.neighbors.KNeighborsClassifier. This is the same one as we use previously.\nThe code is as follows. It is a straightforward code. Note that the () after the class in each step of steps is very important. The codes cannot run if you miss it.\nAfter we setup the pipeline, we may use it as other estimators since it is an estimator. Here we may also use the accuracy function provided by sklearn to perform the computation. It is essentially the same as our acc computation.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.7333333333333333\n\n\n\n\n2.2.5 Visualize the Decision boundary [Optional]\nUsing the classifier we get above, we are able to classify every points on the plane. This enables us to draw the following plot, which is called the Decision boundary. It helps us to visualize the relations between features and the classes.\nWe use DecisionBoundaryDisplay from sklearn.inspection to plot the decision boundary. The function requires us to have a fitted classifier. We may use the classifier pipe we got above. Note that this classifier should have some build-in structures that our classify_kNN function doesn’t have. We may rewrite our codes to make it work, but this goes out of the scope of this section. This is supposed to be Python programming exercise. We will talk about it in the future if we have enough time.\nWe first plot the dicision boundary using DecisionBoundaryDisplay.from_estimator. Then we plot the points from X_test. From the plot it is very clear which points are misclassified.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n            pipe, \n            X_train,\n            response_method=\"predict\",\n            plot_method=\"pcolormesh\",\n            xlabel=iris.feature_names[0],\n            ylabel=iris.feature_names[1],\n            alpha=0.5)\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\")\ndisp.figure_.set_size_inches((10,7))\n\n\n\n\n\n\n\n\n\n\n2.2.6 k-Fold Cross-Validation\nPreviously we perform a random split and test our model in this case. What would happen if we fit our model on another split? We might get a different accuracy score. So in order to evaluate the performance of our model, it is natual to consider several different split and compute the accuracy socre for each case, and combine all these socres together to generate an index to indicate whehter our model is good or bad. This naive idea is called k-Fold Cross-Validation.\nThe algorithm is described as follows. We first randomly split the dataset into k groups. We use one of them as the test set, and the rest together forming the training set, and use this setting to get an accuracy score. We did this for each group to be chosen as the test set. Then the final score is the mean.\nsklearn provides a function sklearn.model_selection.cross_val_score to perform the above computation. The usage is straightforward, as follows.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(pipe, X, y, cv=5)\ncv_scores\n\narray([0.66666667, 0.8       , 0.63333333, 0.8       , 0.7       ])\n\n\n\nnp.mean(cv_scores)\n\n0.7200000000000001\n\n\n\n\n2.2.7 Choosing a k value\nIn the previous example we choose k to be 10 as an example. To choose a k value we usually run some test by trying different k and choose the one with the best performance. In this case, best performance means the highest cross-validation score.\nsklearn.model_selection.GridSearchCV provides a way to do this directly. We only need to setup the esitimator, the metric (which is the cross-validation score in this case), and the hyperparameters to be searched through, and GridSearchCV will run the search automatically.\nWe let k go from 1 to 100. The code is as follows.\nNote that parameters is where we set the search space. It is a dictionary. The key is the name of the estimator plus double _ and then plus the name of the parameter.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n35\n\n\nAfter we fit the data, the best_estimator_.get_params() can be printed. It tells us that it is best to use 31 neibhours for our model. We can directly use the best estimator by calling clf.best_estimator_.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nnp.mean(cv_scores)\n\n0.82\n\n\nThe cross-validation score using k=31 is calculated. This serves as a benchmark score and we may come back to dataset using other methods and compare the scores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "href": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.3 k-NN Project 2: Dating Classification",
    "text": "2.3 k-NN Project 2: Dating Classification\nThe data can be downloaded from here.\n\n2.3.1 Background\nHelen dated several people and rated them using a three-point scale: 3 is best and 1 is worst. She also collected data from all her dates and recorded them in the file attached. These data contains 3 features:\n\nNumber of frequent flyer miles earned per year\nPercentage of time spent playing video games\nLiters of ice cream consumed per week\n\nWe would like to predict her ratings of new dates when we are given the three features.\nThe data contains four columns, while the first column refers to Mileage, the second Gamingtime, the third Icecream and the fourth Rating.\n\n\n2.3.2 Look at Data\nWe first load the data and store it into a DataFrame.\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('datingTestSet2.txt', sep='\\t', header=None)\ndf.head()\n\nTo make it easier to read, we would like to change the name of the columns.\n\ndf = df.rename(columns={0: \"Mileage\", 1: \"Gamingtime\", 2: 'Icecream', 3: 'Rating'})\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nGamingtime\nIcecream\nRating\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\nSince now we have more than 2 features, it is not suitable to directly draw scatter plots. We use seaborn.pairplot to look at the pairplot. From the below plots, before we apply any tricks, it seems that Milegae and Gamingtime are better than Icecream to classify the data points.\n\nimport seaborn as sns\nsns.pairplot(data=df, hue='Rating')\n\n\n\n\n\n\n\n\n\n\n2.3.3 Applying kNN\nSimilar to the previous example, we will apply both methods for comparisons.\n\nfrom sklearn.model_selection import train_test_split\nX = np.array(df[['Mileage', 'Gamingtime', 'Icecream']])\ny = np.array(df['Rating'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\n\nUsing our codes.\n\n\n# Using our codes.\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nacc = np.mean(y_pred == y_test)\nacc\n\n0.93\n\n\n\nUsing sklearn.\n\n\n# Using sklearn.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.93\n\n\n\n\n2.3.4 Choosing k Value\nSimilar to the previous section, we can run tests on k value to choose one to be used in our model using GridSearchCV.\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n4\n\n\nFrom this result, in this case the best k is 4. The corresponding cross-validation score is computed below.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nnp.mean(cv_scores)\n\n0.952",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "href": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.4 k-NN Project 3: Handwritten recognition",
    "text": "2.4 k-NN Project 3: Handwritten recognition\nWe would like to let the machine recognize handwritten digits. The dataset comes from the UCI dataset repository. Now we apply kNN algrotithm to it.\n\n2.4.1 Dataset description\nEvery digit is stored as a \\(8\\times8\\) picture. This is a \\(8\\times8\\) matrix. Every entry represents a gray value of the corresponding pixel, whose value is from 0 to 16. The label of each matrix is the digit it represents. Note that the dataset provided is already splitted into a training set and a test set.\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nX = datasets.load_digits().images\ny = datasets.load_digits().target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15)\n\nLet us play with these data first.\n\nX_train.shape\n\n(1527, 8, 8)\n\n\n\ny_train.shape\n\n(1527,)\n\n\n\nX_test.shape\n\n(270, 8, 8)\n\n\n\ny_test.shape\n\n(270,)\n\n\n\ntype(X_train)\n\nnumpy.ndarray\n\n\nFrom these information we can see that the training set contains 1527 digits and the test set contains 270 digits. Each digit is represented by a \\(8\\times8\\) numpy array. Let us load one and display the digit by matplotlib.pyplot.imshow.\n\ndigit = X_train[0]\n\nimport matplotlib.pyplot as plt\nplt.imshow(digit, cmap='gray')\n\n\n\n\n\n\n\n\nThis image represents a handwritten digit. Could you recognize it? We could check our guess by looking at the label. The following shows that it is a 0.\n\ny_train[0]\n\n0\n\n\nNow we need to reshape these digits from \\(8\\times8\\) numpy arraies to \\(64\\) numpy arraies. Similar to previous examples, we will also normalize the dataset.\n\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\n\n\n2.4.2 Apply k-NN\nLike the previous two examples, we now try to apply the k-NN algorithm to classify these handwritten digits.\n\nimport numpy as np\n\nn_neighbors = 10\nX_test_sample = X_test_norm\ny_test_sample = y_test\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_sample])\n\nacc = np.mean(y_pred == y_test_sample)\nacc\n\n0.9814814814814815\n\n\nNow let us try to apply sklearn package. Note that we could run the code over the whole test set (which contains 10000 digits) and the speed is much faster comparing to our codes. To save time we won’t grid search k here. The code is the same anyway.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.9814814814814815",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#exercises-and-projects",
    "href": "contents/2/intro.html#exercises-and-projects",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.5 Exercises and Projects",
    "text": "2.5 Exercises and Projects\n\nExercise 2.1 Handwritten example :label: ex2handwritten Consider the 1-dimensional data set shown below.\n\n\n\n\n\n\n\nx\n1.5\n2.5\n3.5\n4.5\n5.0\n5.5\n5.75\n6.5\n7.5\n10.5\n\n\n\n\ny\n+\n+\n-\n-\n-\n+\n+\n-\n+\n+\n\n\n\n\n\nPlease use the data to compute the class of \\(x=5.5\\) according to \\(k=1\\), \\(3\\), \\(6\\) and \\(9\\). Please compute everything by hand.\n\n\nExercise 2.2 (Titanic) Please download the titanic dataset from here. This is the same dataset from what you dealt with in Chapter 1 Exercises. Therefore you may use the same way to prepare the data.\nPlease analyze the dataset and build a k-NN model to predict whether someone is survived or not. Note that you have to pick k at the end.\n\n\n\n\n\n[1] Harrington, P. (2012). Machine learning in action. Manning Publications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html",
    "href": "contents/3/intro.html",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1 Gini impurity\nTo split a dataset, we need a metric to tell whether the split is good or not. The two most popular metrics that are used are Gini impurity and Entropy. The two metrics don’t have essential differences, that the results obtained by applying each metric are very similar to each other. Therefore we will only focus on Gini impurity since it is slightly easier to compute and slightly easier to explain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#gini-impurity",
    "href": "contents/3/intro.html#gini-impurity",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1.1 Motivation and Definition\nAssume that we have a dataset of totally \\(n\\) objects, and these objects are divided into \\(k\\) classes. The \\(i\\)-th class has \\(n_i\\) objects. Then if we randomly pick an object, the probability to get an object belonging to the \\(i\\)-th class is\n\\[\np_i=\\frac{n_i}{n}\n\\]\nIf we then guess the class of the object purely based on the distribution of each class, the probability that our guess is incorrect is\n\\[\n1-p_i = 1-\\frac{n_i}{n}.\n\\]\nTherefore, if we randomly pick an object that belongs to the \\(i\\)-th class and randomly guess its class purely based on the distribution but our guess is wrong, the probability that such a thing happens is\n\\[\np_i(1-p_i).\n\\]\nConsider all classes. The probability at which any object of the dataset will be mislabelled when it is randomly labeled is the sum of the probability described above:\n\\[\n\\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2.\n\\]\nThis is the definition formula for the Gini impurity.\n\nDefinition 3.1 The Gini impurity is calculated using the following formula\n\\[\nGini = \\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2,\n\\] where \\(p_i\\) is the probability of class \\(i\\).\n\nThe way to understand Gini impurity is to consider some extreme examples.\n\nExample 3.1 Assume that we only have one class. Therefore \\(k=1\\), and \\(p_1=1\\). Then the Gini impurity is\n\\[\nGini = 1-1^2=0.\n\\] This is the minimum possible Gini impurity. It means that the dataset is pure: all the objects contained are of one unique class. In this case, we won’t make any mistakes if we randomly guess the label.\n\n\nExample 3.2 Assume that we have two classes. Therefore \\(k=2\\). Consider the distribution \\(p_1\\) and \\(p_2\\). We know that \\(p_1+p_2=1\\). Therefore \\(p_2=1-p_1\\). Then the Gini impurity is\n\\[\nGini(p_1) = 1-p_1^2-p_2^2=1-p_1^2-(1-p_1)^2=2p_1-2p_1^2.\n\\] When \\(0\\leq p_1\\leq 1\\), this function \\(Gini(p_1)\\) is between \\(0\\) and \\(0.5\\). - It gets \\(0\\) when \\(p_1=0\\) or \\(1\\). In these two cases, the dataset is still a one-class set since the size of one class is \\(0\\). - It gets \\(0.5\\) when \\(p_1=0.5\\). This means that the Gini impurity is maximized when the size of different classes are balanced.\n\n\n\n3.1.2 Algorithm\n\n\n\n\n\n\nAlgorithm: Gini impurity\n\n\n\nInputs A dataset \\(S=\\{data=[features, label]\\}\\) with labels.\nOutputs The Gini impurity of the dataset.\n\nGet the size \\(n\\) of the dataset.\nGo through the label list, and find all unique labels: \\(uniqueLabelList\\).\nGo through each label \\(l\\) in \\(uniqueLabelList\\) and count how many elements belonging to the label, and record them as \\(n_l\\).\nUse the formula to compute the Gini impurity:\n\\[\nGini = 1-\\sum_{l\\in uniqueLabelList}\\left(\\frac{n_l}{n}\\right)^2.\n\\]\n\n\n\nThe sample codes are listed below:\n\nimport pandas as pd\ndef gini(S):\n    N = len(S)\n    y = S[:, -1].reshape(N)\n    gini = 1 - ((pd.Series(y).value_counts()/N)**2).sum()\n    return gini",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#cart-algorithms",
    "href": "contents/3/intro.html#cart-algorithms",
    "title": "3  Decision Trees",
    "section": "3.2 CART Algorithms",
    "text": "3.2 CART Algorithms\n\n3.2.1 Ideas\nConsider a labeled dataset \\(S\\) with totally \\(m\\) elements. We use a feature \\(k\\) and a threshold \\(t_k\\) to split it into two subsets: \\(S_l\\) with \\(m_l\\) elements and \\(S_r\\) with \\(m_r\\) elements. Then the cost function of this split is\n\\[\nJ(k, t_k)=\\frac{m_l}mGini(S_l)+\\frac{m_r}{m}Gini(S_r).\n\\] It is not hard to see that the more pure the two subsets are the lower the cost function is. Therefore our goal is find a split that can minimize the cost function.\n\n\n\n\n\n\nAlgorithm: Split the Dataset\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\).\nOutputs A best split \\((k, t_k)\\).\n\nFor each feature \\(k\\):\n\nFor each value \\(t\\) of the feature:\n\nSplit the dataset \\(S\\) into two subsets, one with \\(k\\leq t\\) and one with \\(k&gt;t\\).\nCompute the cost function \\(J(k,t)\\).\nCompare it with the current smallest cost. If this split has smaller cost, replace the current smallest cost and pair with \\((k, t)\\).\n\n\nReturn the pair \\((k,t_k)\\) that has the smallest cost function.\n\n\n\nWe then use this split algorithm recursively to get the decision tree.\n\n\n\n\n\n\nClassification and Regression Tree, CART\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\) and a maximal depth max_depth.\nOutputs A decision tree.\n\nStarting from the original dataset \\(S\\). Set the working dataset \\(G=S\\).\nConsider a dataset \\(G\\). If \\(Gini(G)\\neq0\\), split \\(G\\) into \\(G_l\\) and \\(G_r\\) to minimize the cost function. Record the split pair \\((k, t_k)\\).\nNow set the working dataset \\(G=G_l\\) and \\(G=G_r\\) respectively, and apply the above two steps to each of them.\nRepeat the above steps, until max_depth is reached.\n\n\n\nHere are the sample codes.\n\ndef split(G):\n    m = G.shape[0]\n    gmini = gini(G)\n    pair = None\n    if gini(G) != 0:\n        numOffeatures = G.shape[1] - 1\n        for k in range(numOffeatures):\n            for t in range(m):\n                Gl = G[G[:, k] &lt;= G[t, k]]\n                Gr = G[G[:, k] &gt; G[t, k]]\n                gl = gini(Gl)\n                gr = gini(Gr)\n                ml = Gl.shape[0]\n                mr = Gr.shape[0]\n                g = gl*ml/m + gr*mr/m\n                if g &lt; gmini:\n                    gmini = g\n                    pair = (k, G[t, k])\n                    Glm = Gl\n                    Grm = Gr\n        res = {'split': True,\n               'pair': pair,\n               'sets': (Glm, Grm)}\n    else:\n        res = {'split': False,\n               'pair': pair,\n               'sets': G}\n    return res\n\nFor the purpose of counting labels, we also write a code to do so.\n\nimport pandas as pd\ndef countlabels(S):\n    y = S[:, -1].reshape(S.shape[0])\n    labelCount = dict(pd.Series(y).value_counts())\n    return labelCount",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "href": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "title": "3  Decision Trees",
    "section": "3.3 Decision Tree Project 1: the iris dataset",
    "text": "3.3 Decision Tree Project 1: the iris dataset\nWe are going to use the Decision Tree model to study the iris dataset. This dataset has already studied previously using k-NN. Again we will only use the first two features for visualization purpose.\n\n3.3.1 Initial setup\nSince the dataset will be splitted, we will put X and y together as a single variable S. In this case when we split the dataset by selecting rows, the features and the labels are still paired correctly.\nWe also print the labels and the feature names for our convenience.\n\nfrom sklearn.datasets import load_iris\nimport numpy as np\nfrom assests.codes.dt import gini, split, countlabels\n\niris = load_iris()\nX = iris.data[:, 2:]\ny = iris.target\ny = y.reshape((y.shape[0],1))\nS = np.concatenate([X,y], axis=1)\n\nprint(iris.target_names)\nprint(iris.feature_names)\n\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\n\n\n3.3.2 Apply CART manually\nWe apply split to the dataset S.\n\nr = split(S)\nif r['split'] is True:\n    Gl, Gr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Gl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gr)),\n          ' and its label counts is {d}'.format(d=countlabels(Gr)))\n\n(0, 1.9)\nThe left subset's Gini impurity is 0.00,  and its label counts is {0.0: 50}\nThe right subset's Gini impurity is 0.50,  and its label counts is {1.0: 50, 2.0: 50}\n\n\nThe results shows that S is splitted into two subsets based on the 0-th feature and the split value is 1.9.\nThe left subset is already pure since its Gini impurity is 0. All elements in the left subset is label 0 (which is setosa). The right one is mixed since its Gini impurity is 0.5. Therefore we need to apply split again to the right subset.\n\nr = split(Gr)\nif r['split'] is True:\n    Grl, Grr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Grl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grr)),\n          ' and its label counts is {d}'.format(d=countlabels(Grr)))\n\n(1, 1.7)\nThe left subset's Gini impurity is 0.17,  and its label counts is {1.0: 49, 2.0: 5}\nThe right subset's Gini impurity is 0.04,  and its label counts is {2.0: 45, 1.0: 1}\n\n\nThis time the subset is splitted into two more subsets based on the 1-st feature and the split value is 1.7. The total Gini impurity is minimized using this split.\nThe decision we created so far can be described as follows:\n\nCheck the first feature sepal length (cm) to see whether it is smaller or equal to 1.9.\n\nIf it is, classify it as lable 0 which is setosa.\nIf not, continue to the next stage.\n\nCheck the second feature sepal width (cm) to see whether it is smaller or equal to 1.7.\n\nIf it is, classify it as label 1 which is versicolor.\nIf not, classify it as label 2 which is virginica.\n\n\n\n\n3.3.3 Use package sklearn\nNow we would like to use the decision tree package provided by sklearn. The process is straightforward. The parameter random_state=40 will be discussed {ref}later&lt;note-random_state&gt;, and it is not necessary in most cases.\n\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth=2, random_state=40)\nclf.fit(X, y)\n\nDecisionTreeClassifier(max_depth=2, random_state=40)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=2, random_state=40)\n\n\nsklearn provide a way to automatically generate the tree view of the decision tree. The code is as follows.\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(2, 2), dpi=200)\ntree.plot_tree(clf, filled=True, impurity=True)\n\n[Text(0.4, 0.8333333333333334, 'x[1] &lt;= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.2, 0.5, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.6, 0.5, 'x[1] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.4, 0.16666666666666666, 'gini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.8, 0.16666666666666666, 'gini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]')]\n\n\n\n\n\n\n\n\n\nSimilar to k-NN, we may use sklearn.inspection.DecisionBoundaryDisplay to visualize the decision boundary of this decision tree.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap='coolwarm',\n    response_method=\"predict\",\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n)\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=15)\n\n\n\n\n\n\n\n\n\n\n3.3.4 Analyze the differences between the two methods\nThe tree generated by sklearn and the tree we got manually is a little bit different. Let us explore the differences here.\nTo make it easier to split the set, we could convert the numpy.ndarray to pandas.DataFrame.\n\nimport pandas as pd\n\ndf = pd.DataFrame(X)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.4\n0.2\n\n\n1\n1.4\n0.2\n\n\n2\n1.3\n0.2\n\n\n3\n1.5\n0.2\n\n\n4\n1.4\n0.2\n\n\n\n\n\n\n\nNow based on our tree, we would like to get all data points that the first feature (which is marked as 0) is smaller or equal to 1.9. We save it as df1. Similarly based on the tree gotten from sklearn, we would like to get all data points taht the second feature (which is marked as 1) is smaller or equal to 0.8 and save it to df2.\n\ndf1 = df[df[0]&lt;=1.9]\ndf2 = df[df[1]&lt;=0.8]\n\nThen we would like to compare these two dataframes. What we want is to see whether they are the same regardless the order. One way to do this is to sort the two dataframes and then compare them directly.\nTo sort the dataframe we use the method DataFrame.sort_values. The details can be found here. Note that after sort_values we apply reset_index to reset the index just in case the index is massed by the sort operation.\nThen we use DataFrame.equals to check whether they are the same.\n\ndf1sorted = df1.sort_values(by=df1.columns.tolist()).reset_index(drop=True)\ndf2sorted = df2.sort_values(by=df2.columns.tolist()).reset_index(drop=True)\nprint(df1sorted.equals(df2sorted))\n\nTrue\n\n\nSo these two sets are really the same. The reason this happens can be seen from the following two graphs.\n\n\n\n\n\n\n\n\n\nFrom our code\n\n\n\n\n\n\n\nFrom sklearn\n\n\n\n\n\nSo you can see that either way can give us the same classification. This means that given one dataset the construction of the decision tree might be random at some points.\n\n\n\n\n\n\nnote-random_state\n\n\n\nSince the split is random, when using sklearn.DecisionTreeClassifier to construct decision trees, sometimes we might get the same tree as what we get from our naive codes.\nTo illustrate this phenomenaon I need to set the random state in case it will generate the same tree as ours when I need it to generate a different tree. The parameter random_state=40 mentioned before is for this purpose.\n\n\nAnother difference is the split value of the second branch. In our case it is 1.7 and in sklearn case it is 1.75. So after we get the right subset from the first split (which is called dfr), we would split it into two sets based on whether the second feature is above or below 1.7.\n\ndfr = df[df[0]&gt;1.9]\ndf2a = dfr[dfr[1]&gt;1.7]\ndf2b = dfr[dfr[1]&lt;=1.7]\nprint(df2b[1].max())\nprint(df2a[1].min())\n\n1.7\n1.8\n\n\nNow you can see where the split number comes from. In our code, when we found a split, we will directly use that number as the cut. In this case it is 1.7.\nIn sklearn, when it finds a split, the algorithm will go for the middle of the gap as the cut. In this case it is (1.7+1.8)/2=1.75.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-2-make_moons-dataset",
    "href": "contents/3/intro.html#decision-tree-project-2-make_moons-dataset",
    "title": "3  Decision Trees",
    "section": "3.4 Decision Tree Project 2: make_moons dataset",
    "text": "3.4 Decision Tree Project 2: make_moons dataset\nsklearn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. We are going to use make_moons in this section. More details can be found here.\nmake_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. make_moons produces two interleaving half circles. It is useful for visualization.\nLet us explorer the dataset first.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\n\n\n\n\n\n\n\n\nNow we are applying sklearn.DecisionTreeClassifier to construct the decision tree. The steps are as follows.\n\nSplit the dataset into training data and test data.\nConstruct the pipeline. Since we won’t apply any transformers there for this problem, we may just use the classifier sklearn.DecisionTreeClassifier directly without really construct the pipeline object.\nConsider the hyperparameter space for grid search. For this problme we choose min_samples_split and max_leaf_nodes as the hyperparameters we need. We will let min_samples_split run through 2 to 5, and max_leaf_nodes run through 2 to 50. We will use grid_search_cv to find the best hyperparameter for our model. For cross-validation, the number of split is set to be 3 which means that we will run trainning 3 times for each pair of hyperparameters.\nRun grid_search_cv. Find the best hyperparameters and the best estimator. Test it on the test set to get the accuracy score.\n\n\n# Step 1\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Step 3\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nparams = {'min_samples_split': list(range(2, 5)),\n          'max_leaf_nodes': list(range(2, 50))}\ngrid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), \n                              params, verbose=1, cv=3)\ngrid_search_cv.fit(X_train, y_train)\n\nFitting 3 folds for each of 144 candidates, totalling 432 fits\n\n\nGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)estimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=42)DecisionTreeClassifierDecisionTreeClassifier(random_state=42)\n\n\n\n# Step 4\nfrom sklearn.metrics import accuracy_score\n\nclf = grid_search_cv.best_estimator_\nprint(grid_search_cv.best_params_)\ny_pred = clf.predict(X_test)\naccuracy_score(y_pred, y_test)\n\n{'max_leaf_nodes': 17, 'min_samples_split': 2}\n\n\n0.8695\n\n\nNow you can see that for this make_moons dataset, the best decision tree should have at most 17 leaf nodes and the minimum number of samples required to be at a leaft node is 2. The fitted decision tree can get 86.95% accuracy on the test set.\nNow we can plot the decision tree and the decision surface.\n\nfrom sklearn import tree\nplt.figure(figsize=(15, 15), dpi=300)\ntree.plot_tree(clf, filled=True)\n\n[Text(0.5340909090909091, 0.9375, 'x[1] &lt;= 0.296\\ngini = 0.5\\nsamples = 8000\\nvalue = [3987, 4013]'),\n Text(0.25, 0.8125, 'x[0] &lt;= -0.476\\ngini = 0.367\\nsamples = 4275\\nvalue = [1036, 3239]'),\n Text(0.09090909090909091, 0.6875, 'x[0] &lt;= -0.764\\ngini = 0.183\\nsamples = 472\\nvalue = [424, 48]'),\n Text(0.045454545454545456, 0.5625, 'gini = 0.035\\nsamples = 333\\nvalue = [327, 6]'),\n Text(0.13636363636363635, 0.5625, 'x[1] &lt;= 0.047\\ngini = 0.422\\nsamples = 139\\nvalue = [97, 42]'),\n Text(0.09090909090909091, 0.4375, 'gini = 0.496\\nsamples = 70\\nvalue = [38, 32]'),\n Text(0.18181818181818182, 0.4375, 'gini = 0.248\\nsamples = 69\\nvalue = [59, 10]'),\n Text(0.4090909090909091, 0.6875, 'x[1] &lt;= -0.062\\ngini = 0.27\\nsamples = 3803\\nvalue = [612, 3191]'),\n Text(0.3181818181818182, 0.5625, 'x[1] &lt;= -0.371\\ngini = 0.147\\nsamples = 2426\\nvalue = [194, 2232]'),\n Text(0.2727272727272727, 0.4375, 'gini = 0.079\\nsamples = 1336\\nvalue = [55, 1281]'),\n Text(0.36363636363636365, 0.4375, 'gini = 0.223\\nsamples = 1090\\nvalue = [139, 951]'),\n Text(0.5, 0.5625, 'x[0] &lt;= 1.508\\ngini = 0.423\\nsamples = 1377\\nvalue = [418, 959]'),\n Text(0.45454545454545453, 0.4375, 'x[0] &lt;= 0.503\\ngini = 0.48\\nsamples = 1013\\nvalue = [404, 609]'),\n Text(0.36363636363636365, 0.3125, 'x[0] &lt;= -0.162\\ngini = 0.417\\nsamples = 469\\nvalue = [139, 330]'),\n Text(0.3181818181818182, 0.1875, 'gini = 0.5\\nsamples = 120\\nvalue = [61, 59]'),\n Text(0.4090909090909091, 0.1875, 'gini = 0.347\\nsamples = 349\\nvalue = [78, 271]'),\n Text(0.5454545454545454, 0.3125, 'x[0] &lt;= 1.1\\ngini = 0.5\\nsamples = 544\\nvalue = [265, 279]'),\n Text(0.5, 0.1875, 'x[1] &lt;= 0.129\\ngini = 0.49\\nsamples = 339\\nvalue = [193, 146]'),\n Text(0.45454545454545453, 0.0625, 'gini = 0.498\\nsamples = 178\\nvalue = [84, 94]'),\n Text(0.5454545454545454, 0.0625, 'gini = 0.437\\nsamples = 161\\nvalue = [109, 52]'),\n Text(0.5909090909090909, 0.1875, 'gini = 0.456\\nsamples = 205\\nvalue = [72, 133]'),\n Text(0.5454545454545454, 0.4375, 'gini = 0.074\\nsamples = 364\\nvalue = [14, 350]'),\n Text(0.8181818181818182, 0.8125, 'x[0] &lt;= 1.452\\ngini = 0.329\\nsamples = 3725\\nvalue = [2951, 774]'),\n Text(0.7272727272727273, 0.6875, 'x[1] &lt;= 0.757\\ngini = 0.232\\nsamples = 3355\\nvalue = [2905, 450]'),\n Text(0.6818181818181818, 0.5625, 'x[0] &lt;= -0.588\\ngini = 0.349\\nsamples = 1629\\nvalue = [1262, 367]'),\n Text(0.6363636363636364, 0.4375, 'gini = 0.07\\nsamples = 384\\nvalue = [370, 14]'),\n Text(0.7272727272727273, 0.4375, 'x[1] &lt;= 0.439\\ngini = 0.406\\nsamples = 1245\\nvalue = [892, 353]'),\n Text(0.6818181818181818, 0.3125, 'gini = 0.477\\nsamples = 420\\nvalue = [255, 165]'),\n Text(0.7727272727272727, 0.3125, 'gini = 0.352\\nsamples = 825\\nvalue = [637, 188]'),\n Text(0.7727272727272727, 0.5625, 'gini = 0.092\\nsamples = 1726\\nvalue = [1643, 83]'),\n Text(0.9090909090909091, 0.6875, 'x[0] &lt;= 1.782\\ngini = 0.218\\nsamples = 370\\nvalue = [46, 324]'),\n Text(0.8636363636363636, 0.5625, 'gini = 0.416\\nsamples = 132\\nvalue = [39, 93]'),\n Text(0.9545454545454546, 0.5625, 'gini = 0.057\\nsamples = 238\\nvalue = [7, 231]')]\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap=plt.cm.RdYlBu,\n    response_method=\"predict\"\n)\nplt.scatter(\n    X[:, 0],\n    X[:, 1],\n    c=y,\n    cmap='gray',\n    edgecolor=\"black\",\n    s=15,\n    alpha=.15)\n\n\n\n\n\n\n\n\nSince it is not very clear what the boundary looks like, I will draw the decision surface individually below.\n\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap=plt.cm.RdYlBu,\n    response_method=\"predict\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#exercises-and-projects",
    "href": "contents/3/intro.html#exercises-and-projects",
    "title": "3  Decision Trees",
    "section": "3.5 Exercises and Projects",
    "text": "3.5 Exercises and Projects\n\nExercise 3.1 The dataset and its scattering plot is given below.\n\nPlease calculate the Gini impurity of the whole set by hand.\nPlease apply CART to create the decision tree by hand.\nPlease use the tree you created to classify the following points:\n\n\\((0.4, 1.0)\\)\n\\((0.6, 1.0)\\)\n\\((0.6, 0)\\)\n\n\nThe following code is for ploting. You may also get the precise data points by reading the code. You don’t need to write codes to solve the problem.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.2 CHOOSE ONE: Please apply the Decision Tree to one of the following datasets.\n\ndating dataset (in Chpater 2).\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the accuracy (or cross-validation score) of your model and compare it with the models you learned before (kNN).\nPlease find the two most important features and explane your reason.\n(Optional) Use the two most important features to draw the Decision Boundary if possible.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html",
    "href": "contents/4/intro.html",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1 Bootstrap aggregating",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#bootstrap-aggregating",
    "href": "contents/4/intro.html#bootstrap-aggregating",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1.1 Basic bagging\nOne approach to get many estimators is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.\nConsider the following example. The dataset is the one we used in Chpater 3: make_moon. We split the dataset into training and test sets.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\n\n\n\n\n\n\n\nWe would like to sample from the dataset to get some smaller minisets. We will use sklearn.model_selection.ShuffleSplit to perform the action.\nThe output of ShuffleSplit is a generator. To get the index out of it we need a for loop. You may check out the following code.\nNote that ShuffleSplit is originally used to shuffle data into training and test sets. We would only use the shuffle function out of it, so we will set test_size to be 1 and use _ later in the for loop since we won’t use that part of the information.\nWhat we finally get is a generator rs that produces indexes of subsets of X_train and y_train.\n\nfrom sklearn.model_selection import ShuffleSplit\nn_trees = 1000\nn_instances = 100\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\n\nNow we would like to generate a list of Decision Trees. We could use the hyperparameters we get from Chapter 3. We train each tree over a certain mini set, and then evaluate the trained model over the test set. The average accuracy is around 80%.\nNote that rs is a generator. We put it in a for loop, and during each loop it will produce a list of indexes which gives a subset. We will directly train our model over the subset and use it to predict the test set. The result of each tree is put in the list y_pred_list and the accuracy is stored in the list acc_list. The mean of the accuracy is then computed by np.mean(acc_list).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(min_samples_split=2, max_leaf_nodes=17)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\n\nnp.mean(acc_list)\n\n0.7982400000000001\n\n\nNow for each test data, we actually have n_trees=1000 predicted results. We can treat it as the options from 1000 exports and would like to use the majority as our result. For this purpose we would like to use mode() which will find the most frequent entry.\n\nfrom scipy.stats import mode\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\n\nSince the output of mode is a tuple where the first entry is a 2D array, we need to reshape y_pred_mode. This is the result using this voting system. Then we are able to compute the accuracy, and find that it is increased from the previous prediction.\n\naccuracy_score(y_pred_mode, y_test)\n\n0.8646666666666667\n\n\n\n\n4.1.2 Some rough analysis\nThe point of Bagging is to let every classifier study part of the data, and then gather the opinions from everyone. If the performance are almost the same between individual classifers and the Bagging classifiers, this means that the majority of the individual classifiers have the same opinions. One possible reason is that the randomized subsets already catch the main features of the dataset that every individual classifiers behave similar.\n\n4.1.2.1 Case 1\nLet us continue with the previous dataset. We start from using Decision Tree with max_depth=1. In other words each tree only split once.\n\nn_trees = 500\nn_instances = 1000\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.7704213333333334\nThe accuracy of the bagging classifier: 0.772\n\n\nThe two accuracy has some differences, but not much. This is due to the fact that the sample size of the subset is too large: 1000 can already help the individual classifers to capture the major ideas of the datasets. Let us see the first 1000 data points. The scattering plot is very similar to that of the whole dataset shown above.\n\nNpiece = 1000\nplt.scatter(x=X[:Npiece, 0], y=X[:Npiece, 1], c=y[:Npiece])\n\n\n\n\n\n\n\n\n\n\n4.1.2.2 Case 2\nIf we reduce the sample size to be very small, for example, 20, the sampled subset will lose a lot of information and it will be much harder to capture the idea of the original dataset. See the scattering plot of the first 20 data points.\n\nNpiece = 20\nplt.scatter(x=X[:Npiece, 0], y=X[:Npiece, 1], c=y[:Npiece])\n\n\n\n\n\n\n\n\nIn this case, let us see the performance comparison between multiple decision trees and the bagging classifier.\n\nn_trees = 500\nn_instances = 20\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.7273333333333334\nThe accuracy of the bagging classifier: 0.814\n\n\nThis time you may see a significant increase in the performance.\n\n\n\n4.1.3 Using sklearn\nsklearn provides BaggingClassifier to directly perform bagging or pasting. The code is as follows.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(),\n                            n_estimators=1000,\n                            max_samples=100,\n                            bootstrap=True)\n\nIn the above code, bag_clf is a bagging classifier, made of 500 DecisionTreeClassifers, and is trained over subsets of size 100. The option bootstrap=True means that it is bagging. If you would like to use pasting, the option is bootstrap=False.\nThis bag_clf also has .fit() and .predict() methods. It is used the same as our previous classifiers. Let us try the make_moon dataset.\n\nbag_clf.fit(X_train, y_train)\ny_pred_bag = bag_clf.predict(X_test)\naccuracy_score(y_pred_bag, y_test)\n\n0.8606666666666667\n\n\n\n\n4.1.4 OOB score\nWhen we use bagging, it is possible that some of the training data are not used. In this case, we could record which data are not used, and just use them as the test set, instead of providing extra data for test. The data that are not used is called out-of-bag instances, or oob for short. The accuracy over the oob data is called the oob score.\nWe could set oob_score=True to enable the function when creating a BaggingClassifier, and use .oob_score_ to get the oob score after training.\n\nbag_clf_oob = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=1000,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True)\nbag_clf_oob.fit(X_train, y_train)\nbag_clf_oob.oob_score_\n\n0.8649411764705882\n\n\n\n\n4.1.5 Random Forests\nWhen the classifiers used in a bagging classifier are all Decision Trees, the bagging classifier is called a random forest. sklearn provide RandomForestClassifier class. It is almost the same as BaggingClassifier + DecisionTreeClassifer.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\ny_pred_rnd = rnd_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.8613333333333333\n\n\nWhen we use the Decision Tree as our base estimators, the class RandomForestClassifier provides more control over growing the random forest, with a certain optimizations. If you would like to use other estimators, then BaggingClassifier should be used.\n\n\n4.1.6 Extra-trees\nWhen growing a Decision Tree, our method is to search through all possible ways to find the best split point that get the lowest Gini impurity. Anohter method is to use a random split. Of course a random tree performs much worse, but if we use it to form a random forest, the voting system can help to increase the accuracy. On the other hand, random split is much faster than a regular Decision Tree.\nThis type of forest is called Extremely Randomized Trees, or Extra-Trees for short. We could modify the above random forest classifier code to implement the extra-tree algorithm. The key point is that we don’t apply the Decision Tree algorithm to X_subset. Instead we perform a random split.\n\nn_trees = 500\nn_instances = 20\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n# random split\n    i = np.random.randint(0, X_subset.shape[0])\n    j = np.random.randint(0, X_subset.shape[1])\n    split_threshold = X_subset[i, j]\n    lsetindex = np.where(X_subset[:, j]&lt;split_threshold)[0]\n\n    if len(lsetindex) == 0:\n        rsetindex = np.where(X_subset[:, j]&gt;=split_threshold)\n        rmode, _ = mode(y_subset[rsetindex], keepdims=True)\n        rmode = rmode[0]\n        lmode = 1 - rmode\n    else:\n        lmode, _ = mode(y_subset[lsetindex], keepdims=True)\n        lmode = lmode[0]\n        rmode = 1 - lmode\n    y_pred = np.where(X_test[:, j] &lt; split_threshold, lmode, rmode).reshape(-1)\n# The above code is used to use the random split to classify the data points\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.6313213333333333\nThe accuracy of the bagging classifier: 0.8273333333333334\n\n\nFrom the above example, you may find a significant increase in the performace from the mean individual accuracy to the Extra-tree classifier accuracy. The accuracy of the Extra-tree classifier is also very close to what we get from the original data points, although its base classifier is much simpler.\nIn sklearn there is an ExtraTreesClassifier to create such a classifier. It is hard to say which random forest is better beforehand. What we can do is to test and calculate the cross-validation scores (with grid search for hyperparameters tuning).\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\ny_pred_rnd = ext_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.858\n\n\nIn the above example, RandomForestClassifier and ExtraTreesClassifier get similar accuracy. However from the code below, you will see that in this example ExtraTreesClassifier is much faster than RandomForestClassifier.\n\nfrom time import time\nt0 = time()\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\nt1 = time()\nprint('Random Frorest: {}'.format(t1 - t0))\n\nt0 = time()\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\nt1 = time()\nprint('Extremely Randomized Trees: {}'.format(t1 - t0))\n\nRandom Frorest: 9.64048957824707\nExtremely Randomized Trees: 1.4413414001464844\n\n\n\n\n4.1.7 Gini importance\nAfter training a Decision Tree, we could look at each node. Each split is against a feature, which decrease the Gini impurity the most. In other words, we could say that the feature is the most important during the split.\nUsing the average Gini impurity decreased as a metric, we could measure the importance of each feature. This is called Gini importance. If the feature is useful, it tends to split mixed labeled nodes into pure single class nodes.\nIn the case of random forest, since there are many trees, we might compute the weighted average of the Gini importance across all trees. The weight depends on how many times the feature is used in a specific node.\nUsing RandomForestClassifier, we can directly get access to the Gini importance of each feature by .feature_importance_. Please see the following example.\n\nrnd_clf.fit(X_train, y_train)\nrnd_clf.feature_importances_\n\narray([0.43823588, 0.56176412])\n\n\nIn this example, you may see that the two features are relavely equally important, where the second feature is slightly more important since on average it decrease the Gini impurity a little bit more.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#voting-machine",
    "href": "contents/4/intro.html#voting-machine",
    "title": "4  Ensemble methods",
    "section": "4.2 Voting machine",
    "text": "4.2 Voting machine\n\n4.2.1 Voting classifier\nAssume that we have several trained classifiers. The easiest way to make a better classifer out of what we already have is to build a voting system. That is, each classifier give its own prediction, and it will be considered as a vote, and finally the highest vote will be the prediction of the system.\nIn sklearn, you may use VotingClassifier. It works as follows.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclfs = [('knn', KNeighborsClassifier(n_neighbors=5)),\n        ('dt', DecisionTreeClassifier(max_depth=2))]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\n\nAll classifiers are stored in the list clfs, whose elements are tuples. The syntax is very similar to Pipeline. What the classifier does is to train all listed classifiers and use the majority vote to predict the class of given test data. If each classifier has one vote, the voting method is hard. There is also a soft voting method. In this case, every classifiers not only can predict the classes of the given data, but also estimiate the probability of the given data that belongs to certain classes. On coding level, each classifier should have the predict_proba() method. In this case, the weight of each vote is determined by the probability computed. In our course we mainly use hard voting.\nLet us use make_moon as an example. We first load the dataset.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nWe would like to apply kNN model. As before, we build a data pipeline pipe to first apply MinMaxScaler and then KNeighborsClassifier.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline(steps=[('scalar', MinMaxScaler()),\n                       ('knn', KNeighborsClassifier())])\nparameters = {'knn__n_neighbors': list(range(1, 51))}\ngs_knn = GridSearchCV(pipe, param_grid=parameters) \ngs_knn.fit(X_train, y_train)\nclf_knn = gs_knn.best_estimator_\nclf_knn.score(X_test, y_test)\n\n0.8633333333333333\n\n\nThe resulted accuracy is shown above.\nWe then try it with the Decision Tree.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ngs_dt = GridSearchCV(DecisionTreeClassifier(), param_grid={'max_depth': list(range(1, 11)), 'max_leaf_nodes': list(range(10, 30))})\ngs_dt.fit(X_train, y_train)\nclf_dt = gs_dt.best_estimator_\nclf_dt.score(X_test, y_test)\n\n0.8606666666666667\n\n\nWe would also want to try Logistic regression method. This will be covered in the next Chapter. At current stage we just use the default setting without changing any hyperparameters.\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(X_train, y_train)\nclf_lr.score(X_test, y_test)\n\n0.8266666666666667\n\n\nNow we use a voting classifier to combine the results.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = [('knn', KNeighborsClassifier()),\n        ('dt', DecisionTreeClassifier()),\n        ('lr', LogisticRegression())]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n0.8346666666666667\n\n\nYou may compare the results of all these four classifiers. The voting classifier is not guaranteed to be better. It is just a way to form a model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#adaboost",
    "href": "contents/4/intro.html#adaboost",
    "title": "4  Ensemble methods",
    "section": "4.3 AdaBoost",
    "text": "4.3 AdaBoost\nThis is the first algorithm that successfully implements the boosting idea. AdaBoost is short for Adaptive Boosting.\n\n4.3.1 Weighted dataset\nWe firstly talk about training a Decision Tree on a weighted dataset. The idea is very simple. When building a Decision Tree, we use some method to determine the split. In this course the Gini impurity is used. There are at least two other methods: cross-entropy and misclassified rate. For all three, the count of the elemnts in some classes is the essnetial part. To train the model over the weighted dataset, we just need to upgrade the count of the elements by the weighted count.\n\nExample 4.1 Consider the following data:\n\n\n\n\n\n\n\nx0\nx1\ny\nWeight\n\n\n\n\n1.0\n2.1\n+\n0.5\n\n\n1.0\n1.1\n+\n0.125\n\n\n1.3\n1.0\n-\n0.125\n\n\n1.0\n1.0\n-\n0.125\n\n\n2.0\n1.0\n+\n0.125\n\n\n\n\n\nThe weighted Gini impurity is\n\\[\n\\text{WeightedGini}=1-(0.5+0.125+0.125)^2-(0.125+0.125)^2=0.375.\n\\]\nYou may see that the original Gini impurity is just the weighted Gini impurity with equal weights. Therefore the first tree we get from AdaBoost (see below) is the same tree we get from the Decision Tree model in Chpater 3.\n\n\n\n4.3.2 General process\nHere is the rough description of AdaBoost.\n\nAssign weights to each data point. At the begining we could assign weights equally.\nTrain a classifier based on the weighted dataset, and use it to predict on the training set. Find out all wrong answers.\nAdjust the weights, by inceasing the weights of data points that are done wrongly in the previous generation.\nTrain a new classifier using the new weighted dataset. Predict on the training set and record the wrong answers.\nRepeat the above process to get many classifiers. The training stops either by hitting \\(0\\) error rate, or after a specific number of rounds.\nThe final results is based on the weighted total votes from all classifiers we trained.\n\nNow let us talk about the details. Assume there are \\(N\\) data points. Then the inital weights are set to be \\(\\dfrac1N\\). There are 2 sets of weights. Let \\(w^{(i)}\\) be weights of the \\(i\\)th data points. Let \\(\\alpha_j\\) be the weights of the \\(j\\)th classifier. After training the \\(j\\)th classifier, the error rate is denoted by \\(e_j\\). Then we have\n\\[\ne_j=\\frac{\\text{the total weights of data points that are misclassified by the $j$th classifier}}{\\text{the total weights of data points}}\n\\]\n\\[\n\\alpha_j=\\eta\\ln\\left(\\dfrac{1-e_j}{e_j}\\right).\n\\]\n\\[\nw^{(i)}_{\\text{new}}\\leftarrow\\text{normalization} \\leftarrow w^{(i)}\\leftarrow\\begin{cases}w^{(i)}&\\text{if the $i$th data is correctly classified,}\\\\w^{(i)}\\exp(\\alpha_j)&\\text{if the $i$th data is misclassified.}\\end{cases}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe first tree is the same tree we get from the regular Decision Tree model. In the rest of the training process, more weights are put on the data that we are wrong in the previous iteration. Therefore the process is the mimic of “learning from mistakes”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe \\(\\eta\\) in computing \\(\\alpha_j\\) is called the learning rate. It is a hyperparameter that will be specified mannually. It does exactly what it appears to do: alter the weights of each classifier. The default is 1.0. When the number is very small (which is recommended although it can be any positive number), more iterations will be expected.\n\n\n\n\n4.3.3 Example 1: the iris dataset\nSimilar to all previous models, sklearn provides AdaBoostClassifier. The way to use it is similar to previous models. Note that although we are able to use any classifiers for AdaBoost, the most popular choice is Decision Tree with max_depth=1. This type of Decision Trees are also called Decision Stumps.\nIn the following examples, we initialize an AdaBoostClassifier with 500 Decision Stumps and learning_rate=0.5.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=1000,\n                             learning_rate=.5)\n\nWe will use the iris dataset for illustration. The cross_val_score is calculated as follows.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nscores = cross_val_score(ada_clf, X, y, cv=5)\nscores.mean()\n\n0.9533333333333334\n\n\n\n\n4.3.4 Example 2: the Horse Colic dataset\nThis dataset is from UCI Machine Learning Repository. The data is about whether horses survive if they get a disease called Colic. The dataset is preprocessed as follows. Note that there are a few missing values inside, and we replace them with 0.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\ndf = df.fillna(0)\nX = df.iloc[:, 1:].to_numpy().astype(float)\ny = df[0].to_numpy().astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nclf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.2)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.6222222222222222",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#exercises",
    "href": "contents/4/intro.html#exercises",
    "title": "4  Ensemble methods",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\nExercise 4.1 CHOOSE ONE: Please apply the random forest to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the cross-validation score and the OOB score of your model and compare it with the models you learned before (kNN, Decision Trees).\nPlease find some typical features (using the Gini importance) and draw the Decision Boundary against the features you choose.\n\n\n\nExercise 4.2 Please use the following code to get the mgq dataset.\n\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300,\n                                 n_features=2, n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, -y2 + 1))\n\nPlease build an AdaBoost model.\n\n\nExercise 4.3 Please use RandomForestClassifier, ExtraTreesClassifier and KNeighbourClassifier to form a voting classifier, and apply to the MNIST dataset.\n\n\n\n\n\n\n\nMNIST\n\n\n\nThis dataset can be loaded using the following code.\n\nimport numpy as np\nimport requests\nfrom io import BytesIO\nr = requests.get('https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz', stream = True) \ndata = np.load(BytesIO(r.raw.read()))\nX_train = data['x_train']\nX_test = data['x_test']\ny_train = data['y_train']\ny_test = data['y_test']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html",
    "href": "contents/5/intro.html",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1 Basic idea\nAssume that we have a binary classfification problem with \\(N\\) features. Our model starts from the logit instead of the label \\(y\\) itself.\n\\[\nlogit(y)=\\theta_0+\\sum_{j=1}^N\\theta_jx_j.\n\\]\nThe logit function is used to describe the logorithm of the binary odds. The odd ratio is the ratio between the probability of success and the probability of failure. Assume the probability of success is \\(p\\). Then\n\\[\noddratio(p)=\\frac{p}{1-p},\\quad logit(p)=z = \\log\\qty(\\frac{p}{1-p}).\n\\] We could solve the logit function, and get its inverse: the function is the Sigmoid function. Once we have the logit value, we could use it to get the probability. \\[\np=\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n\\]\nTherefore the model for Logistic regression is as follows:\n\\[\np=\\sigma(L(x))=\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\sigma\\left(\\Theta \\hat{x}^T\\right).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#basic-idea",
    "href": "contents/5/intro.html#basic-idea",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1.1 Sigmoid function\nThe Sigmoid function is defined as follows:\n\\[\n\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n\\] The graph of the function is shown below.\n\n\n\n\n\n\n\n\n\nThe main properties of \\(\\sigma\\) are listed below as a Lemma.\n\nLemma 6.1 The Sigmoid function \\(\\sigma(z)\\) satisfies the following properties.\n\n\\(\\sigma(z)\\rightarrow \\infty\\) when \\(z\\mapsto \\infty\\).\n\\(\\sigma(z)\\rightarrow -\\infty\\) when \\(z\\mapsto -\\infty\\).\n\\(\\sigma(0)=0.5\\).\n\\(\\sigma(z)\\) is always increasing.\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\).\n\n\n\nSolution. We will only look at the last one.\n\\[\n\\begin{split}\n\\sigma'(z)&=-\\frac{(1+\\mathrm e^{-z})'}{(1+\\mathrm e^{-z})^2}=\\frac{\\mathrm e^{-z}}{(1+\\mathrm e^{-z})^2}=\\frac{1}{1+\\mathrm e^{-z}}\\frac{\\mathrm e^{-z}}{1+\\mathrm e^{-z}}\\\\\n&=\\sigma(z)\\left(\\frac{1+\\mathrm e^{-z}}{1+\\mathrm e^{-z}}-\\frac{1}{1+\\mathrm e^{-z}}\\right)=\\sigma(z)(1-\\sigma(z)).\n\\end{split}\n\\]\n\n\n\n6.1.2 Gradient descent\n\n\nWe would like to use Gradient descent to sovle Logistic regression problems. For binary classification problem, the cost function is defined to be\n\\[\nJ(\\Theta)=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\log(p^{(i)})+(1-y^{(i)})\\log(1-p^{(i)})\\right].\n\\] Here \\(m\\) is the number of data points, \\(y^{(i)}\\) is the labelled result (which is either \\(0\\) or \\(1\\)), \\(p^{(i)}\\) is the predicted value (which is between \\(0\\) and \\(1\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe algorithm gets its name since we are using the gradient to find a direction to lower our height.\n\n\n\n\n6.1.3 The Formulas\n\nTheorem 6.1 The gradient of \\(J\\) is computed by\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\tag{6.1}\\]\n\n\n\nClick for details.\n\n\nProof. The formula is an application of the chain rule for the multivariable functions.\n\\[\n\\begin{split}\n\\dfrac{\\partial p}{\\partial \\theta_k}&=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma(L(\\Theta))\\\\\n&=\\sigma(L)(1-\\sigma(L))\\dfrac{\\partial}{\\partial \\theta_k}\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)\\\\\n&=\\begin{cases}\np(1-p)&\\text{ if }k=0,\\\\\np(1-p)x_k&\\text{ otherwise}.\n\\end{cases}\n\\end{split}\n\\] Then\n\\[\n\\nabla p = \\left(\\frac{\\partial p}{\\partial\\theta_0},\\ldots,\\frac{\\partial p}{\\partial\\theta_n}\\right) = p(1-p)\\hat{x}.\n\\]\nThen\n\\[\n\\nabla \\log(p) = \\frac{\\nabla p}p =\\frac{p(1-p)\\hat{x}}{p}=(1-p)\\hat{x}.\n\\]\n\\[\n\\nabla \\log(1-p) = \\frac{-\\nabla p}{1-p} =-\\frac{p(1-p)\\hat{x}}{1-p}=-p\\hat{x}.\n\\]\nThen\n\\[\n\\begin{split}\n\\nabla J& = -\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\nabla \\log(p^{(i)})+(1-y^{(i)})\\nabla \\log(1-p^{(i)})\\right]\\\\\n&=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}(1-p^{(i)})\\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\\hat{x}^{(i)})\\right]\\\\\n&=-\\frac1m\\sum_{i=1}^m\\left[(y^{(i)}-p^{(i)})\\hat{x}^{(i)}\\right].\n\\end{split}\n\\]\nWe write \\(\\hat{x}^{(i)}\\) as row vectors, and stack all these row vectors vertically. What we get is a matrix \\(\\hat{\\textbf X}\\) of the size \\(m\\times (1+n)\\). We stack all \\(y^{(i)}\\) (resp. \\(p^{(i)}\\)) vectically to get the \\(m\\)-dim column vector \\(\\textbf y\\) (resp. \\(\\textbf p\\)).\nUsing this notation, the previous formula becomes\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\]\nAfter the gradient can be computed, we can start to use the gradient descent method. Note that, although \\(\\Theta\\) are not explicitly presented in the formula of \\(\\nabla J\\), this is used to modify \\(\\Theta\\):\n\\[\n\\Theta_{s+1} = \\Theta_s - \\alpha\\nabla J.\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you directly use library, like sklearn or PyTorch, they will handle the concrete computation of these gradients.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#regularization",
    "href": "contents/5/intro.html#regularization",
    "title": "6  Logistic regression",
    "section": "6.2 Regularization",
    "text": "6.2 Regularization\n\n6.2.1 Three types of errors\nEvery estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.\n\n\n6.2.2 Underfit vs Overfit\nWhen fit a model to data, it is highly possible that the model is underfit or overfit.\nRoughly speaking, underfit means the model is not sufficient to fit the training samples, and overfit means that the models learns too many noise from the data. In many cases, high bias is related to underfit, and high variance is related to overfit.\nThe following example is from the sklearn guide. Although it is a polynomial regression example, it grasps the key idea of underfit and overfit.\n\n\n\n\n\n\n\n\n\n\n\n6.2.3 Learning curves (accuracy vs training size)\nA learning curve shows the validation and training score of an estimator for varying a key hyperparameter. In most cases the key hyperparameter is the training size or the number of epochs. It is a tool to find out how much we benefit from altering the hyperparameter by training more data or training for more epochs, and whether the estimator suffers more from a variance error or a bias error.\nsklearn provides sklearn.model_selection.learning_curve() to generate the values that are required to plot such a learning curve. However this function is just related to the sample size. If we would like to talk about epochs, we need other packages.\nLet us first look at the learning curve about sample size. The official document page is here. The function takes input estimator, dataset X, y, and an arry-like argument train_sizes. The dataset (X, y) will be split into pieces using the cross-validation technique. The number of pieces is set by the argument cv. The default value is cv=5. For details about cross-validation please see Section 2.2.6.\nThen the model is trained over a random sample of the training set, and evaluate the score over the test set. The size of the sample of the training set is set by the argument train_sizes. This argument is array-like. Therefore the process will be repeated several times, and we can see the impact of increasing the training size.\nThe output contains three pieces. The first is train_sizes_abs which is the number of elements in each training set. This output is mainly for reference. The difference between the output and the input train_sizes is that the input can be float which represents the percentagy. The output is always the exact number of elements.\nThe second output is train_scores and the third is test_scores, both of which are the scores we get from the training and testing process. Note that both are 2D numpy arrays, of the size (number of different sizes, cv). Each row is a 1D numpy array representing the cross-validation scores, which is corresponding to a train size. If we want the mean as the cross-validation score, we could use train_scores.mean(axis=1).\nAfter understanding the input and output, we could plot the learning curve. We still use the horse colic as the example. The details about the dataset can be found here.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nC:\\Users\\Xinli\\AppData\\Local\\Temp\\ipykernel_65712\\73942173.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  df = pd.read_csv(url, delim_whitespace=True, header=None)\nC:\\Users\\Xinli\\AppData\\Local\\Temp\\ipykernel_65712\\73942173.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[23].replace({1: 1, 2: 0}, inplace=True)\n\n\nWe use the model LogisticRegression. The following code plot the learning curve for this model.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nclf = LogisticRegression(max_iter=1000)\nsteps = [('scalar', MinMaxScaler()),\n         ('log', clf)]\npipe = Pipeline(steps=steps)\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\ntrain_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n                                                        train_sizes=np.linspace(0.1, 1, 20))\n\nimport matplotlib.pyplot as plt\nplt.plot(train_sizes, train_scores.mean(axis=1), label='train')\nplt.plot(train_sizes, test_scores.mean(axis=1), label='test')\nplt.legend()\n\n\n\n\n\n\n\n\nThe learning curve is a primary tool for us to study the bias and variance. Usually\n\nIf the two training curve and the testing curve are very close to each other, this means that the variance is low. Otherwise the variance is high, and this means that the model probabily suffer from overfitting.\nIf the absolute training curve score is high, this means that the bias is low. Otherwise the bias is high, and this means that the model probabily suffer from underfitting.\n\nIn the above example, although regularization is applied by default, you may still notice some overfitting there.\n\n\n6.2.4 Regularization\nRegularization is a technique to deal with overfitting. Here we only talk about the simplest method: ridge regression, also known as Tikhonov regularizaiton. Because of the formula given below, it is also called \\(L_2\\) regularization. The idea is to add an additional term \\(\\dfrac{\\alpha}{2m}\\sum_{i=1}^m\\theta_i^2\\) to the original cost function. When training with the new cost function, this additional term will force the parameters in the original term to be as small as possible. After finishing training, the additional term will be dropped, and we use the original cost function for validation and testing. Note that in the additional term \\(\\theta_0\\) is not presented.\nThe hyperparameter \\(\\alpha\\) is the regularization strength. If \\(\\alpha=0\\), the new cost function becomes the original one; If \\(\\alpha\\) is very large, the additional term dominates, and it will force all parameters to be almost \\(0\\). In different context, the regularization strength is also given by \\(C=\\dfrac{1}{2\\alpha}\\), called inverse of regularization strength.\n\n6.2.4.1 The math of regularization\n\nTheorem 6.2 The gradient of the ridge regression cost function is\n\\[\n\\nabla J=\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}+\\frac{\\alpha}{m}\\Theta.\n\\]\nNote that \\(\\Theta\\) doesn’t contain \\(\\theta_0\\), or you may treat \\(\\theta_0=0\\).\n\nThe computation is straightforward.\n\n\n6.2.4.2 The code\nRegularization is directly provided by the logistic regression functions.\n\nIn LogisticRegression, the regularization is given by the argument penalty and C. penalty specifies the regularizaiton method. It is l2 by default, which is the method above. C is the inverse of regularization strength, whose default value is 1.\nIn SGDClassifier, the regularization is given by the argument penalty and alpha. penalty is the same as that in LogisticRegression, and alpha is the regularization strength, whose default value is 0.0001.\n\nLet us see the above example.\n\nclf = LogisticRegression(max_iter=1000, C=0.1)\nsteps = [('scalar', MinMaxScaler()),\n         ('log', clf)]\npipe = Pipeline(steps=steps)\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\ntrain_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n                                                        train_sizes=np.linspace(0.1, 1, 20))\n\nimport matplotlib.pyplot as plt\nplt.plot(train_sizes, train_scores.mean(axis=1), label='train')\nplt.plot(train_sizes, test_scores.mean(axis=1), label='test')\nplt.legend()\n\n\n\n\n\n\n\n\nAfter we reduce C from 1 to 0.1, the regularization strength is increased. Then you may find that the gap between the two curves are reduced. However the overall performace is also reduced, from 85%~90% in C=1 case to around 80% in C=0.1 case. This means that the model doesn’t fit the data well as the previous one. Therefore this is a trade-off: decrease the variance but increase the bias.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#neural-network-implement-of-logistic-regression",
    "href": "contents/5/intro.html#neural-network-implement-of-logistic-regression",
    "title": "6  Logistic regression",
    "section": "6.3 Neural network implement of Logistic regression",
    "text": "6.3 Neural network implement of Logistic regression\nIn the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. We are going to use PyTorch to implement it. We will reuse many codes we wrote in the previous chapter.\n\n6.3.1 A simple example\nWe\n\n\n6.3.2 Example\nWe still use the horse colic dataset as an example. We first prepare the dataset.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, sep='\\\\s+', header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf = df.drop(columns=[2, 24, 25, 26, 27])\ndf[23] = df[23].replace({1: 1, 2: 0})\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nSEED = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=SEED)\n\nWe need to perform normalization before throwing the data into the model. Here we use the MinMaxScaler() from sklearn package.\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nX_train = mms.fit_transform(X_train, y_train)\nX_test = mms.transform(X_test)\n\nThen we write a Dataset class to build the dataset and create the dataloaders. Since the set is already split, we don’t need to random_split here.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MyData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=float)\n        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.X[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\n\ntrain_set = MyData(X_train, y_train)\nval_set = MyData(X_test, y_test)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32)\n\nIn the following code, we first set up the original model.\n\nimport torch.nn as nn\nfrom torch.nn.modules import Linear\n\nclass LoR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.linear = Linear(in_features=22, out_features=1, dtype=float)\n        self.activation = nn.Sigmoid()\n\n    def forward(self, X):\n        # pred = self.activation(self.linear(X))\n        pred = self.linear(X)\n        # return (pred &gt;= 0).float()\n        return pred\n\nThen we derive the base ModelTemplate class.\n\nclass LoRModel(ModelTemplate):\n    def __init__(self, model, loss_fn, optimizer):\n        super().__init__(model, loss_fn, optimizer)\n        self.stats['acc_train'] = []\n        self.stats['acc_val'] = []\n\n    def compute_acc(self, dataloader):\n        with torch.no_grad():\n            acc = []\n            for X_batch, y_batch in dataloader:\n                yhat = torch.sigmoid(self.model(X_batch))\n                y_pred = (yhat&gt;=0.5).to(float)\n                acc.append((y_pred==y_batch).sum().item())\n            # print(acc_train)\n        return np.sum(acc)/len(dataloader.dataset)\n\n    def log_update(self, train_time, loss, val_time, val_loss, train_loader, val_loader):\n        super().log_update(train_time, loss, val_time, val_loss, train_loader, val_loader)\n        acc_train = self.compute_acc(train_loader)\n        acc_val = self.compute_acc(val_loader)\n        self.stats['acc_train'].append(acc_train)\n        self.stats['acc_val'].append(acc_val)\n\n\n        # p = self.model.state_dict()\n        # self.stats['acc'].append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n    def log_output(self, verbose=0):\n        s = super().log_output(verbose=0, formatstr=':.6f')\n        s.append(f'acc_train: {self.stats['acc_train'][-1]:.6f}')\n        s.append(f'acc_val: {self.stats['acc_val'][-1]:.6f}')\n        # s.append(f'p: [{self.stats['p'][-1][0]:.6f}, {self.stats['p'][-1][1]:.6f}]')\n        if verbose == 1:\n            print(' '.join(s))\n        return s\n\n\nfrom torch.optim import SGD\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\n\noriginal_model = LoR()\nmodel = LoRModel(model=original_model, loss_fn=BCEWithLogitsLoss(),\n                 optimizer=SGD(original_model.parameters(), lr = 0.1))\n\nmodel.train(train_loader, val_loader, epoch_num=100, verbose=1)\n\nepoch 1 train_time: 0.009149 loss: 0.674627 val_time: 0.001110 val_loss: 0.619009 acc_train: 0.627451 acc_val: 0.688889\nepoch 2 train_time: 0.004470 loss: 0.647466 val_time: 0.001009 val_loss: 0.606279 acc_train: 0.627451 acc_val: 0.688889\nepoch 3 train_time: 0.004992 loss: 0.631382 val_time: 0.001513 val_loss: 0.587835 acc_train: 0.627451 acc_val: 0.688889\nepoch 4 train_time: 0.004513 loss: 0.619163 val_time: 0.000999 val_loss: 0.579341 acc_train: 0.631373 acc_val: 0.688889\nepoch 5 train_time: 0.005146 loss: 0.609541 val_time: 0.000000 val_loss: 0.570777 acc_train: 0.631373 acc_val: 0.711111\nepoch 6 train_time: 0.005018 loss: 0.600289 val_time: 0.000999 val_loss: 0.564059 acc_train: 0.639216 acc_val: 0.711111\nepoch 7 train_time: 0.005141 loss: 0.590659 val_time: 0.001001 val_loss: 0.561695 acc_train: 0.654902 acc_val: 0.688889\nepoch 8 train_time: 0.006529 loss: 0.581817 val_time: 0.001006 val_loss: 0.556573 acc_train: 0.658824 acc_val: 0.666667\nepoch 9 train_time: 0.006056 loss: 0.575897 val_time: 0.001006 val_loss: 0.555410 acc_train: 0.701961 acc_val: 0.711111\nepoch 10 train_time: 0.004529 loss: 0.568207 val_time: 0.001007 val_loss: 0.549390 acc_train: 0.709804 acc_val: 0.711111\nepoch 11 train_time: 0.006530 loss: 0.561794 val_time: 0.000997 val_loss: 0.543227 acc_train: 0.717647 acc_val: 0.711111\nepoch 12 train_time: 0.006521 loss: 0.556270 val_time: 0.001000 val_loss: 0.538794 acc_train: 0.729412 acc_val: 0.711111\nepoch 13 train_time: 0.006516 loss: 0.551529 val_time: 0.001503 val_loss: 0.538478 acc_train: 0.745098 acc_val: 0.711111\nepoch 14 train_time: 0.006579 loss: 0.546595 val_time: 0.001001 val_loss: 0.537497 acc_train: 0.749020 acc_val: 0.733333\nepoch 15 train_time: 0.006550 loss: 0.543805 val_time: 0.000991 val_loss: 0.533154 acc_train: 0.752941 acc_val: 0.733333\nepoch 16 train_time: 0.006036 loss: 0.537000 val_time: 0.000999 val_loss: 0.530011 acc_train: 0.752941 acc_val: 0.733333\nepoch 17 train_time: 0.004514 loss: 0.533244 val_time: 0.001003 val_loss: 0.526669 acc_train: 0.760784 acc_val: 0.733333\nepoch 18 train_time: 0.005025 loss: 0.529177 val_time: 0.001003 val_loss: 0.525024 acc_train: 0.764706 acc_val: 0.733333\nepoch 19 train_time: 0.006732 loss: 0.525873 val_time: 0.001000 val_loss: 0.523801 acc_train: 0.776471 acc_val: 0.733333\nepoch 20 train_time: 0.008019 loss: 0.521512 val_time: 0.001125 val_loss: 0.522176 acc_train: 0.780392 acc_val: 0.755556\nepoch 21 train_time: 0.006521 loss: 0.518520 val_time: 0.000000 val_loss: 0.519269 acc_train: 0.780392 acc_val: 0.755556\nepoch 22 train_time: 0.006311 loss: 0.515101 val_time: 0.001008 val_loss: 0.515778 acc_train: 0.776471 acc_val: 0.733333\nepoch 23 train_time: 0.006001 loss: 0.513203 val_time: 0.001000 val_loss: 0.519432 acc_train: 0.784314 acc_val: 0.755556\nepoch 24 train_time: 0.005550 loss: 0.509660 val_time: 0.000999 val_loss: 0.515220 acc_train: 0.784314 acc_val: 0.777778\nepoch 25 train_time: 0.004506 loss: 0.506943 val_time: 0.001003 val_loss: 0.512260 acc_train: 0.788235 acc_val: 0.777778\nepoch 26 train_time: 0.005409 loss: 0.505380 val_time: 0.001000 val_loss: 0.513752 acc_train: 0.780392 acc_val: 0.755556\nepoch 27 train_time: 0.007043 loss: 0.501728 val_time: 0.000000 val_loss: 0.513422 acc_train: 0.780392 acc_val: 0.755556\nepoch 28 train_time: 0.004558 loss: 0.500234 val_time: 0.000000 val_loss: 0.508129 acc_train: 0.780392 acc_val: 0.755556\nepoch 29 train_time: 0.005512 loss: 0.497392 val_time: 0.001000 val_loss: 0.510038 acc_train: 0.784314 acc_val: 0.755556\nepoch 30 train_time: 0.004509 loss: 0.495293 val_time: 0.001000 val_loss: 0.506375 acc_train: 0.784314 acc_val: 0.755556\nepoch 31 train_time: 0.006531 loss: 0.493891 val_time: 0.001010 val_loss: 0.508324 acc_train: 0.784314 acc_val: 0.755556\nepoch 32 train_time: 0.006678 loss: 0.490733 val_time: 0.001011 val_loss: 0.502308 acc_train: 0.784314 acc_val: 0.755556\nepoch 33 train_time: 0.006102 loss: 0.489358 val_time: 0.001507 val_loss: 0.504019 acc_train: 0.788235 acc_val: 0.755556\nepoch 34 train_time: 0.006013 loss: 0.486646 val_time: 0.001000 val_loss: 0.504261 acc_train: 0.784314 acc_val: 0.755556\nepoch 35 train_time: 0.005564 loss: 0.485487 val_time: 0.000976 val_loss: 0.502926 acc_train: 0.784314 acc_val: 0.755556\nepoch 36 train_time: 0.005718 loss: 0.483562 val_time: 0.001007 val_loss: 0.504548 acc_train: 0.792157 acc_val: 0.755556\nepoch 37 train_time: 0.006582 loss: 0.481137 val_time: 0.001072 val_loss: 0.502033 acc_train: 0.792157 acc_val: 0.755556\nepoch 38 train_time: 0.005511 loss: 0.479781 val_time: 0.001003 val_loss: 0.501860 acc_train: 0.796078 acc_val: 0.755556\nepoch 39 train_time: 0.006415 loss: 0.478170 val_time: 0.001165 val_loss: 0.500380 acc_train: 0.800000 acc_val: 0.755556\nepoch 40 train_time: 0.006022 loss: 0.476871 val_time: 0.001000 val_loss: 0.498162 acc_train: 0.796078 acc_val: 0.755556\nepoch 41 train_time: 0.005652 loss: 0.475505 val_time: 0.001009 val_loss: 0.499792 acc_train: 0.796078 acc_val: 0.777778\nepoch 42 train_time: 0.006061 loss: 0.473835 val_time: 0.001005 val_loss: 0.498708 acc_train: 0.796078 acc_val: 0.777778\nepoch 43 train_time: 0.006543 loss: 0.472051 val_time: 0.001000 val_loss: 0.501097 acc_train: 0.796078 acc_val: 0.777778\nepoch 44 train_time: 0.007028 loss: 0.470318 val_time: 0.001007 val_loss: 0.497102 acc_train: 0.800000 acc_val: 0.777778\nepoch 45 train_time: 0.007029 loss: 0.468182 val_time: 0.000000 val_loss: 0.494473 acc_train: 0.800000 acc_val: 0.777778\nepoch 46 train_time: 0.006011 loss: 0.467393 val_time: 0.001154 val_loss: 0.495783 acc_train: 0.796078 acc_val: 0.777778\nepoch 47 train_time: 0.005069 loss: 0.465967 val_time: 0.000514 val_loss: 0.495563 acc_train: 0.796078 acc_val: 0.777778\nepoch 48 train_time: 0.006578 loss: 0.465339 val_time: 0.001000 val_loss: 0.495331 acc_train: 0.796078 acc_val: 0.777778\nepoch 49 train_time: 0.005827 loss: 0.463593 val_time: 0.001001 val_loss: 0.493717 acc_train: 0.796078 acc_val: 0.777778\nepoch 50 train_time: 0.007028 loss: 0.462370 val_time: 0.001006 val_loss: 0.492275 acc_train: 0.800000 acc_val: 0.777778\nepoch 51 train_time: 0.007019 loss: 0.461523 val_time: 0.000518 val_loss: 0.493138 acc_train: 0.796078 acc_val: 0.800000\nepoch 52 train_time: 0.005528 loss: 0.460417 val_time: 0.001007 val_loss: 0.494454 acc_train: 0.800000 acc_val: 0.777778\nepoch 53 train_time: 0.006044 loss: 0.459334 val_time: 0.001998 val_loss: 0.492033 acc_train: 0.796078 acc_val: 0.800000\nepoch 54 train_time: 0.005609 loss: 0.457327 val_time: 0.001000 val_loss: 0.487873 acc_train: 0.803922 acc_val: 0.777778\nepoch 55 train_time: 0.006538 loss: 0.456415 val_time: 0.001002 val_loss: 0.487303 acc_train: 0.803922 acc_val: 0.777778\nepoch 56 train_time: 0.005528 loss: 0.455030 val_time: 0.000999 val_loss: 0.486743 acc_train: 0.803922 acc_val: 0.777778\nepoch 57 train_time: 0.004508 loss: 0.454154 val_time: 0.001000 val_loss: 0.487643 acc_train: 0.803922 acc_val: 0.800000\nepoch 58 train_time: 0.004515 loss: 0.453704 val_time: 0.000999 val_loss: 0.483372 acc_train: 0.807843 acc_val: 0.777778\nepoch 59 train_time: 0.004568 loss: 0.451267 val_time: 0.001021 val_loss: 0.484756 acc_train: 0.807843 acc_val: 0.800000\nepoch 60 train_time: 0.006513 loss: 0.451604 val_time: 0.000000 val_loss: 0.486329 acc_train: 0.803922 acc_val: 0.777778\nepoch 61 train_time: 0.005353 loss: 0.449811 val_time: 0.001000 val_loss: 0.484585 acc_train: 0.803922 acc_val: 0.777778\nepoch 62 train_time: 0.005524 loss: 0.449721 val_time: 0.000998 val_loss: 0.482820 acc_train: 0.807843 acc_val: 0.800000\nepoch 63 train_time: 0.005516 loss: 0.447913 val_time: 0.001507 val_loss: 0.482491 acc_train: 0.803922 acc_val: 0.777778\nepoch 64 train_time: 0.007041 loss: 0.448823 val_time: 0.001000 val_loss: 0.482010 acc_train: 0.803922 acc_val: 0.777778\nepoch 65 train_time: 0.005012 loss: 0.446369 val_time: 0.001503 val_loss: 0.483129 acc_train: 0.800000 acc_val: 0.755556\nepoch 66 train_time: 0.006522 loss: 0.445447 val_time: 0.002009 val_loss: 0.480999 acc_train: 0.800000 acc_val: 0.777778\nepoch 67 train_time: 0.005015 loss: 0.445404 val_time: 0.001000 val_loss: 0.486650 acc_train: 0.796078 acc_val: 0.777778\nepoch 68 train_time: 0.005522 loss: 0.444460 val_time: 0.001000 val_loss: 0.487049 acc_train: 0.796078 acc_val: 0.777778\nepoch 69 train_time: 0.006112 loss: 0.442527 val_time: 0.001505 val_loss: 0.482234 acc_train: 0.800000 acc_val: 0.777778\nepoch 70 train_time: 0.006519 loss: 0.442984 val_time: 0.001000 val_loss: 0.482030 acc_train: 0.800000 acc_val: 0.777778\nepoch 71 train_time: 0.004506 loss: 0.440611 val_time: 0.001512 val_loss: 0.480820 acc_train: 0.803922 acc_val: 0.777778\nepoch 72 train_time: 0.006124 loss: 0.440156 val_time: 0.000894 val_loss: 0.480842 acc_train: 0.800000 acc_val: 0.777778\nepoch 73 train_time: 0.005010 loss: 0.438593 val_time: 0.001000 val_loss: 0.479417 acc_train: 0.803922 acc_val: 0.777778\nepoch 74 train_time: 0.006529 loss: 0.438962 val_time: 0.001007 val_loss: 0.479688 acc_train: 0.803922 acc_val: 0.777778\nepoch 75 train_time: 0.006538 loss: 0.437273 val_time: 0.000999 val_loss: 0.480948 acc_train: 0.800000 acc_val: 0.777778\nepoch 76 train_time: 0.008034 loss: 0.436836 val_time: 0.000504 val_loss: 0.478903 acc_train: 0.803922 acc_val: 0.777778\nepoch 77 train_time: 0.007015 loss: 0.436190 val_time: 0.000000 val_loss: 0.481736 acc_train: 0.796078 acc_val: 0.777778\nepoch 78 train_time: 0.006032 loss: 0.436840 val_time: 0.001045 val_loss: 0.481888 acc_train: 0.796078 acc_val: 0.777778\nepoch 79 train_time: 0.005521 loss: 0.433567 val_time: 0.001003 val_loss: 0.478438 acc_train: 0.803922 acc_val: 0.777778\nepoch 80 train_time: 0.006090 loss: 0.433801 val_time: 0.001002 val_loss: 0.479732 acc_train: 0.800000 acc_val: 0.777778\nepoch 81 train_time: 0.005529 loss: 0.433351 val_time: 0.001000 val_loss: 0.476616 acc_train: 0.800000 acc_val: 0.777778\nepoch 82 train_time: 0.006518 loss: 0.432495 val_time: 0.001505 val_loss: 0.475177 acc_train: 0.800000 acc_val: 0.777778\nepoch 83 train_time: 0.006561 loss: 0.431201 val_time: 0.001000 val_loss: 0.477437 acc_train: 0.800000 acc_val: 0.777778\nepoch 84 train_time: 0.005056 loss: 0.430826 val_time: 0.001011 val_loss: 0.475507 acc_train: 0.800000 acc_val: 0.777778\nepoch 85 train_time: 0.005032 loss: 0.431591 val_time: 0.001000 val_loss: 0.477952 acc_train: 0.800000 acc_val: 0.777778\nepoch 86 train_time: 0.005018 loss: 0.428879 val_time: 0.001003 val_loss: 0.475768 acc_train: 0.800000 acc_val: 0.777778\nepoch 87 train_time: 0.006157 loss: 0.429469 val_time: 0.000864 val_loss: 0.479029 acc_train: 0.796078 acc_val: 0.777778\nepoch 88 train_time: 0.006541 loss: 0.427537 val_time: 0.002007 val_loss: 0.475638 acc_train: 0.807843 acc_val: 0.777778\nepoch 89 train_time: 0.007041 loss: 0.426961 val_time: 0.001001 val_loss: 0.474318 acc_train: 0.807843 acc_val: 0.777778\nepoch 90 train_time: 0.007595 loss: 0.425945 val_time: 0.000936 val_loss: 0.473740 acc_train: 0.807843 acc_val: 0.777778\nepoch 91 train_time: 0.007540 loss: 0.426258 val_time: 0.001009 val_loss: 0.474088 acc_train: 0.807843 acc_val: 0.777778\nepoch 92 train_time: 0.006417 loss: 0.425213 val_time: 0.001503 val_loss: 0.473682 acc_train: 0.807843 acc_val: 0.777778\nepoch 93 train_time: 0.006039 loss: 0.424503 val_time: 0.001009 val_loss: 0.473505 acc_train: 0.807843 acc_val: 0.777778\nepoch 94 train_time: 0.004512 loss: 0.425619 val_time: 0.001010 val_loss: 0.473489 acc_train: 0.807843 acc_val: 0.777778\nepoch 95 train_time: 0.006037 loss: 0.423174 val_time: 0.001014 val_loss: 0.471925 acc_train: 0.807843 acc_val: 0.777778\nepoch 96 train_time: 0.008095 loss: 0.423395 val_time: 0.000000 val_loss: 0.473106 acc_train: 0.803922 acc_val: 0.777778\nepoch 97 train_time: 0.004768 loss: 0.422295 val_time: 0.003011 val_loss: 0.470060 acc_train: 0.807843 acc_val: 0.777778\nepoch 98 train_time: 0.005531 loss: 0.421285 val_time: 0.001005 val_loss: 0.468067 acc_train: 0.803922 acc_val: 0.777778\nepoch 99 train_time: 0.006522 loss: 0.422199 val_time: 0.000999 val_loss: 0.468618 acc_train: 0.807843 acc_val: 0.777778\nepoch 100 train_time: 0.007038 loss: 0.420444 val_time: 0.001007 val_loss: 0.467997 acc_train: 0.803922 acc_val: 0.777778",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#pytorch-crash-course",
    "href": "contents/5/intro.html#pytorch-crash-course",
    "title": "6  Logistic regression",
    "section": "6.4 Pytorch crash course",
    "text": "6.4 Pytorch crash course\n\n6.4.1 Tensor\nThis is the basic data structure. It is very similar to numpy.ndarray, but with many more features. There are a few things that we need to mention at the beginning.\n\nA tensor with only one item is mathematically equal to a number. In Pytorch, you may use .item() to extract the number from a tensor with only one item.\n\n\nimport torch\n\na = torch.tensor([1])\na\n\ntensor([1])\n\n\n\na.item()\n\n1\n\n\n\nIt is type sensitive. Pytorch expect you to assign the exact data type to each tensor, and it won’t automatically guess it in most cases. You may specify data type when you create a tensor.\n\n\nb = torch.tensor([1], dtype=torch.float64)\nb\n\ntensor([1.], dtype=torch.float64)\n\n\nIf you want to convert data type, you could use .to().\n\nb = torch.tensor([1], dtype=torch.float64)\nb = b.to(torch.int)\nb\n\ntensor([1], dtype=torch.int32)\n\n\nTensor data structure has many other features that will be introduced later.\n\n\n6.4.2 Gradient descent\nTo implement the gradient descent algorithm for the neural network, there would be a series of computations:\n\nFrom the input, feedforward the network to get the output y_pred.\nBased on the real output y_true, compute the loss function loss = loss_fn(y_true, y_pred).\nCompute the gradient based on the information provided. For this step many data are needed. You may look up the gradient descent formula (backprop).\nBased on the gradient computed in Step 3, weights are updated, according to the optimizer we choose.\n\nIn Pytorch, the above steps are implemented as follows.\n\nYou have to define a model function to indicate how to feedforward the network to get an output. Here for a lot of reasons, the typical way is to define a model class, which contains a forward method that can compute the output of the model. Let us consider the following example: the dataset is as follows:\n\n\nx = torch.tensor([[1, 2], [3, 4], [0, 1]], dtype=torch.float)\ny = torch.tensor([[3], [7], [1]], dtype=torch.float)\n\nThe model is defined as follows.\n\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n\n        self.fc = nn.Linear(in_features=2, out_features=1)\n    \n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\nIn this example, we define a 2-input linear regression model. Pytorch doesn’t need the class to work. Actually the minimal working example of the above code is as follows. To put things into a class can make it easier in larger models.\n\ndef model(x):\n    return nn.Linear(in_features=2, out_features=1)(x)\n\nThe reason the model can be written in a very simple way is because the information about computing gradients is recorded in the parameter tensors, on the level of tensors, instead of on the level of the model class. Therefore it is important to get access to the parameters of the model.\n\nmodel = MyModel()\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[-0.3573, -0.2246]], requires_grad=True),\n Parameter containing:\n tensor([-0.2558], requires_grad=True)]\n\n\nNote that the parameters we get here is a iterator. So to look at it we need to convert it inot a list. In this example, there are two sets of tensors: the first is the coefficients, and the second is the bias term. This bias term can be turned on/off by setting the argument bias=True or False when using nn.Linear() to create fully connected layers. The default is True.\nTo evaluate the model, we just directly apply the model to the input tensor.\n\ny_pred = model(x)\ny_pred\n\ntensor([[-1.0623],\n        [-2.2262],\n        [-0.4804]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nYou may use the coefficients provided above to validate the resutl.\nNote that, although we define the .forward() method, we don’t use it explicitly. The reason is that model(x) will not only excute .forward(x) method, but many other operations, like recording many intermediate results that can be used for debugging, visualization and modifying gradients.\n\nWe may define the loss function. We mannually define the MSE loss function.\n\n\ndef loss_fn(y_true, y_pred):\n    return ((y_true-y_pred)**2).mean()\n\nloss = loss_fn(y, y_pred)\nloss\n\ntensor(34.6054, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nNow we need to do gradient descent. The manual way to loss.backward(). What it does is to\n\n\nprint(list(model.parameters()))\nprint(list(model.parameters())[0].grad)\nprint(list(model.parameters())[1].grad)\n\n[Parameter containing:\ntensor([[-0.3573, -0.2246]], requires_grad=True), Parameter containing:\ntensor([-0.2558], requires_grad=True)]\nNone\nNone\n\n\n\nimport torch.optim as optim\noptimizer = optim.SGD(model.parameters(), lr=1e-2)\n\n\nloss.backward()\noptimizer.step()\n\n\nfor i in range(100):\n    optimizer.zero_grad()\n    # print(optimizer.param_groups)\n\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n\n    # print(optimizer.param_groups)\n\n    loss.backward()\n    optimizer.step()\n\n    # print(optimizer.param_groups)\n\n    # print(list(model.parameters()))\n    # print(list(model.parameters())[0].grad)\n    # print(list(model.parameters())[1].grad)\n\n\nUpdate the parameters by optim or manually done.\n\n\n\n6.4.3 Mini-batch\n\n\n\n6.4.4 Codes\nWe will only talk about using packages. sklearn provides two methods to implement the Logistic regression. The API interface is very similar to other models. Later we will use PyTorch and our\nNote that Logistic regression is very sensitive to the scale of features. Therefore we need to normalize the features before throwing them into the model.\nLet’s still take iris as an example.\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nThe first method is sklearn.linear_model.LogisticRegression.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nsteps = [('normalize', MinMaxScaler()),\n         ('log', LogisticRegression())]\n\nlog_reg = Pipeline(steps=steps)\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_test, y_test)\n\n0.9565217391304348\n\n\nNote that this method has an option solver that will set the way to solve the Logistic regression problem, and there is no “stochastic gradient descent” provided. The default solver for this LogsiticRegression is lbfgs which will NOT be discussed in lectures.\nThe second method is sklearn.linear_model.SGDClassifier.\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nsteps = [('normalize', MinMaxScaler()),\n         ('log', SGDClassifier(loss='log_loss', max_iter=100))]\n\nsgd_clf = Pipeline(steps=steps)\nsgd_clf.fit(X_train, y_train)\nsgd_clf.score(X_test, y_test)\n\n0.9565217391304348\n\n\nThis method is the one we discussed in lectures. The log_loss loss function is the binary entropy function we mentioned in lectures. If you change to other loss functions it will become other models.\nFrom the above example, you may notice that SGDClassifier doesn’t perform as well as LogisticRegression. This is due to the algorithm. To make SGDClassifier better you need to tune the hyperparameters, like max_iter, learning_rate/alpha, penalty, etc..\n\n\n\n\n\n\nNote\n\n\n\nThe argument warm_start is used to set whether you want to use your previous model. When set to True, it will reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. The default is False.\nRepeatedly calling fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that for both methods, regularization (which will be discussed later) is applied by default.\n\n\n\n\n6.4.5 Several important side topics\n\n6.4.5.1 Epochs\nWe use epoch to describe feeding data into the model. One Epoch is when an entire dataset is passed through the model once. When using gradient descent, we tend to run several epochs. The number of maximal epochs is one important hyperparameter of this model.\nThe general idea is that more epochs are better for the score of the model, but it will definitely be slower. In addition, sometimes due to many other factors, after a few epochs, the model becomes stall. To train for more epochs cannot improve the model. In this case you have to turn to other methods.\n\n\n6.4.5.2 Batch Gradient Descent vs SGD vs Minibatch\nRecall the Formula Equation 6.1:\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\] We could rewrite this formula:\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}=\\frac1m\\sum_{i=1}^m\\left[(p^{(i)}-y^{(i)})\\hat{x}^{(i)}\\right].\n\\] This new formula can be understood in the following way: For every data point, we could get one gradient direction. Then \\(\\nabla J\\) is the average of all gradient directions. So this algorithm can be expressed as that compute the gradient for every data points and then take the average, and finally update the parameters once. This algorithm is called batch gradient descent.\nFollowing the idea, there is another way to update the model. For every data point, we could compute one gradient direction, and we could use the gradient direction to update the parameters of the model. This algorithm is called stochastic gradient descent.\nThen there is an algrothm living in the middle, called mini-batch gradient descent. In this case, we will group the data set into a collection of subsets of a fiexed number of training examples. Each subset is called a mini-batch, and the fixed number of elements of each mini-batch is called the batch size. Using this method, we will just go through mini-batches one at a time, compute the average of the gradient for these data, and then update the parameters of the model after we finish one mini-batch. Assume that the total number of the dataset is N, the mini-batch size is m. Then there are N/m mini-batches, and during one epoch we will update the model N/m times.\nMini-batch size is one important hyperparameters of this model. Usually the larger the batch size is, the less variance the model has. Then it tends to behave more smoothly, but it will also be slower, and might be stuck to a local minimal. The smaller batch size is more chaotic. It might go faster, but it tends not to converge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#multi-class-case",
    "href": "contents/5/intro.html#multi-class-case",
    "title": "5  Logistic regression",
    "section": "5.5 Multi class case",
    "text": "5.5 Multi class case\n\n5.5.1 Naive idea (one-vs-all)\nAssume that there are \\(N\\) classes. The naive idea is to decompose this \\(N\\)-class classification problem into \\(N\\) binary classification problems. The model will contains \\(N\\) classifiers. The \\(i\\)th classifer is used to classify whehter the given features has the label i or not.\nFor example, asusme we are dealing with the iris dataset. There are 3 classes. By the naive idea, we will modify the labels into Setosa and not Setosa, and use it to train the first classifier. Similarly we can have two more classifiers to tell Versicolour/not Versicolour and Virginica/not Virginica. Then we combine all three classifiers to get a final classifier to put the data into one of the three classes.\n\n\n5.5.2 Softmax function\nA better method is mimic the sigmoid function. Recall that in binary classification problem, after\n\\[\nz=L(x)=\\theta_0+\\sum_{i=1}^n\\theta_ix_i,\n\\] the sigmoid function is applied \\(p=\\sigma(z)\\). This \\(p\\) is interepreted as the probability for the data belonging to class \\(1\\). For \\(N\\)-class problem, we could generalize the sigmoid function to softmax function, whose value is a \\(N\\)-dimensional vector \\(p=[p_k]_{i=1}^N\\). Here \\(p_k\\) represents the probability for the data belonging to class \\(k\\). Then after we get the vector \\(p\\), we then find the highest probability and that indicates the class of the data point.\nThe softmax function is defined in the following way:\n\\[\np_k=\\sigma(z)=\\dfrac{\\exp(z_k)}{\\sum_{i=1}^N\\exp(z_i)},\\quad \\text{ for }z=[z_1, z_2,\\ldots,z_N].\n\\] In the model, each \\(z_i=L_i(x)=\\theta^{(i)}_0+\\sum_{i=1}^n\\theta^{(i)}_ix_i,\\) has its own weights.\nThe related cost function is also updated:\n\\[\nJ(\\Theta)=-\\sum_{i=1}^Ny_k\\ln(p_i).\n\\] Therefore the same gradient descent algorithm can be applied.\n\n\n\n\n\n\nNote\n\n\n\nNote that sigmoid function and the binary crossentropy cost functions are the special case of softmax function.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe labels are not recorded as labels, but as vectors. This is called dummy variables, or one-hot encodings.\n\n\n\n\n5.5.3 Codes\n\nBoth LogisticRegression() and SGDClassifier() by default uses the one-vs-all naive idea.\nUsing kears, softmax can be implemented. The key configuration is the loss function loss='categorical_crossentropy' and the activation function softmax. Note that in this case the labels should be translated into one-hot vectors.\n\nWe use make_classification as an example. To save time we won’t carefully tune the hyperparameters here.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=2, n_repeated=2, n_classes=3, n_clusters_per_class=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nAlthough there are totally 10 features, the dataset can be visualized using the informative features. By description, the informative features are the first two.\n\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.8466666666666667\n\n\n\nfrom sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.8266666666666667\n\n\nTo apply keras package, we should first change y into one-hot vectors. Here we use the function provided by keras.\n\n# import keras_core as keras\nfrom keras.utils import to_categorical\nfrom keras import models, layers\n\nvy_train = to_categorical(y_train, num_classes=3)\nvy_test = to_categorical(y_test, num_classes=3)\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(3, activation='softmax', input_dim=10))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(X_train, vy_train, epochs=50, batch_size=50, verbose=0)\n_ = model.evaluate(X_test, vy_test)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#exercises-and-projects",
    "href": "contents/5/intro.html#exercises-and-projects",
    "title": "6  Logistic regression",
    "section": "6.5 Exercises and Projects",
    "text": "6.5 Exercises and Projects\n\nExercise 6.1 Please hand write a report about the details of the math formulas for Logistic regression.\n\n\nExercise 6.2 CHOOSE ONE: Please use sklearn to apply the LogisticRegression to one of the following datasets. You may either use LogisticRegression or SGDClassifier.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nPlot the learning curve (accuracy vs training sizes).\n\n\n\nExercise 6.3 CHOOSE ONE: Please use keras to apply the LogisticRegression to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nWhat is the batch size do you use?\nPlot the learning curve (loss vs epochs, accuracy vs epochs).\nAnalyze the bias / variance status.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html",
    "href": "contents/6/intro.html",
    "title": "6  Netural networks",
    "section": "",
    "text": "6.1 Neural network: Back propagation\nTo train a MLP model, we still use gradient descent. Therefore it is very important to know how to compute the gradient. Actually the idea is the same as logistic regreesion. The only issue is that now the model is more complicated. The gradient computation is summrized as an algorithm called back propagation. It is described as follows.\nHere is an example of a Neural network with one hidden layer.\n\\(\\Theta\\) is the coefficients of the whole Neural network.\nThe dependency is as follows:\nEach layer is represented by the following diagram:\nThe diagram says:\n\\[\nz^{(k+1)}=b^{(k)}+\\Theta^{(k)}a^{(k)},\\quad z^{(k+1)}_j=b^{(k)}_j+\\sum \\Theta^{(k)}_{jl}a^{(k)}_l,\\quad a^{(k)}_j=\\sigma(z^{(k)}_j).\n\\]\nAssume \\(r,j\\geq1\\). Then\n\\[\n\\begin{aligned}\n\\diffp{z^{(k+1)}_i}{a^{(k)}_r}&=\\diffp*{\\left(b^{(k)}_i+\\sum\\Theta^{(k)}_{il}a^{(k)}_l\\right)}{a^{(k)}_r}=\\Theta_{ir}^{(k)},\\\\\n% \\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{ij}}&=\\diffp*{\\qty(a^{(k)}_0+\\sum\\Theta^{(k)}_{il}a^{(k)}_l)}{\\Theta^{(k)}_{ij}}=a^{(k)}_j,\\\\\n\\diffp{z^{(k+1)}_i}{z^{(k)}_j}&=\\sum_r \\diffp{z^{(k+1)}_i}{a^{k}_r}\\diffp{a^{(k)}_r}{z^{(k)}_j}+\\sum_{p,g}\\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{pq}}\\diffp{\\Theta^{(k)}_{pq}}{z^{(k)}_j}+\\sum_r \\diffp{z^{(k+1)}_i}{b^{k}_r}\\diffp{b^{(k)}_r}{z^{(k)}_j}\\\\\n&=\\sum_r \\Theta^{(k)}_{ir}\\diffp{a^{(k)}_r}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\diffp{a^{(k)}_j}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\sigma'(z^{(k)}_j),\\\\\n\\diffp{J}{z^{(k)}_j}&=\\sum_r \\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{z^{(k)}_j}=\\sum_r\\diffp{J}{z^{(k+1)}_r}\\Theta^{(k)}_{rj}\\sigma'(z^{(k)}_j).\n\\end{aligned}\n\\]\nWe set\nThen we have the following formula. Note that there are ``\\(z_0\\)’’ terms.\n\\[\n    \\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k).\n\\]\n\\[\n\\begin{aligned}\n\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}&=\\diffp*{\\left(b^{(k)}_r+\\sum_l\\Theta^{(k)}_{rl}a^{(k)}_l\\right)}{\\Theta^{(k)}_{pq}}=\\begin{cases}\n0&\\text{ for }r\\neq q,\\\\\na^{(k)}_q&\\text{ for }r=q,\n\\end{cases}\\\\\n\\diffp{J}{\\Theta^{(k)}_{pq}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}=\\diffp{J}{z^{(k+1)}_p}\\diffp{z^{(k+1)}_p}{\\Theta^{(k)}_{pq}}=\\delta^{k+1}_pa^{k}_q,\\\\\n\\diffp{J}{b^{(k)}_{j}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}\\diffp{z^{(k+1)}_j}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}=\\delta^{k+1}_j.\n\\end{aligned}\n\\]\nExtend \\(\\hat{\\Theta}=\\left[b^{(k)},\\Theta^{(k)}\\right]\\), and \\(\\partial^k J=\\left[\\diffp{J}{\\hat{\\Theta}^{(k)}_{ij}}\\right]\\). Then \\[\n    \\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right].\n\\] Then the algorithm is as follows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#neural-network-back-propagation",
    "href": "contents/6/intro.html#neural-network-back-propagation",
    "title": "6  Netural networks",
    "section": "",
    "text": "\\[\n\\newcommand\\diffp[2]{\\dfrac{\\partial #1}{\\partial #2}}\n\\]\n\n\n\n\n\n\n\\(a^{(1)}=\\hat{\\textbf{x}}\\) is the input. \\(a_0^{(1)}\\) is added. This is an \\((n+1)\\)-dimension column vector.\n\\(\\Theta^{(1)}\\) is the coefficient matrix from the input layer to the hidden layer, of size \\(k\\times(n+1)\\).\n\\(z^{(2)}=\\Theta^{(1)}a^{(1)}\\).\n\\(a^{(2)}=\\sigma(z^{(2)})\\), and then add \\(a^{(2)}_0\\). This is an \\((k+1)\\)-dimension column vector.\n\\(\\Theta^{(2)}\\) is the coefficient matrix from the hidden layer to the output layer, of size \\(r\\times(k+1)\\).\n\\(z^{(3)}=\\Theta^{(2)}a^{(2)}\\).\n\\(a^{(3)}=\\sigma(z^{(3)})\\). Since this is the output layer, \\(a^{(3)}_0\\) won’t be added. %\nThese \\(a^{(3)}\\) are \\(h_{\\Theta}(\\textbf{x})\\).\n\n\n\n\\(J\\) depends on \\(z^{(3)}\\) and \\(a^{(3)}\\).\n\\(z^{(3)}\\) and \\(a^{(3)}\\) depends on \\(\\Theta^{(2)}\\) and \\(a^{(2)}\\).\n\\(z^{(2)}\\) and \\(a^{(2)}\\) depends on \\(\\Theta^{(1)}\\) and \\(a^{(1)}\\).\n\\(J\\) depends on \\(\\Theta^{(1)}\\), \\(\\Theta^{(2)}\\) and \\(a^{(1)}\\).\n\n\n\n\n\n\n\n\n\n\\(\\delta^k_j=\\diffp{J}{z^{(k)}_j}\\), \\(\\delta^k=\\left[\\delta^k_1,\\delta_2^k,\\ldots\\right]^T\\).\n\\(\\mathbf{z}^k=\\left[z^{(k)}_1,z^{(k)}_2,\\ldots\\right]^T\\), \\(\\mathbf{a}^k=\\left[a^{(k)}_1,a^{(k)}_2,\\ldots\\right]^T\\), \\(\\hat{\\mathbf{a}}^k=\\left[a^{(k)}_0,a^{(k)}_1,\\ldots\\right]^T\\).\n\\(\\Theta^{k}=\\left[\\Theta^{(k)}_{ij}\\right]\\).\n\n\n\n\n\n\nStarting from \\(x\\), \\(y\\) and some random \\(\\Theta\\).\nForward computation: compute \\(z^{(k)}\\) and \\(a^{(k)}\\). The last \\(a^{(n)}\\) is \\(h\\).\nCompute \\(\\delta^n=\\nabla J\\circ\\sigma'(z^{(n)})\\). In the case of \\(J=\\frac12||{h-y}||^2\\), \\(\\nabla J=(a^{(n)}-y)\\), and then \\(\\delta^n=(a^{(n)}-y)\\circ\\sigma'(z^{(n)})\\).\nBackwards: \\(\\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k)\\), and \\(\\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right]\\) .\n\n\nExample 6.1 Consider there are 3 layers: input, hidden and output. There are \\(m+1\\) nodes in the input layer, \\(n+1\\) nodes in the hidden layer and \\(k\\) in the output layer. Therefore\n\n\\(a^{(1)}\\) and \\(\\delta^1\\) are \\(m\\)-dim column vectors.\n\\(z^{(2)}\\), \\(a^{(2)}\\) and \\(\\delta^2\\) are \\(n\\)-dim column vectors.\n\\(z^{(3)}\\), \\(a^{(3)}\\) and \\(\\delta^3\\) are \\(k\\)-dim column vectors.\n\\(\\hat{\\Theta}^1\\) is \\(n\\times(m+1)\\), \\(\\hat{\\Theta}^2\\) is \\(k\\times(n+1)\\).\n\\(z^{(2)}=b^{(1)}+\\Theta^{(1)}a^{(1)}=\\hat{\\Theta}^{(1)}\\hat{a}^{(1)}\\), \\(z^{(3)}=b^{(2)}+\\Theta^{(2)}a^{(2)}=\\hat{\\Theta}^{(2)}\\hat{a}^{(2)}\\).\n\\(\\delta^3=\\nabla_aJ\\circ\\sigma'(z^{(3)})\\). This is a \\(k\\)-dim column vector.\n\\(\\partial^2 J=\\left[\\delta^3,\\delta^3(a^{(2)})^T\\right]\\).\n\\(\\delta^2=\\left[(\\Theta^2)^T\\delta^3\\right]\\circ \\sigma'(z^{(2)})\\), where \\((\\hat{\\Theta^2})^T\\delta^3=(\\hat{\\Theta^2})^T\\delta^3\\) and then remove the first row.\n\\(\\delta^1=\\begin{bmatrix}(\\Theta^1)^T\\delta^2\\end{bmatrix}\\circ \\sigma'(z^{(1)})\\), where \\((\\hat{\\Theta^1})^T\\delta^2=(\\hat{\\Theta^1})^T\\delta^2\\) and then remove the first row.\n\\(\\partial^1 J=\\left[\\delta^2,\\delta^2(a^{(1)})^T\\right]\\).\nWhen \\(J=-\\frac1m\\sum y\\ln a+(1-y)\\ln(1-a)\\), \\(\\delta^3=\\frac1m(\\sum a^{(3)}-\\sum y)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#example",
    "href": "contents/6/intro.html#example",
    "title": "6  Netural networks",
    "section": "6.2 Example",
    "text": "6.2 Example\nLet us take some of our old dataset as an example. This is an continuation of the horse colic dataset from Logistic regression.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\nmms.fit(X_train)\nX_train = mms.transform(X_train)\nX_test = mms.transform(X_test)\n\nNow we build a neural network. This is a 2-layer model, with 1 hidden layer with 10 nodes.\n\n# import keras_core as keras\nfrom keras import models, layers, Input\nmodel = models.Sequential()\n\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(layers.Dense(10, activation='sigmoid'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\nAnd the learning curve are shown in the following plots.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nIt seems that our model has overfitting issues. Therefore we need to modifify the architects of our model. The first idea is to add L2 regularization as we talked about it in LogsiticRegression case. Here we use 0.01 as the regularization strenth.\nLet us add the layer to the model and retrain it.\n\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1], kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAnother way to deal with overfitting is to add a Dropout layer. The idea is that when training the model, part of the data will be randomly discarded. Then after fitting, the model tends to reduce the variance, and then reduce the overfitting.\nThe code of a Dropout layer is listed below. Note that the number represents the percentage of the training data that will be dropped.\n\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAfter playing with different hyperparameters, the overfitting issues seem to be better (but not entirely fixed). However, the overall performance is getting worse. This means that the model is moving towards underfitting side. Then we may add more layers to make the model more complicated in order to capture more information.\n\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAs you may see, to build a netural network model it requires many testing. There are many established models. When you build your own architecture, you may start from there and modify it to fit your data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#exercises-and-projects",
    "href": "contents/6/intro.html#exercises-and-projects",
    "title": "6  Netural networks",
    "section": "6.3 Exercises and Projects",
    "text": "6.3 Exercises and Projects\n\nExercise 6.1 Please hand write a report about the details of back propagation.\n\n\nExercise 6.2 CHOOSE ONE: Please use netural network to one of the following datasets. - the iris dataset. - the dating dataset. - the titanic dataset.\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nWhat is the batch size do you use?\nPlot the learning curve (loss vs epochs, accuracy vs epochs).\nAnalyze the bias / variance status.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html",
    "href": "contents/intro2pt/intro.html",
    "title": "5  Intro to Pytorch",
    "section": "",
    "text": "5.1 Linear regression (math)\nWe only consider the simplest case: simple linear regression (SLR). The idea is very simple. The dataset contains two variables (the independent variable \\(x\\) and the response variable \\(y\\).) The goal is to find the relation between \\(x\\) and \\(y\\) with the given dataset. We assume their relation is \\(y=b+wx\\). How do we find \\(b\\) and \\(w\\)?\nLet us first see an example. We would like to find the red line (which is the best fitted curve) shown below.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-pytorch-version",
    "href": "contents/intro2pt/intro.html#linear-regression-pytorch-version",
    "title": "5  Intro to Pytorch",
    "section": "5.3 Linear regression (Pytorch version)",
    "text": "5.3 Linear regression (Pytorch version)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#exercises",
    "href": "contents/intro2pt/intro.html#exercises",
    "title": "5  Intro to Pytorch",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\nTry to reconstruct all the plots in Section Section 5.1.\n\n\n\n\n\n[1] Godoy, D. V. (2022). Deep learning with PyTorch step-by-step: A beginner’s guide. https://leanpub.com/pytorch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-numpy-version",
    "href": "contents/intro2pt/intro.html#linear-regression-numpy-version",
    "title": "5  Intro to Pytorch",
    "section": "5.2 Linear regression (numpy version)",
    "text": "5.2 Linear regression (numpy version)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#rewrite-in-classes",
    "href": "contents/intro2pt/intro.html#rewrite-in-classes",
    "title": "5  Intro to Pytorch",
    "section": "5.4 Rewrite in classes",
    "text": "5.4 Rewrite in classes\n\n5.4.1 Use class to describe the model\nWe now want to upgrade the code we wrote in previous sections in terms of classes, since it is a good way to wrap up our own code.\n\nimport torch\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.b = nn.Parameter(torch.tensor(1, requires_grad=True, dtype=torch.float))\n        self.w = nn.Parameter(torch.tensor(1.5, requires_grad=True, dtype=torch.float))\n\n    def forward(self, x):\n        return self.b + self.w * x\n\nRANDOMSEED = 42\ntorch.manual_seed(RANDOMSEED)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = LR().to(device)\nmodel.state_dict()\n\nOrderedDict([('b', tensor(1.)), ('w', tensor(1.5000))])\n\n\nWe could use model.state_dict() to look at the parameters of the model. Another way to see the parameters is to use model.parameters() method. The latter will return an iterator that help you go through all parameters.\n\nfor item in model.parameters():\n    print(item)\n\nParameter containing:\ntensor(1., requires_grad=True)\nParameter containing:\ntensor(1.5000, requires_grad=True)\n\n\nNow we reproduce the training code for LR class.\n\nfrom torch.optim import SGD\n\ndef loss_fn(yhat, y):\n    return ((yhat-y)**2).mean()\n\nlr = 0.2\noptimizer = SGD(model.parameters(), lr=lr)\n\nepoch_num = 10\n\nplist = []\nfor epoch in range(epoch_num):\n    model.train()\n\n    yhat = model(X_tensor_train)\n    loss = loss_fn(yhat, y_tensor_train)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['b'].item(), p['w'].item()])\n\nplist\n\n[[1.46212899684906, 1.7026045322418213],\n [1.7017914056777954, 1.7949450016021729],\n [1.8284450769424438, 1.8316394090652466],\n [1.8976247310638428, 1.8403884172439575],\n [1.937508225440979, 1.8352371454238892],\n [1.9623947143554688, 1.8233033418655396],\n [1.9795421361923218, 1.8081902265548706],\n [1.9926364421844482, 1.791718602180481],\n [2.0035512447357178, 1.7748050689697266],\n [2.013240098953247, 1.7579076290130615]]\n\n\n\n\n5.4.2 Model standard models\nWe hand write our models and set parameters in our previous versions. PyTorch provides many standard modules that we can use directly. For example, the linear regression model can be found in nn.modules as Linear, while our loss function is the mean square differene function which is MSELoss from nn.\n\nfrom torch.optim import SGD\nfrom torch.nn.modules import Linear\nfrom torch.nn import MSELoss\n\nclass BetterLR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.linear = Linear(in_features=1, out_features=1)\n        self.linear.bias = torch.nn.Parameter(torch.tensor([1.0], dtype=torch.float))\n        self.linear.weight = torch.nn.Parameter(torch.tensor([[1.5]], dtype=torch.float))\n\n    def forward(self, x):\n        return self.linear(x)\n\nlr = 0.2\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel2 = BetterLR().to(device)\noptimizer2 = SGD(model2.parameters(), lr=lr)\n\nepoch_num = 10\nplist = []\n\nfor epoch in range(epoch_num):\n    model2.train()\n\n    yhat = model2(X_tensor_train.reshape(-1, 1))\n    loss2 = MSELoss(reduction='mean')(yhat, y_tensor_train.reshape(-1, 1))\n    loss2.backward()\n    optimizer2.step()\n    optimizer2.zero_grad()\n    p = model2.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\n\nplist\n\n[[1.46212899684906, 1.7026045322418213],\n [1.7017914056777954, 1.7949450016021729],\n [1.8284450769424438, 1.8316394090652466],\n [1.8976247310638428, 1.8403884172439575],\n [1.937508225440979, 1.8352371454238892],\n [1.9623947143554688, 1.8233033418655396],\n [1.9795421361923218, 1.8081902265548706],\n [1.9926364421844482, 1.791718602180481],\n [2.0035512447357178, 1.7748050689697266],\n [2.013240098953247, 1.7579076290130615]]\n\n\n\n\n\n\n\n\nInitialize the parameters\n\n\n\nIn all our examples we initialize the parameters to be \\((1, 1.5)\\) for the purpose of comparision. In most cases, we don’t manually set the intial values, but use random numbers. In this case, we simply delete the manual codes.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that if we directly change our function to the standard one we will encounter some issues. The main reason is that our previous code is an oversimplifed version that we treat b and w as two scalars. They are scalars in our particular problem, but it is better to treat them as a special case of tensors for the purpose of better generalization. Actually based on the standard functions from PyTorch (as well as many others like sklearn) X and y are expected to be 2D tensors. This is the reason why there are some strange reshape(-1, 1 ) in the codes.\nWe will reconstruct it in the next section.\n\n\n\n\n5.4.3 Dataloader\nUsually we use a class to provide data. The class is based on Dataset class, and need to implement the constructor, __getitem__ method and __len__ method. Here is an example. Note that we directly change X and y to be 2D tensors here.\n\nfrom torch.utils.data import Dataset, TensorDataset\n\nclass MyData(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x, dtype=torch.float).reshape(-1, 1)\n        self.y = torch.tensor(y, dtype=torch.float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\ntrain_data = MyData(X_train, y_train)\ntrain_data[1]\n\n(tensor([0.7713]), tensor([3.1575]))\n\n\nThen we use Dataloader to feed the data into our model.\n\nfrom torch.utils.data import DataLoader \n\nlr = 0.2\nepoch_num = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = BetterLR().to(device)\noptimizer = SGD(model.parameters(), lr=lr)\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n\nplist = []\nfor epoch in range(epoch_num):\n    for X_batch, y_batch in train_loader:\n        yhat = model(X_batch)\n        loss = MSELoss(reduction='mean')(yhat, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    p= model.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\nplist\n\n[[1.8239108324050903, 1.8337838649749756],\n [1.9430465698242188, 1.815915584564209],\n [2.008997917175293, 1.7804700136184692],\n [2.028524875640869, 1.7270125150680542],\n [2.0563058853149414, 1.6825157403945923],\n [2.0699679851531982, 1.6356185674667358],\n [2.105228900909424, 1.604027509689331],\n [2.0967822074890137, 1.5525143146514893],\n [2.124683141708374, 1.524984359741211],\n [2.1368484497070312, 1.4938127994537354]]\n\n\nWhen applying mini-batch, usually we will shuffle the dataset. If we disable the shuffle here as well as the shuffle in numpy case, you will see that we get exactly the same answer.\n\n\n\n\n\n\nNote\n\n\n\n\n\nHere is the result for non-shuffle version.\n\nfrom torch.utils.data import DataLoader \n\nlr = 0.2\nepoch_num = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = BetterLR().to(device)\noptimizer = SGD(model.parameters(), lr=lr)\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=False)\n\nplist = []\nfor epoch in range(epoch_num):\n    for X_batch, y_batch in train_loader:\n        yhat = model(X_batch)\n        loss = MSELoss(reduction='mean')(yhat, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    p= model.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\nplist\n\n[[1.8277027606964111, 1.8368194103240967],\n [1.9607104063034058, 1.8293981552124023],\n [2.0016260147094727, 1.7815078496932983],\n [2.028693675994873, 1.732171654701233],\n [2.0522055625915527, 1.6861382722854614],\n [2.0736379623413086, 1.6437404155731201],\n [2.093313217163086, 1.6047618389129639],\n [2.111393451690674, 1.5689359903335571],\n [2.1280105113983154, 1.5360088348388672],\n [2.143282651901245, 1.5057460069656372]]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that we use some not-very-elegent way to display the result. Don’t worry about it. We will work on a better solution in the next section.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-numpy",
    "href": "contents/intro2pt/intro.html#linear-regression-numpy",
    "title": "5  Intro to Pytorch",
    "section": "5.2 Linear regression (numpy)",
    "text": "5.2 Linear regression (numpy)\n\n\n\\[\n\\require{physics}\n\\require{braket}\n\\]\n\\[\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n\\]\n\n\\[\n\\newcommand{\\Exp}{\\operatorname{E}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Mode}{\\operatorname{mode}}\n\\]\n\n\\[\n\\newcommand{\\pdfbinom}{{\\tt binom}}\n\\newcommand{\\pdfbeta}{{\\tt beta}}\n\\newcommand{\\pdfpois}{{\\tt poisson}}\n\\newcommand{\\pdfgamma}{{\\tt gamma}}\n\\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n\\]\n\n\\[\n\\newcommand{\\distbinom}{\\operatorname{B}}\n\\newcommand{\\distbeta}{\\operatorname{Beta}}\n\\newcommand{\\distgamma}{\\operatorname{Gamma}}\n\\newcommand{\\distexp}{\\operatorname{Exp}}\n\\newcommand{\\distpois}{\\operatorname{Poisson}}\n\\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n\\]\n\nWe will translate everything from the previous sections into codes.\n\n5.2.1 Prepare the dataset\nWe first randomly generate a dataset (X, y) for the linear regression problem.\n\nimport numpy as np\n\nRANDOMSEED = 42\nnp.random.seed(RANDOMSEED)\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\n\nWe set the seed to be 42 for reproducing the results. We will also split the dataset into training and test sets.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=RANDOMSEED)\n\nWe will only focus only on the training set in this Chapter.\n\n\n5.2.2 Compute gradient\nRecall Equation 5.1 and Equation 5.2\n\\[\n\\begin{aligned}\nJ(b,w)&=\\frac1N\\sum_{i=1}^N(y_i-b-wx_i)^2,\\\\\n\\pdv{J}{b}&=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\\\\n\\pdv{J}{w}&=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-x_i).\n\\end{aligned}\n\\]\n\ndef J(parameters, X, y):    \n    b = parameters[0]\n    w = parameters[1]\n    return ((y-b-w*X)**2).mean().item()\n\ndef dJ(parameters, X, y):\n    b = parameters[0]\n    w = parameters[1]\n    db = (2*(y-b-w*X)*(-1)).mean()\n    dw = (2*(y-b-w*X)*(-X)).mean()\n    return np.array([db, dw])\n\n\n\n5.2.3 Gradient descent\nIn general we need to random select a starting point. Here for the purpose of comparing to what we get from previous section, we will use a manual selected starting point \\((1, 1.5)\\). We then follow the path and move for a few steps. Here we will use \\(\\eta=0.2\\) as the learning rate.\n\np = np.array([1.0, 1.5])\nlr = 0.2\n\nplist = []\nfor _ in range(10):\n    J_i = J(p, X_train, y_train)\n    dJ_i = dJ(p, X_train, y_train)\n    p = p - lr * dJ_i\n    plist.append([p[0], p[1]])\n\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456744, 1.840388291132826],\n [1.937508248869905, 1.8352370440485253],\n [1.96239470703006, 1.8233031967656035],\n [1.9795421883017092, 1.8081901205764057],\n [1.9926365306310478, 1.7917185352872653],\n [2.0035512067846226, 1.7748049887103947],\n [2.013240136591026, 1.7579075228763736]]\n\n\nYou may compare the answer with the PyTorch implementation in Section 5.3.3.\n\n\n5.2.4 Mini-batch and optimizers\nReview the gradient formula Equation 5.2, the gradient is computed by looking at each given data point and putting the results together. Therefore it is possible to get the partial information of the gradient by just looking at part of the data. In other words, the updating process can be modify in the following way: divide the original dataset into several groups, run through each group to compute the gradient with the data in only one group and then update the parameters. In general there are three types:\n\nThere is only 1 group: we update the parameters only once when we finish looking at all data points. This is the way we mentioned previously. It is called batch gradient descent.\nEvery single point forms a group: we update the parameters eachtime we look at one data point. This method is called stocastic gradient descent (SGD). Since we compute the gradient with only one data point, it is expected that the direction is far from perfect, and the descent process is expected to be more “random”.\nMultiple groups of the same size are formed, with a reasonable group size and group number. This is called mini-batch gradient descent. It is the middle point between the above two methods. The batch size, which is the size of each group, is a very important hyperparameter for trainning.\n\n\n\n\n\n\n\nEpochs\n\n\n\nOne epoch is the process that you see each data point exactly once, no matter what the batch size is.\n\n\nUsually batch gradient descent is expected to have a more smooth trajection but move slowly, while SGD is expected to move faster to the minimal point but may never really get to it since the trajection is too jumpy. Mini-batch is meant to strike a balanced point by finding a good batch size. In the example below, we show the mini-batch gradient descent in the first 10 epochs.\n\np = np.array([1.0, 1.5])\nlr = 0.2\nbatchsize = 32\n\nN = X_train.shape[0]\nindx = np.arange(N)\n\nnp.random.seed(RANDOMSEED)\nnp.random.shuffle(indx)\nbatches = []\n\nbatch_num = int(np.ceil(N / batchsize))\nfor i in range(batch_num):\n    last = np.minimum((i+1)*batchsize, N)\n    batches.append(indx[i*batchsize: last])\n\nplist = []\nfor epoch in range(10):\n    for i in range(batch_num):\n        dJ_i = dJ(p, X_train[batches[i]], y_train[batches[i]])\n        p = p - lr * dJ_i\n    plist.append([p[0], p[1]])\nplist\n\n[[1.8396487079358765, 1.8350571390420554],\n [1.9759220547433842, 1.826737782139383],\n [2.0168483670845423, 1.7772907765564647],\n [2.043224019190362, 1.726425817587749],\n [2.065921614604292, 1.6790714304374827],\n [2.086528794877608, 1.635572387082582],\n [2.1053898608038017, 1.595691439736766],\n [2.1226730826892424, 1.559137786653576],\n [2.1385131544625695, 1.5256351733493791],\n [2.1530309370105214, 1.494929115540467]]\n\n\n\n\n\n\n\n\nNon-shuffle version\n\n\n\n\n\nHere is the result for the non-shuffle version. You could compare the results with what we do later.\n\np = np.array([1.0, 1.5])\nlr = 0.2\nbatchsize = 32\n\nN = X_train.shape[0]\nindx = np.arange(N)\n\nbatches = []\nbatch_num = int(np.ceil(N / batchsize))\nfor i in range(batch_num):\n    last = np.minimum((i+1)*batchsize, N)\n    batches.append(indx[i*batchsize: last])\n\nplist = []\nfor epoch in range(10):\n    for i in range(batch_num):\n        dJ_i = dJ(p, X_train[batches[i]], y_train[batches[i]])\n        p = p - lr * dJ_i\n    plist.append([p[0], p[1]])\nplist\n\n[[1.8277028504755573, 1.8368193572906044],\n [1.9607104449826838, 1.8293981130981023],\n [2.001626059409397, 1.7815077539441087],\n [2.0286935704191, 1.7321715193480243],\n [2.0522055690695757, 1.686138097785071],\n [2.0736381185747943, 1.6437403254745735],\n [2.0933134526016604, 1.6047617600958677],\n [2.111393711486754, 1.5689357968513453],\n [2.1280105514943686, 1.5360086358936902],\n [2.143282725696795, 1.505745878510758]]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#linear-regression-pytorch",
    "href": "contents/intro2pt/intro.html#linear-regression-pytorch",
    "title": "5  Intro to Pytorch",
    "section": "5.3 Linear regression (PyTorch)",
    "text": "5.3 Linear regression (PyTorch)\n\n5.3.1 Construct torch.Tensor\nThere are multiple ways to construct a tensor. I just discuss those confusing ones.\n\ntorch.Tensor is the PyTorch tensor data structure. Itself serves as the constructor of the class, therefore you may use torch.Tensor(data) to construct a tensor. This is relative basic, and will have a default float type.\ntorch.tensor is the recommendated function to construct a tensor from data. It has two benefits over torch.Tensor: it will automatically induce the datatype from data instead of always using float; and it is easier to change datatype with the argument dtype.\ntorch.as_tensor is a function to construct a tensor from data. If the original data is numpy array, this tensor shares data with it. This means that if one is changed, the other is changed as well.\n\n\nimport numpy as np\nimport torch\n\nexample = np.array([1, 2])\nexample_tensor0 = torch.Tensor(example)\nexample_tensor1 = torch.tensor(example)\nexample_tensor2 = torch.as_tensor(example)\n\nprint(f'Tensor: dtype: {example_tensor0.dtype}, tensor: dtype: {example_tensor1.dtype}')\n\nprint(f'tensor: {example_tensor1}, as_tensor: {example_tensor2}, original: {example}')\n\nexample[0] = 0\nprint(f'tensor: {example_tensor1}, as_tensor: {example_tensor2}, original: {example}')\n\nTensor: dtype: torch.float32, tensor: dtype: torch.int32\ntensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([1, 2], dtype=torch.int32), original: [1 2]\ntensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([0, 2], dtype=torch.int32), original: [0 2]\n\n\nIn general, it is recommended to use torch.as_tensor over torch.tensor (since for large data to create a view is much faster than to create a copy) and to use torch.tensor over torch.Tensor (due to the benefits mentioned above).\n\n\n\n\n\n\nScalar\n\n\n\nA tensor with only one element is still a tensor in PyTorch. To use it as a scalar, you need to use itme() method.\n\na = torch.tensor(1)\na\n\ntensor(1)\n\n\n\na.item()\n\n1\n\n\nNote that for numpy, before 2.0 version an array with one element is considered as scalar. However after 2.0, it behaves very similar to PyTorch.\n\n\n\n\n\n\n\n\ndatatype\n\n\n\nThe datatype in PyTorch is very strict. Many functions can work with only some of the datatypes. In most cases float and double are used. Other types may or may not be supported by a specific function.\nHowever, there are a lot of ways to play with types. For example, you may use torch.tensor([1], dtype=torch.double) to directly construct a double tensor, or use torch.tensor([1]).double() to first construct an int tensor and then cast it into a double tensor.\n\n\n\n\n\n\n\n\nanother datatype note\n\n\n\nnumpy also has dtype setting but since it is not strict on it, we ignored it previous. Here is the case: the default setting for numpy is double type, or float64, while in PyTorch float, or float32, is commonly used. Since the precision is different, when cast from double to float, the number might be changed a little bit.\n\na = np.random.random(1)\nb = torch.as_tensor(a)\nc = torch.as_tensor(a, dtype=torch.float)\nd = torch.as_tensor(a, dtype=float)\ne = torch.as_tensor(a, dtype=torch.float64)\nf = b.float()\ng = f.double()\nprint(f'a: {a[0]}, type of a: {a.dtype}\\n'\n      f'b: {b.item()}, type of b: {b.dtype}\\n'\n      f'c: {c.item()}, type of c: {c.dtype}\\n'\n      f'd: {d.item()}, type of d: {d.dtype}\\n'\n      f'e: {e.item()}, type of e: {e.dtype}\\n'\n      f'f: {f.item()}, type of e: {f.dtype}\\n'\n      f'g: {g.item()}, type of e: {g.dtype}\\n')\n\na: 0.28093450968738076, type of a: float64\nb: 0.28093450968738076, type of b: torch.float64\nc: 0.28093451261520386, type of c: torch.float32\nd: 0.28093450968738076, type of d: torch.float64\ne: 0.28093450968738076, type of e: torch.float64\nf: 0.28093451261520386, type of e: torch.float32\ng: 0.28093451261520386, type of e: torch.float64\n\n\n\nYou may notice the difference from the example, and also take notes about the convetion of which setting is corresponding to which type. Note that dtype=float actually create double type.\nIn this notes we will use double type by setting dtype=float or torch.float64 to reduce the possibility of that small differences.\n\n\nWe now construct a PyTorch tensor version of the dataset we used in previous sections. The device part will be introduced later.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\n\nRANDOMSEED = 42\nnp.random.seed(RANDOMSEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=RANDOMSEED)\nX_tensor_train = torch.as_tensor(X_train, device=device, dtype=float)\ny_tensor_train = torch.as_tensor(y_train, device=device, dtype=float)\n\n\n\n\n\n\n\nBack to numpy\n\n\n\nIf we would like to turn a tensor back to numpy, usually we would like to remove it from related computation graphs and just keep the data. Therefore usually we would like to apply detach() method to the tensor before converting. Also later we will talk about devices. When taking GPU into consideration, we also want to send the tensor back to CPU before converting. Therefore the code to turn a PyTorch tensor to a numpy array is x.detach().cpu().numpy().\n\n\n\n\n5.3.2 devices\nWe coulde use torch.cuda.is_available() to check whether we have GPU/CUDA supported devices. If the answer is no, we don’t need to change any codes and everything works fine but slow.\nIf we have GPU/CUDA supported devices, we could send our tensors to them and do computations there. Google Colab is a good place to play with it if we don’t have our own hardware.\nIn most cases we use to(device) method to send a tensor to a device. Sometimes some function has device=device argument to automatically construct tensors in a device. Note that if one needs to compute the gradient of a tensor and send the tensor to a device, we need to manually set requires_grad_(True) or create the tensor with device argument.\n\n\n\n\n\n\nto(device)\n\n\n\nWhen sending tensors to other devices by to, gradient info might be lost. Therefore if we need to send trainable tensors to GPU some special methods should be used (e.g. setting device when creating the tensor). However for the dataset we don’t need to worry about it.\n\n\nHere are some examples, although they only makes sense in a GPU environment.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nt1 = torch.tensor(1, dtype=float, device=device)\nt2 = torch.tensor(1, dtype=float)\nprint(f't1: {t1.type()}, t2: {t2.type()}')\n\nIf you can see cuda in the output of type, it is a GPU tensor. Otherwise it is a CPU tensor. We may use to to convert a CPU tensor to be a GPU tensor. If this tensor requires gradient, we should set it manually.\n\nt3 = t2.to(device)\nt3 = t3.requires_grad_(True)\n\nIt is usually recommended to write codes with device in mind like above, since the codes work for both CPU and GPU machines.\n\n\n5.3.3 Gradient\nPyTorch can use autograd to automatically compute the gradient of given formula. All computations are done within the context of tensors. The biggest difference between PyTorch tensor and numpy array is that PyTorch tensor carries gradient infomation on its own.\nThe step is very easy: first use PyTorch tensor to write a formula, enable gradients on correct tensors, and then use the backward() method.\n\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\n\nloss = ((y_tensor_train - b - w * X_tensor_train)**2).mean()\nloss.backward()\nprint(f'db: {b.grad}, dw: {w.grad}')\n\ndb: -2.310644882519191, dw: -1.0130224137389\n\n\nWe could manually compute the first few iterations and record the results. You may compare it with the numpy implementation in Section 5.2.3. The answer is exactly the same.\n\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\nlr = 0.2\nplist = []\nfor _ in range(10):\n    loss = ((y_tensor_train - b - w * X_tensor_train)**2).mean()\n    loss.backward()\n    with torch.no_grad():\n        b -= lr * b.grad\n        w -= lr * w.grad\n    b.grad.zero_()\n    w.grad.zero_()\n    plist.append([b.item(), w.item()])\n    \nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code has some tricky parts. The main issue is to let PyTorch know which gradient infomation should be kept, which should not. In this code, to make it run correctly, we need to pay attention to the following three things:\n\nBefore updating b and w, with torch.no_grad() should be used, to tell PyTorch don’t compute gradient here.\nWhen updating b and w, we should use the in-place syntax b -= db instead of b = b - db. Again, the reason is related to updating gradient: the out-of-place syntax b = b - db will lose the grad info.\nAfter updating b and w, we need to zero out the grad info by applying b.grad.zero_() and w.grad.zero_().\n\n\n\nWe will skip mini-batch gradient descent here, and leave it to the next section with a more systematic treatment.\n\n\n5.3.4 Optimizers\nAfter we get the gradient, there are still many tricks to move one step further. We already talked about the learning rate before. It is not the only case. Another example is that sometimes we don’t really want to move in the direction given by the gradient, but we want to modify it a little bit. All these tricks are combined together and are called optimizers.\nAn optimizer is a set of rules to update parameters after the gradient is computed. We already talked about SGD (stochastic gradient descent). Other common ones include RMSprop and Adam. In general, Adam is the generic best optimizer. If you don’t know which optimizer to use, Adam is always the go-to choice.\nHere we rewrite our previous code by optimizers. We use SGD in this example. Again, we may compare the results to Section 5.2.3 and Section 5.3.3.\n\nfrom torch.optim import SGD\nimport torch\n\nlr = 0.2\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\n\noptimizer = SGD([b, w], lr=lr)\nplist = []\n\nfor epoch in range(10):\n    loss = ((y_tensor_train - b - w*X_tensor_train)**2).mean()\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    plist.append([b.item(), w.item()])\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n5.3.5 Use class to describe the model\nWe now want to upgrade the code we wrote in previous sections in terms of classes, since it is a good way to wrap up our own code.\n\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.b = nn.Parameter(torch.tensor(1, requires_grad=True, dtype=float))\n        self.w = nn.Parameter(torch.tensor(1.5, requires_grad=True, dtype=float))\n\n    def forward(self, x):\n        return self.b + self.w * x\n\nRANDOMSEED = 42\ntorch.manual_seed(RANDOMSEED)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = LR().to(device)\nmodel.state_dict()\n\nOrderedDict([('b', tensor(1., dtype=torch.float64)),\n             ('w', tensor(1.5000, dtype=torch.float64))])\n\n\nWe could use model.state_dict() to look at the parameters of the model. Another way to see the parameters is to use model.parameters() method. The latter will return an iterator that help you go through all parameters.\n\nfor item in model.parameters():\n    print(item)\n\nParameter containing:\ntensor(1., dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor(1.5000, dtype=torch.float64, requires_grad=True)\n\n\nNow we reproduce the training code for LR class.\n\nfrom torch.optim import SGD\n\ndef loss_fn(yhat, y):\n    return ((yhat-y)**2).mean()\n\nlr = 0.2\noptimizer = SGD(model.parameters(), lr=lr)\n\nepoch_num = 10\n\nplist = []\nfor epoch in range(epoch_num):\n    model.train()\n\n    yhat = model(X_tensor_train)\n    loss = loss_fn(yhat, y_tensor_train)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['b'].item(), p['w'].item()])\n\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n5.3.6 Using standard modules\nWe hand write our models and set parameters in our previous versions. PyTorch provides many standard modules that we can use directly. For example, the linear regression model can be found in nn.modules as Linear, while our loss function is the mean square differene function which is MSELoss from nn.\n\nfrom torch.nn.modules import Linear\nfrom torch.nn import MSELoss\n\nclass BetterLR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.linear = Linear(in_features=1, out_features=1)\n        self.linear.bias = torch.nn.Parameter(torch.tensor([1.0], dtype=float))\n        self.linear.weight = torch.nn.Parameter(torch.tensor([[1.5]], dtype=float))\n\n    def forward(self, x):\n        return self.linear(x)\n\nlr = 0.2\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel2 = BetterLR().to(device)\noptimizer2 = SGD(model2.parameters(), lr=lr)\n\nepoch_num = 10\nplist = []\n\nfor epoch in range(epoch_num):\n    model2.train()\n\n    yhat = model2(X_tensor_train.reshape(-1, 1))\n    loss2 = MSELoss(reduction='mean')(yhat, y_tensor_train.reshape(-1, 1))\n    loss2.backward()\n    optimizer2.step()\n    optimizer2.zero_grad()\n    p = model2.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\n\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n\n\n\n\nInitialize the parameters\n\n\n\nIn all our examples we initialize the parameters to be \\((1, 1.5)\\) for the purpose of comparision. In most cases, we don’t manually set the intial values, but use random numbers. In this case, we simply delete the manual codes.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that if we directly change our function to the standard one we will encounter some issues. The main reason is that our previous code is an oversimplifed version that we treat b and w as two scalars. They are scalars in our particular problem, but it is better to treat them as a special case of tensors for the purpose of better generalization. Actually based on the standard functions from PyTorch (as well as many others like sklearn) X and y are expected to be 2D tensors. This is the reason why there are some strange reshape(-1, 1 ) in the codes.\nWe will reconstruct it in the later sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#sec-linearregression_math",
    "href": "contents/intro2pt/intro.html#sec-linearregression_math",
    "title": "5  Intro to Pytorch",
    "section": "",
    "text": "\\[\n\\require{physics}\n\\require{braket}\n\\]\n\\[\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n\\]\n\n\\[\n\\newcommand{\\Exp}{\\operatorname{E}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Mode}{\\operatorname{mode}}\n\\]\n\n\\[\n\\newcommand{\\pdfbinom}{{\\tt binom}}\n\\newcommand{\\pdfbeta}{{\\tt beta}}\n\\newcommand{\\pdfpois}{{\\tt poisson}}\n\\newcommand{\\pdfgamma}{{\\tt gamma}}\n\\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n\\]\n\n\\[\n\\newcommand{\\distbinom}{\\operatorname{B}}\n\\newcommand{\\distbeta}{\\operatorname{Beta}}\n\\newcommand{\\distgamma}{\\operatorname{Gamma}}\n\\newcommand{\\distexp}{\\operatorname{Exp}}\n\\newcommand{\\distpois}{\\operatorname{Poisson}}\n\\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n\\]\n\n\n\n\n\n5.1.1 Parameter space\nThe key here is to understand the idea of “parameter space”. Since we already know that the function we are looking for has a formula \\(y=b+wx\\), we could use the pair \\((b, w)\\) to denote different candidates of our answer. For example, the following plot show some possibilities in green dashed lines, while each possiblity is denoted by \\((b, w)\\). Then the problem is reworded as to find the best pair \\((b, w)\\).\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Loss function\nThe “best” is defined in the following way. The dataset is given \\(\\{(x_i, y_i)\\}\\). If we choose a pair of parameters \\((b,w)\\), we will have an estimated regression line, as well as a set of estimated \\(\\hat{y_i}\\). The idea is to let the difference between \\(y_i\\) and \\(\\hat{y_i}\\) is as small as possible. In other words, a loss function \\(J\\) is defined as follows:\n\\[\nJ_{\\{(x_i,y_i)\\}}(b,w)=\\frac1N\\sum_{i=1}^N(y_i-\\hat{y_i})^2=\\frac1N\\sum_{i=1}^N(y_i-b-wx_i)^2\n\\tag{5.1}\\] and we are expected to find the \\((b,w)\\) such that the loss function is minimized. The contour map of \\(J\\) is shown below.\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Gradient Descent\nWe use a technique called “gradient descent” to find the global minimal of \\(J\\). We start from a random point. For example \\((1.0, 1.5)\\). Then we find a direction where the cost \\(J\\) reduces the most, and move in that direction. This direction is computed by the gradient of the cost \\(J\\), and this is the reason why the algorithm is called “gradient descent”. After we get to a new point, we evaluate the new gradient and move in the new direction. The process is repeated and we are expected to get close to the minimal point after several iterations. Just like shown in the following plot.\n\n\n\n\n\n\n\n\n\nThe parameter updating rule is shown below. The \\(\\eta\\) is called the learning rate. It is a hyperparameter that is used to control the learning process.\n\\[\n\\begin{aligned}\n&\\pdv{J}{b}=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\quad &b_{new} = b_{old}-\\eta*\\pdv{J}{b},\\\\\n&\\pdv{J}{w}=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-x_i),\\quad &w_{new} = w_{old}-\\eta*\\pdv{J}{w},\n\\end{aligned}\n\\tag{5.2}\\]\n\n\n\n\n\n\nLearning rate \\(\\eta\\)\n\n\n\n\n\nGenerally speaking, larger \\(\\eta\\) will move faster to the global minimal, but might be jumpy which cause it harder to converge. On the other side, smaller \\(\\eta\\) moves in a more stable fashion, but may take a long time to converge. See the following examples.\n\n\n\n\n\n\n\n\n\nIn the first example, \\(\\eta\\) is too small, that after 200 iterations it is not very close to the minimal. In the second example, \\(\\eta\\) becomes large. Although it gets to somewhere near the minimal, the path is very jumpy. It is able to converge only because the problem is indeed an easy one.\n\n\n\nWe may record the curve of the cost function.\n\n\nAfter 200 iterations, the parameters are (2.291241352364798, 1.203587494484257).\n\n\n\n\n\n\n\n\n\nThe cost is close to \\(0\\) after 200 iterations and seems to be convergent. Therefore we believe that we are close to the minimal point. The point we get is (2.291241352364798, 1.203587494484257).\n\n\n5.1.4 Summary\nLet us summarize the example above and generalize it to the general case.\n\nLet \\(\\{(X_i, y_i)\\}\\) be a given dataset. Assume that \\(y=f_{\\Theta}(X)\\) where \\(\\Theta\\) is the set of all parameters.\nThe cost function \\(J_{\\Theta, \\{(X_i, y_i)\\}}\\) is defined.\nTo find the minimal point of the cost function, the gradient descent is applied:\n\nStart from a random initial point \\(\\theta_0\\).\nCompute the gradient \\(\\nabla J\\) and update \\(\\theta_i=\\theta_{i-1}- \\eta \\nabla J\\) and repeat the process multiple times.\nDraw the learning curve and determine when to stop. Then we get the estimated best parameters \\(\\hat{\\Theta}\\).\n\nOur model under this setting is sovled. We then turn to evaluation phase.\n\n\n\n\n\n\n\nNote\n\n\n\nThe above process can be further developped. We will discuss many of them in later sections.\n\nThe cost function is related to each concerte problem.\nTo compute the gradient of the cost function, chain rule is usually used. In the setting of MLP which we will discuss later, the gradient computations with chain rule are summarized as the so-called Back propagation.\nWe go through the data points to compute the graident. How many points do we use? What is the frenqucy to update the gradient? This belongs to the topic of mini-batch.\nEven when we know that the graident gives the best direction, sometimes we don’t really want to go in that direction, but make some modifications for some reason. To modify the direction, as well as choosing the learning rate \\(\\eta\\), is the subject of optimizers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#organize-the-outputs",
    "href": "contents/intro2pt/intro.html#organize-the-outputs",
    "title": "5  Intro to Pytorch",
    "section": "5.5 Organize the outputs",
    "text": "5.5 Organize the outputs\n\nWe create a Logger class to receive results and produce output for our training. The idea is that each time a mini-batch is trained, the model is sent to the Logger to record all infomation you need.\nThis class is supposed to be a placeholder at current stage. It is expected to overwrite it everytime since for different models we may want different information.\nOne key idea about the Logger class is that you are supposed to modify it for different models instead of modifying the Model class. After training one epoch, the model will send enough information to the Logger class and you can compute any metrics as you wish, like accuracy, precesion, etc., and create outputs.\n\nclass CustomLogger(object):\n    def __init__(self):\n        self.losses = []\n        self.val_losses = []\n        self.deltatime = []\n        self.n_epochs = 0\n        \n    def update(self, loss, val_loss, delta_time,\n               model, loss_fn, train_loader, val_loader):\n        self.losses.append(loss)\n        self.deltatime.append(delta_time)\n        self.val_losses.append(val_loss)\n        self.n_epochs += 1\n\n    def output(self):\n        s = f'epoch ({self.n_epochs}) ' \\\n            f'time: {self.deltatime[-1]} ' \\\n            f'loss: {self.losses[-1]} '    \n        if self.val_losses[-1] is not None:\n            s += f'val_loss: {self.val_losses[-1]}' \n        print(s)\n\nIn the logger our default records is the time used for each epoch as well as the loss and the validation loss after training for one epoch.\nIn the particular case, assuming we want more information, we could create a new class out of CustomLogger.\n\nclass MyLogger(CustomLogger):\n    def __init__(self):\n        super().__init__()\n        self.p = []\n    \n    def update(self, delta_time, loss, val_loss,\n               model, loss_fn, train_loader, val_loader):\n        super().update(delta_time, loss, val_loss,\n                       model, loss_fn, train_loader, val_loader)\n        p = model.state_dict()\n        self.p.append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n    def output(self):\n        s = f'epoch ({self.n_epochs}) ' \\\n            f'time: {self.deltatime[-1]} ' \\\n            f'loss: {self.losses[-1]} ' \\\n            f'p: {self.p[-1]} '\n        if self.val_losses[-1] is not None:\n            s += f'val_loss: {self.val_losses[-1]}' \n        print(s)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#sec-dataloader",
    "href": "contents/intro2pt/intro.html#sec-dataloader",
    "title": "5  Intro to Pytorch",
    "section": "5.4 Dataloader",
    "text": "5.4 Dataloader\n\n5.4.1 Convert the previous dataset using DataLoader\nUsually we use a class to provide data. The class is based on Dataset class, and need to implement the constructor, __getitem__ method and __len__ method. Here is an example.\n\n\n\n\n\n\nCaution\n\n\n\nNote that we directly change X and y to be 2D tensors when we create the dataset.\n\n\n\nfrom torch.utils.data import Dataset\n\nclass MyData(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x, dtype=float).reshape(-1, 1)\n        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\ntrain_data = MyData(X_train, y_train)\ntrain_data[1]\n\n(tensor([0.7713], dtype=torch.float64), tensor([3.1575], dtype=torch.float64))\n\n\nThen we use Dataloader to feed the data into our model.\n\nfrom torch.utils.data import DataLoader \ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n\nIt is used in the following way.\n\nlr = 0.2\nepoch_num = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = BetterLR().to(device)\noptimizer = SGD(model.parameters(), lr=lr)\n\nplist = []\nfor epoch in range(epoch_num):\n    for X_batch, y_batch in train_loader:\n        yhat = model(X_batch)\n        loss = MSELoss(reduction='mean')(yhat, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\nplist\n\n[[1.823910885110298, 1.8337838971747065],\n [1.943046588179278, 1.8159156063329505],\n [2.008997965681829, 1.7804701154492042],\n [2.0285249013384035, 1.7270125310793967],\n [2.0563058412652087, 1.6825156563053996],\n [2.0699681006441395, 1.6356185362954465],\n [2.105228875384819, 1.6040273694037541],\n [2.0967822112836743, 1.552514187091518],\n [2.124683201405048, 1.5249842249986072],\n [2.1368486965881486, 1.493812615037102]]\n\n\nWhen applying mini-batch, usually we will shuffle the dataset. If we disable the shuffle here as well as the shuffle in numpy case, you will see that we get exactly the same answer.\n\n\n\n\n\n\nNon-shuffle version\n\n\n\n\n\nHere is the result for non-shuffle version.\n\nfrom torch.utils.data import DataLoader \n\nlr = 0.2\nepoch_num = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = BetterLR().to(device)\noptimizer = SGD(model.parameters(), lr=lr)\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=False)\n\nplist = []\nfor epoch in range(epoch_num):\n    for X_batch, y_batch in train_loader:\n        yhat = model(X_batch)\n        loss = MSELoss(reduction='mean')(yhat, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\nplist\n\n[[1.8277028504755573, 1.8368193572906044],\n [1.9607104449826838, 1.8293981130981023],\n [2.001626059409397, 1.7815077539441087],\n [2.0286935704191, 1.7321715193480243],\n [2.0522055690695757, 1.686138097785071],\n [2.0736381185747943, 1.6437403254745735],\n [2.0933134526016604, 1.6047617600958677],\n [2.111393711486754, 1.5689357968513453],\n [2.1280105514943686, 1.5360086358936902],\n [2.143282725696795, 1.505745878510758]]\n\n\n\n\n\nYou may notice that we use some not-very-elegent way to display the result. Don’t worry about it. We will work on a better solution in the next section.\n\n\n5.4.2 Rewrite using random_split\nIt is possible to purely use PyTorch instead of going through sklearn. After we get the Dataset, we could use random_split to create training set and testing set.\n\nfrom torch.utils.data import random_split\nimport numpy as np\n\ndataset = MyData(X, y)\ntrain_data, val_data = random_split(dataset, [.85, .15], generator=torch.Generator().manual_seed(42))\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=32)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#assemble-everything-into-a-class",
    "href": "contents/intro2pt/intro.html#assemble-everything-into-a-class",
    "title": "5  Intro to Pytorch",
    "section": "5.6 Assemble everything into a class",
    "text": "5.6 Assemble everything into a class\nNow we can start to wrap up our previous code into classes. The idea is:\n\na Dataset class and corresponding dataloaders;\na Logger class which handle collect losses and other important information;\na Model class which load data from Dataset and then send training results to Logger.\n\nTo put the above things together, we have the following code.\n\nDefine all the classes (CustomLogger, ModelTemplete, MyData, etc.) we mentioned before as well as import all modules we need.\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import MSELoss\nfrom torch.nn.modules import Linear\nfrom torch.optim import SGD\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nimport numpy as np\nimport time\n\n\nclass MyData(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x, dtype=float).reshape(-1, 1)\n        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\n\nclass CustomLogger(object):\n    def __init__(self):\n        # self.losses = []\n        # self.val_losses = []\n        # self.deltatime = []\n        # self.n_epochs = 0\n        self.states = {'losses': [],\n                       'val_losses': [],\n                       'deltatime': [],\n                       'n_epochs': 0}\n        \n    def update(self, loss, val_loss, delta_time,\n               model, loss_fn, train_loader, val_loader):\n        self.states['losses'].append(loss)\n        self.states['deltatime'].append(delta_time)\n        self.states['val_losses'].append(val_loss)\n        self.states['n_epochs'] += 1\n\n    def output(self):\n        s = f'epoch ({self.states['n_epochs']}) ' \\\n            f'time: {self.states['deltatime'][-1]} ' \\\n            f'loss: {self.states['losses'][-1]} '    \n        if self.states['val_losses'][-1] is not None:\n            s += f'val_loss: {self.states['val_losses'][-1]}' \n        print(s)\n\n\nclass ModelTemplete():\n    def __init__(self, model, loss_fn, optimizer, logger=None):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model.to(self.device)\n        self.train_loader = None\n        self.val_loader = None\n        self.logger = logger\n        self.n_epochs = 0\n\n    def set_loaders(self, train_loader, val_loader=None):\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def set_logger(self, msg):\n        self.logger = msg\n\n    def to(self, device):\n        self.device = device\n        self.model.to(device)\n    \n    def _minibatch_one_epoch(self, val=False):\n        if val is False:\n            self.model.train()\n        else:\n            self.model.eval()\n        dataloader = self.val_loader if val else self.train_loader\n\n        losses = []\n        for X_batch, y_batch in dataloader:\n            X_batch = X_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n            yhat = self.model(X_batch)\n            loss = self.loss_fn(yhat, y_batch)\n            if val is False:\n                loss.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n            losses.append(loss.item())\n        return np.mean(losses)\n    \n    def train(self, epoch_num=10, verbose=0):\n        for _ in range(epoch_num):\n            start_time = time.time()\n            loss = self._minibatch_one_epoch(val=False)\n            end_time = time.time()\n            self.n_epochs += 1\n            delta_time = end_time - start_time\n            val_loss = self._minibatch_one_epoch(val=True) if self.val_loader is not None else None\n            self.logger.update(loss, val_loss, delta_time,\n                               self.model, self.loss_fn, self.train_loader, self.val_loader)\n            if verbose == 1:\n                self.logger.output()\n\n\nDefine the dataset and the dataloaders.\n\n\nSEED = 42\nnp.random.seed(SEED)\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\n\ndataset = MyData(X, y)\ntrain_data, val_data = random_split(dataset, [.85, .15], generator=torch.Generator().manual_seed(SEED))\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=32)\n\n\nOverwrite the Logger to accomdate displaying parameters. We also restirct the number of digits to be 6.\n\n\nclass MyLogger(CustomLogger):\n    def __init__(self):\n        super().__init__()\n        self.states['p'] = []\n    \n    def update(self, delta_time, loss, val_loss,\n               model, loss_fn, train_loader, val_loader):\n        super().update(delta_time, loss, val_loss,\n                       model, loss_fn, train_loader, val_loader)\n        p = model.state_dict()\n        self.states['p'].append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n    def output(self):\n        s = f'epoch ({self.states['n_epochs']}) ' \\\n            f'time: {self.states['deltatime'][-1]:.6f} ' \\\n            f'loss: {self.states['losses'][-1]:.6f} ' \\\n            f'p: [{self.states['p'][-1][0]:.6f}, {self.states['p'][-1][1]:.6f}] '\n        if self.states['val_losses'][-1] is not None:\n            s += f'val_loss: {self.states['val_losses'][-1]:.6f}' \n        print(s)\n\n\nDefine the model and create its instance, and then connect it with the Logger instance. Note that we remove the fixed starting point, and setting dtype to be float (which is actually double since it will be understood as torch.float64 as shown before).\n\n\nclass BetterLR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.linear = Linear(in_features=1, out_features=1, dtype=float)\n\n    def forward(self, x):\n        return self.linear(x)\n\nlr = 0.2\n\nmodel_template = BetterLR()\noptimizer = SGD(model_template.parameters(), lr=lr)\nmsg = MyLogger()\n\nmodel = ModelTemplete(model_template, MSELoss(reduction='mean'), optimizer, msg)\nmodel.set_loaders(train_loader, val_loader)\n\nmodel.train(epoch_num=10, verbose=1)    \n\nepoch (1) time: 0.003022 loss: 3.600374 p: [2.186446, 0.693327] val_loss: 0.129974\nepoch (2) time: 0.002090 loss: 0.077938 p: [2.420139, 0.849980] val_loss: 0.008887\nepoch (3) time: 0.000983 loss: 0.017664 p: [2.440170, 0.891922] val_loss: 0.006367\nepoch (4) time: 0.001950 loss: 0.015303 p: [2.438123, 0.920726] val_loss: 0.005891\nepoch (5) time: 0.002028 loss: 0.013888 p: [2.433903, 0.945942] val_loss: 0.005665\nepoch (6) time: 0.002994 loss: 0.013115 p: [2.428473, 0.966176] val_loss: 0.005525\nepoch (7) time: 0.002005 loss: 0.012366 p: [2.397525, 0.971507] val_loss: 0.005009\nepoch (8) time: 0.003089 loss: 0.011742 p: [2.395197, 0.989601] val_loss: 0.004777\nepoch (9) time: 0.001947 loss: 0.011255 p: [2.400681, 1.011229] val_loss: 0.004815\nepoch (10) time: 0.001999 loss: 0.011060 p: [2.384192, 1.020468] val_loss: 0.004562",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#other-misc-stuffs",
    "href": "contents/intro2pt/intro.html#other-misc-stuffs",
    "title": "5  Intro to Pytorch",
    "section": "5.5 Other misc stuffs",
    "text": "5.5 Other misc stuffs\n\n5.5.1 Organize the outputs\nWe create a Logger class to receive results and produce output for our training. The idea is that each time a mini-batch is trained, the model is sent to the Logger to record all infomation you need.\nThis class is supposed to be a placeholder at current stage. It is expected to overwrite it everytime since for different models we may want different information.\nOne key idea about the Logger class is that you are supposed to modify it for different models instead of modifying the Model class. After training one epoch, the model will send enough information to the Logger class and you can compute any metrics as you wish, like accuracy, precesion, etc., and create outputs.\n\nclass CustomLogger(object):\n    def __init__(self):\n        self.states = {'losses': [],\n                       'val_losses': [],\n                       'deltatime': [],\n                       'n_epochs': 0}\n        \n    def update(self, loss, val_loss, delta_time,\n               model, loss_fn, train_loader, val_loader):\n        self.states['losses'].append(loss)\n        self.states['deltatime'].append(delta_time)\n        self.states['val_losses'].append(val_loss)\n        self.sates['n_epochs'] += 1\n\n    def output(self):\n        s = f'epoch ({self.states['n_epochs']}) ' \\\n            f'time: {self.states['deltatime'][-1]} ' \\\n            f'loss: {self.states['losses'][-1]} '    \n        if self.states['val_losses'][-1] is not None:\n            s += f'val_loss: {self.states['val_losses'][-1]}' \n        print(s)\n\nIn the logger our default records is the time used for each epoch as well as the loss and the validation loss after training for one epoch.\nIn the particular case, assuming we want more information, we could create a new class out of CustomLogger.\n\nclass MyLogger(CustomLogger):\n    def __init__(self):\n        super().__init__()\n        self.states['p'] = []\n    \n    def update(self, delta_time, loss, val_loss,\n               model, loss_fn, train_loader, val_loader):\n        super().update(delta_time, loss, val_loss,\n                       model, loss_fn, train_loader, val_loader)\n        p = model.state_dict()\n        self.states['p'].append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n    def output(self):\n        s = f'epoch ({self.states['n_epochs']}) ' \\\n            f'time: {self.states['deltatime'][-1]} ' \\\n            f'loss: {self.states['losses'][-1]} ' \\\n            f'p: {self.states['p'][-1]} '\n        if self.states['val_losses'][-1] is not None:\n            s += f'val_loss: {self.states['val_losses'][-1]}' \n        print(s)\n\n\n\n5.5.2 Setting random seeds\n\n\n5.5.3 Save the model\nSometimes we would like to save and load the model state. To be specific, we would like to record:\n\nmodel parameters: which is model.state_dict();\nthe state of the optimizer: optimizer.state_dict();\nthe state of the logger",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/intro2pt/intro.html#wrap-up-in-a-class",
    "href": "contents/intro2pt/intro.html#wrap-up-in-a-class",
    "title": "5  Intro to Pytorch",
    "section": "5.5 Wrap up in a class",
    "text": "5.5 Wrap up in a class\nWe use a class to wrap up our model and the training process. One of the benefits is that we could record a lot of information in the class.\n\n5.5.1 Organize the outputs\nWe add a dict stats in the class as an attribute. After we train the model, we will evalute some metrics and add it to the stats dict. The default stats we choose is the training loss, validation loss and the time to train an epoch. We could inherite the class to add more stats. the following code is not complete, so I fold it. Complete code can be found later.\n\n\nCode\nclass ModelTemplete():\n    def __init__(self, model, loss_fn, optimizer):\n        # ignore some other stuffs\n        self.stats = {'losses': [],\n                        'val_losses': [],\n                        'delta_time': [],\n                        'n_epochs': 0}\n\n    def log_update(self, delta_time, loss, val_loss):\n        self.stats['delta_time'].append(delta_time)\n        self.stats['losses'].append(loss)\n        self.stats['val_losses'].append(val_loss)\n        self.stats['n_epochs'] += 1\n\n    def log_output(self, verbose=0, formatstr=''):\n        s = [f'epoch {self.stats['n_epochs']}',\n             f'train_time: {{{formatstr}}}'.format(self.stats['train_time'][-1]),\n             f'loss: {{{formatstr}}}'.format(self.stats['losses'][-1])]\n        if self.stats['val_losses'][-1] is not None:\n            s.append(f'val_time: {{{formatstr}}}'.format(self.stats['val_time'][-1]))\n            s.append(f'val_loss: {{{formatstr}}}'.format(self.stats['val_losses'][-1]))\n        if verbose == 1:\n            print(' '.join(s))\n        return s\n    \n\n    def train(self, train_loader, val_loader=None, epoch_num=10, verbose=0):\n        for _ in range(epoch_num):\n           # ignore some other stuffs\n            self.log_update(delta_time, loss, val_loss)\n            self.log_output(verbose=verbose)\n\n\nWhen we derive the base class, we could add more stats to it. Note that the output is stored as a list, so it is easy to add more display to it as well as changing display order if necessary.\nIn this example, the base class is extended, and the bias and the weight are shown in the output.\n\n\nCode\nclass MyModel(ModelTemplete):\n    def __init__(self, model, loss_fn, optimizer):\n        super().__init__(model, loss_fn, optimizer)\n        self.states['p'] = []\n\n    def log_update(self, delta_time, loss, val_loss):\n        super().log_update(delta_time, loss, val_loss)\n        p = self.model.state_dict()\n        self.states['p'].append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n\n    def log_output(self, verbose=0):\n        s = super().log_output(verbose=0, formatstr=':.6f')\n        s.append(f'p: [{self.states['p'][-1][0]}, {self.states['p'][-1][1]}]')\n        if verbose==1:\n            print(' '.join(s))\n        return s\n\n\n\n\n5.5.2 Setting random seeds\nFollowing the guideline, we add a method to our class to manually set random seed in order to reproduce our results.\n\n\nCode\nimport numpy as np\nimport random\n\ndef set_seed(self, seed=42):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n\n\n5.5.3 Save the model\nSometimes we would like to save and load the model state. To be specific, we would like to record:\n\nmodel parameters: which is model.state_dict();\nthe state of the optimizer: optimizer.state_dict();\nthe stats of the model: stats dict.\n\nWe use torch.save and torch.load to write and read files. We use a dict to store all the above information.\n\n\nCode\nimport torch\ndef save(self, filename='model.pth'):\n    model_state = {\n        'model': self.model.state_dict()\n        'optimizer': self.optimizer.state_dict()\n        'stats': self.stats\n    }\n    torch.save(model_state, filename)\n\ndef load(self, filename='model.pth'):\n    model_state = torch.load(filename, weights_only=False)\n    self.model.load_state_dict(model_state['model'])\n    self.optimizer.load_state_dict(model_state['optimizer'])\n    self.stats = model_state['stats']\n\n    self.model.train()\n\n\n\n\n5.5.4 Predict values\nFor this part, we assume that our data comes from numpy.array and we will produce results in terms of numpy.array. Note that to turn a torch.tensor to a numpy.array, we need to detach it first, send it back to cpu and then transform it to numpy array.\n\n\nCode\ndef predict(self, X):\n    self.model.eval()\n    X_tensor = torch.as_tensor(X, dtype=float)\n    y_tensor = self.model(X_tensor.to(self.device))\n    self.model.train()\n    y = y_tensor.detach().cpu().numpy()\n\n    return y\n\n\n\n\n5.5.5 Put things together\nNow we can start to wrap up our previous code into classes. The idea is:\n\na Dataset class and corresponding dataloaders;\na Model class which load data from Dataset for training.\n\nThe ModelTemplate class and MyData are defined as follows.\n\nimport torch\nfrom torch.utils.data import Dataset\n\nimport numpy as np\nimport random\nimport time\n\n\nclass MyData(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x, dtype=float).reshape(-1, 1)\n        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\n\nclass ModelTemplate():\n    def __init__(self, model, loss_fn, optimizer):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model.to(self.device)\n\n        self.stats = {'losses': [],\n                       'val_losses': [],\n                       'train_time': [],\n                       'val_time': [],\n                       'n_epochs': 0}\n    \n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def save(self, filename='model.pth'):\n        model_state = {\n            'model': self.model.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'stats': self.stats\n        }\n        torch.save(model_state, filename)\n\n    def load(self, filename='model.pth'):\n        model_state = torch.load(filename, weights_only=False)\n        self.model.load_state_dict(model_state['model'])\n        self.optimizer.load_state_dict(model_state['optimizer'])\n        self.stats = model_state['stats']\n\n        self.model.train()\n\n    def log_update(self, train_time, loss, val_time, val_loss):\n        self.stats['train_time'].append(train_time)\n        self.stats['losses'].append(loss)\n        self.stats['val_time'].append(val_time)\n        self.stats['val_losses'].append(val_loss)\n        self.stats['n_epochs'] += 1\n\n    def log_output(self, verbose=1, formatstr=''):\n        s = [f'epoch {self.stats['n_epochs']}',\n             f'train_time: {{{formatstr}}}'.format(self.stats['train_time'][-1]),\n             f'loss: {{{formatstr}}}'.format(self.stats['losses'][-1])]\n        if self.stats['val_losses'][-1] is not None:\n            s.append(f'val_time: {{{formatstr}}}'.format(self.stats['val_time'][-1]))\n            s.append(f'val_loss: {{{formatstr}}}'.format(self.stats['val_losses'][-1]))\n        if verbose == 1:\n            print(' '.join(s))\n        return s\n    \n    def _train_one_epoch(self, dataloader):\n        self.model.train()\n\n        losses = []\n        for X_batch, y_batch in dataloader:\n            X_batch = X_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n            yhat = self.model(X_batch)\n            loss = self.loss_fn(yhat, y_batch)\n\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            losses.append(loss.item())\n        return np.mean(losses)\n\n    def _eval_one_epoch(self, dataloader):\n        self.model.eval()\n        losses = []\n        for X_batch, y_batch in dataloader:\n            X_batch = X_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n            yhat = self.model(X_batch)\n            loss = self.loss_fn(yhat, y_batch)\n            losses.append(loss.item())\n        return np.mean(losses)\n    \n    def train(self, train_loader, val_loader=None, epoch_num=10, verbose=0, SEED=42):\n        self.set_seed(SEED)\n        for _ in range(epoch_num):\n            start_time = time.time()\n            loss = self._train_one_epoch(train_loader)\n            end_time = time.time()\n            train_time = end_time - start_time\n\n            val_loss = None\n            val_time = None\n            if val_loader is not None:\n                start_time = time.time()\n                val_loss = self._eval_one_epoch(val_loader)\n                end_time = time.time()\n                val_time = end_time - start_time\n\n            self.log_update(train_time, loss, val_time, val_loss)\n            self.log_output(verbose=verbose)\n\n    def predict(self, X):\n        self.model.eval()\n        X_tensor = torch.as_tensor(X, dtype=float)\n        y_tensor = self.model(X_tensor.to(self.device))\n        self.model.train()\n        y = y_tensor.detach().cpu().numpy()\n\n        return y\n\nHere is a quick example to use it.\n\nWe use our initial dataset. We wrap it into a Dataset class and then create Dataloader.\n\n\nfrom torch.utils.data import DataLoader, random_split\n\nSEED = 42\nnp.random.seed(SEED)\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\n\ndataset = MyData(X, y)\ntrain_data, val_data = random_split(dataset, [.85, .15], generator=torch.Generator().manual_seed(SEED))\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=32)\n\nThe code to go from train_test_split is kept for comparison.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=SEED)\ntrain_data = MyData(X_train, y_train)\nval_data = MyData(X_test, y_test)\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=False)\nval_loader = DataLoader(val_data, batch_size=32)\n\n\n\nThe ModelTemplate class is derived, since we would like to add some stats to the output.\n\n\nclass MyModel(ModelTemplate):\n    def __init__(self, model, loss_fn, optimizer):\n        super().__init__(model, loss_fn, optimizer)\n        self.stats['p'] = []\n\n    def log_update(self, train_time, loss, val_time, val_loss):\n        super().log_update(train_time, loss, val_time, val_loss)\n        p = self.model.state_dict()\n        self.stats['p'].append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n\n    def log_output(self, verbose=0):\n        s = super().log_output(verbose=0, formatstr='')\n        s.append(f'p: [{self.stats['p'][-1][0]}, {self.stats['p'][-1][1]}]')\n        if verbose==1:\n            print(' '.join(s))\n        return s\n\n\nDefine the model. Note that for comparison we still keep the fixed inital point.\n\n\nimport torch.nn as nn\nfrom torch.nn.modules import Linear\n\nclass BetterLR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.linear = Linear(in_features=1, out_features=1)\n        self.linear.bias = torch.nn.Parameter(torch.tensor([1.0], dtype=float))\n        self.linear.weight = torch.nn.Parameter(torch.tensor([[1.5]], dtype=float))\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nFinally we create instances for all our classes and train the model.\n\n\nfrom torch.nn import MSELoss\nfrom torch.optim import SGD\n\nlr = 0.2\n\noriginal_model = BetterLR()\noptimizer = SGD(original_model.parameters(), lr=lr)\n\nmodel = MyModel(original_model, MSELoss(reduction='mean'), optimizer)\n\nmodel.train(train_loader, val_loader, epoch_num=10, verbose=1)   \n\nepoch 1 train_time: 0.0019986629486083984 loss: 0.6285549775585894 val_time: 0.0 val_loss: 0.08309839380360311 p: [1.8326121457016133, 1.8436582542519988]\nepoch 2 train_time: 0.0020220279693603516 loss: 0.06333274128811019 val_time: 0.0005290508270263672 val_loss: 0.05205464137577776 p: [1.9544549867326009, 1.8263730567501348]\nepoch 3 train_time: 0.002012968063354492 loss: 0.049854150673716284 val_time: 0.0010311603546142578 val_loss: 0.04319500869177139 p: [2.0241479022478166, 1.7870470286875666]\nepoch 4 train_time: 0.0029675960540771484 loss: 0.04365503207473389 val_time: 0.0 val_loss: 0.03790580120725924 p: [2.0573670857921185, 1.7376097906636723]\nepoch 5 train_time: 0.001998424530029297 loss: 0.0368256617708487 val_time: 0.0 val_loss: 0.033074176924915925 p: [2.065529356635095, 1.6803134595301177]\nepoch 6 train_time: 0.0020356178283691406 loss: 0.030429029638366592 val_time: 0.0 val_loss: 0.029300502227132194 p: [2.084207932176236, 1.6360982163365212]\nepoch 7 train_time: 0.002086162567138672 loss: 0.027455579120048812 val_time: 0.0010027885437011719 val_loss: 0.0261865606482943 p: [2.09661619189228, 1.5921630628600543]\nepoch 8 train_time: 0.00299835205078125 loss: 0.02469912122202487 val_time: 0.0 val_loss: 0.02398854935480747 p: [2.103263011691683, 1.54999103204098]\nepoch 9 train_time: 0.002998828887939453 loss: 0.022019571680916682 val_time: 0.0010263919830322266 val_loss: 0.022135819467295682 p: [2.112790380739411, 1.5138505373151196]\nepoch 10 train_time: 0.00403285026550293 loss: 0.01975395452013642 val_time: 0.0015120506286621094 val_loss: 0.01886746694692108 p: [2.149469768365474, 1.495273613868113]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  }
]