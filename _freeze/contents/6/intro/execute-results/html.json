{
  "hash": "fcc04043416afa5d8c790e0b2fa1e416",
  "result": {
    "engine": "jupyter",
    "markdown": "# Netural networks\n\nThere are many different architects of netural networks. In our course we will only talk about the simplest one: multilayer perceptron (MLP). We will treat it as the generalization of logistic regression. In other words, we will treat logistic regression as an one-layer netural network. Under this idea, all the concepts and ideas, like gradient descent, mini-batch training, loss functions, learning curves, etc.. will be used. \n\n\n\n## Neural network: Back propagation\n\n\n\n\n::: {.hidden}\n$$\n\\newcommand\\diffp[2]{\\dfrac{\\partial #1}{\\partial #2}}\n$$\n:::\n\nTo train a MLP model, we still use gradient descent. Therefore it is very important to know how to compute the gradient. Actually the idea is the same as logistic regreesion. The only issue is that now the model is more complicated. The gradient computation is summrized as an algorithm called `back propagation`. It is described as follows.\n\nHere is an example of a Neural network with one hidden layer.\n\n![](assests/img/20221114232327.png)  \n\n    \n$\\Theta$ is the coefficients of the whole Neural network. \n \n\n\n- $a^{(1)}=\\hat{\\textbf{x}}$ is the input. $a_0^{(1)}$ is added. This is an $(n+1)$-dimension column vector.\n- $\\Theta^{(1)}$ is the coefficient matrix from the input layer to the hidden layer, of size $k\\times(n+1)$.\n- $z^{(2)}=\\Theta^{(1)}a^{(1)}$.\n- $a^{(2)}=\\sigma(z^{(2)})$, and then add $a^{(2)}_0$. This is an $(k+1)$-dimension column vector.\n- $\\Theta^{(2)}$ is the coefficient matrix from the hidden layer to the output layer, of size $r\\times(k+1)$.\n- $z^{(3)}=\\Theta^{(2)}a^{(2)}$.\n- $a^{(3)}=\\sigma(z^{(3)})$. Since this is the output layer, $a^{(3)}_0$ won't be added.\n    % \\item These $a^{(3)}$ are $h_{\\Theta}(\\textbf{x})$.\n\n\nThe dependency is as follows:\n\n- $J$ depends on $z^{(3)}$ and $a^{(3)}$.\n- $z^{(3)}$ and $a^{(3)}$ depends on $\\Theta^{(2)}$ and $a^{(2)}$.\n- $z^{(2)}$ and $a^{(2)}$ depends on $\\Theta^{(1)}$ and $a^{(1)}$.\n- $J$ depends on $\\Theta^{(1)}$, $\\Theta^{(2)}$ and $a^{(1)}$.\n\n\nEach layer is represented by the following diagram:\n\n![](assests/img/20221114232354.png)  \n\n\n\nThe diagram says:\n\n$$\nz^{(k+1)}=b^{(k)}+\\Theta^{(k)}a^{(k)},\\quad z^{(k+1)}_j=b^{(k)}_j+\\sum \\Theta^{(k)}_{jl}a^{(k)}_l,\\quad a^{(k)}_j=\\sigma(z^{(k)}_j).\n$$\n\nAssume $r,j\\geq1$. Then\n\n$$\n\\begin{aligned}\n\\diffp{z^{(k+1)}_i}{a^{(k)}_r}&=\\diffp*{\\left(b^{(k)}_i+\\sum\\Theta^{(k)}_{il}a^{(k)}_l\\right)}{a^{(k)}_r}=\\Theta_{ir}^{(k)},\\\\\n% \\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{ij}}&=\\diffp*{\\qty(a^{(k)}_0+\\sum\\Theta^{(k)}_{il}a^{(k)}_l)}{\\Theta^{(k)}_{ij}}=a^{(k)}_j,\\\\\n\\diffp{z^{(k+1)}_i}{z^{(k)}_j}&=\\sum_r \\diffp{z^{(k+1)}_i}{a^{k}_r}\\diffp{a^{(k)}_r}{z^{(k)}_j}+\\sum_{p,g}\\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{pq}}\\diffp{\\Theta^{(k)}_{pq}}{z^{(k)}_j}+\\sum_r \\diffp{z^{(k+1)}_i}{b^{k}_r}\\diffp{b^{(k)}_r}{z^{(k)}_j}\\\\\n&=\\sum_r \\Theta^{(k)}_{ir}\\diffp{a^{(k)}_r}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\diffp{a^{(k)}_j}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\sigma'(z^{(k)}_j),\\\\\n\\diffp{J}{z^{(k)}_j}&=\\sum_r \\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{z^{(k)}_j}=\\sum_r\\diffp{J}{z^{(k+1)}_r}\\Theta^{(k)}_{rj}\\sigma'(z^{(k)}_j).\n\\end{aligned}\n$$\n\nWe set \n\n- $\\delta^k_j=\\diffp{J}{z^{(k)}_j}$, $\\delta^k=\\left[\\delta^k_1,\\delta_2^k,\\ldots\\right]^T$.\n- $\\mathbf{z}^k=\\left[z^{(k)}_1,z^{(k)}_2,\\ldots\\right]^T$, $\\mathbf{a}^k=\\left[a^{(k)}_1,a^{(k)}_2,\\ldots\\right]^T$,\n    $\\hat{\\mathbf{a}}^k=\\left[a^{(k)}_0,a^{(k)}_1,\\ldots\\right]^T$.\n- $\\Theta^{k}=\\left[\\Theta^{(k)}_{ij}\\right]$.\n\nThen we have the following formula. Note that there are ``$z_0$'' terms.\n\n$$\n    \\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k).\n$$\n\n\n\n$$\n\\begin{aligned}\n\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}&=\\diffp*{\\left(b^{(k)}_r+\\sum_l\\Theta^{(k)}_{rl}a^{(k)}_l\\right)}{\\Theta^{(k)}_{pq}}=\\begin{cases}\n0&\\text{ for }r\\neq q,\\\\\na^{(k)}_q&\\text{ for }r=q,\n\\end{cases}\\\\\n\\diffp{J}{\\Theta^{(k)}_{pq}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}=\\diffp{J}{z^{(k+1)}_p}\\diffp{z^{(k+1)}_p}{\\Theta^{(k)}_{pq}}=\\delta^{k+1}_pa^{k}_q,\\\\\n\\diffp{J}{b^{(k)}_{j}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}\\diffp{z^{(k+1)}_j}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}=\\delta^{k+1}_j.\n\\end{aligned}\n$$\n\nExtend $\\hat{\\Theta}=\\left[b^{(k)},\\Theta^{(k)}\\right]$, and $\\partial^k J=\\left[\\diffp{J}{\\hat{\\Theta}^{(k)}_{ij}}\\right]$. Then\n$$\n    \\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right].\n$$\nThen the algorithm is as follows.\n\n1. Starting from $x$, $y$ and some random $\\Theta$.\n1. Forward computation: compute $z^{(k)}$ and $a^{(k)}$. The last $a^{(n)}$ is $h$.\n1. Compute $\\delta^n=\\nabla J\\circ\\sigma'(z^{(n)})$. In the case of $J=\\frac12||{h-y}||^2$, $\\nabla J=(a^{(n)}-y)$, and then $\\delta^n=(a^{(n)}-y)\\circ\\sigma'(z^{(n)})$.\n1. Backwards: $\\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k)$, and $\\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right]$ .\n\n\n\n\n::: {#exm-}\n\n\nConsider there are 3 layers: input, hidden and output. There are $m+1$ nodes in the input layer, $n+1$ nodes in the hidden layer and $k$ in the output layer. Therefore\n\n- $a^{(1)}$ and $\\delta^1$ are $m$-dim column vectors.\n- $z^{(2)}$, $a^{(2)}$ and $\\delta^2$ are $n$-dim column vectors.\n- $z^{(3)}$, $a^{(3)}$ and $\\delta^3$ are $k$-dim column vectors.\n- $\\hat{\\Theta}^1$ is $n\\times(m+1)$, $\\hat{\\Theta}^2$ is $k\\times(n+1)$.\n- $z^{(2)}=b^{(1)}+\\Theta^{(1)}a^{(1)}=\\hat{\\Theta}^{(1)}\\hat{a}^{(1)}$, $z^{(3)}=b^{(2)}+\\Theta^{(2)}a^{(2)}=\\hat{\\Theta}^{(2)}\\hat{a}^{(2)}$.\n- $\\delta^3=\\nabla_aJ\\circ\\sigma'(z^{(3)})$. This is a $k$-dim column vector.\n- $\\partial^2 J=\\left[\\delta^3,\\delta^3(a^{(2)})^T\\right]$.\n- $\\delta^2=\\left[(\\Theta^2)^T\\delta^3\\right]\\circ \\sigma'(z^{(2)})$, where $(\\hat{\\Theta^2})^T\\delta^3=(\\hat{\\Theta^2})^T\\delta^3$ and then remove the first row.\n- $\\delta^1=\\begin{bmatrix}(\\Theta^1)^T\\delta^2\\end{bmatrix}\\circ \\sigma'(z^{(1)})$, where $(\\hat{\\Theta^1})^T\\delta^2=(\\hat{\\Theta^1})^T\\delta^2$ and then remove the first row.\n- $\\partial^1 J=\\left[\\delta^2,\\delta^2(a^{(1)})^T\\right]$.\n- When $J=-\\frac1m\\sum y\\ln a+(1-y)\\ln(1-a)$, $\\delta^3=\\frac1m(\\sum a^{(3)}-\\sum y)$.\n\n\n\n:::\n\n\n\n## Example\n\nLet us take some of our old dataset as an example. This is an continuation of the horse colic dataset from Logistic regression.\n\n::: {#d8c119ad .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\nmms.fit(X_train)\nX_train = mms.transform(X_train)\nX_test = mms.transform(X_test)\n```\n:::\n\n\nNow we build a neural network. This is a 2-layer model, with 1 hidden layer with 10 nodes.\n\n::: {#b404334f .cell execution_count=2}\n``` {.python .cell-code}\n# import keras_core as keras\nfrom keras import models, layers, Input\nmodel = models.Sequential()\n\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(layers.Dense(10, activation='sigmoid'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n```\n:::\n\n\nAnd the learning curve are shown in the following plots.\n\n::: {#d88d53f1 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-4-output-1.png){width=579 height=411}\n:::\n:::\n\n\nIt seems that our model has overfitting issues. Therefore we need to modifify the architects of our model. The first idea is to add `L2` regularization as we talked about it in LogsiticRegression case. Here we use `0.01` as the regularization strenth.\n\nLet us add the layer to the model and retrain it.\n\n::: {#93883ce4 .cell execution_count=4}\n``` {.python .cell-code}\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1], kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n```\n:::\n\n\n::: {#2e9f922e .cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-6-output-1.png){width=571 height=411}\n:::\n:::\n\n\nAnother way to deal with overfitting is to add a `Dropout` layer. The idea is that when training the model, part of the data will be randomly discarded. Then after fitting, the model tends to reduce the variance, and then reduce the overfitting. \n\nThe code of a `Dropout` layer is listed below. Note that the number represents the percentage of the training data that will be dropped.\n\n::: {#08b9f8ae .cell execution_count=6}\n``` {.python .cell-code}\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n```\n:::\n\n\n::: {#489e09ab .cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-8-output-1.png){width=579 height=412}\n:::\n:::\n\n\nAfter playing with different hyperparameters, the overfitting issues seem to be better (but not entirely fixed). However, the overall performance is getting worse. This means that the model is moving towards underfitting side. Then we may add more layers to make the model more complicated in order to capture more information.\n\n::: {#eb282e2a .cell execution_count=8}\n``` {.python .cell-code}\n# import keras_core as keras\nfrom keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(10, activation='sigmoid', input_dim=X_train.shape[1]))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=500, batch_size=30, validation_data=(X_test, y_test), verbose=0)\n\nloss_train = hist.history['loss']\nloss_val = hist.history['val_loss']\n\nacc_train = hist.history['accuracy']\nacc_val = hist.history['val_accuracy']\n```\n:::\n\n\n::: {#1e0e5143 .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-10-output-1.png){width=579 height=411}\n:::\n:::\n\n\nAs you may see, to build a netural network model it requires many testing. There are many established models. When you build your own architecture, you may start from there and modify it to fit your data.\n\n## Exercises and Projects\n\n\n::: {#exr-}\nPlease hand write a report about the details of back propagation.\n\n:::\n\n\n\n\n\n::: {#exr-}\nCHOOSE ONE: Please use netural network to one of the following datasets. \n- the `iris` dataset.\n- the dating dataset.\n- the `titanic` dataset.\n\nPlease in addition answer the following questions.\n\n1. What is your accuracy score?\n2. How many epochs do you use?\n3. What is the batch size do you use?\n4. Plot the learning curve (loss vs epochs, accuracy vs epochs).\n5. Analyze the bias / variance status.\n\n:::\n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}