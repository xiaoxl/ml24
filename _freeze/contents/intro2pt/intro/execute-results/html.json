{
  "hash": "30ff26624ce5070e8a7a57f022ee9fc2",
  "result": {
    "engine": "jupyter",
    "markdown": "# Intro to Pytorch\n\n\nMost materials are based on @Godoy2022.\n\n\n\n## Linear regression (math) {#sec-linearregression_math}\n\nWe only consider the simplest case: simple linear regression (SLR). The idea is very simple. The dataset contains two variables (the independent variable $x$ and the response variable $y$.) The goal is to find the relation between $x$ and $y$ with the given dataset. We assume their relation is $y=b+wx$. How do we find $b$ and $w$?\n\nLet us first see an example. We would like to find the red line (which is the best fitted curve) shown below.\n\n::: {#d32a167c .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-2-output-1.png){width=573 height=421}\n:::\n:::\n\n\n### Parameter space\n\nThe key here is to understand the idea of \"parameter space\". Since we already know that the function we are looking for has a formula $y=b+wx$, we could use the pair $(b, w)$ to denote different candidates of our answer. For example, the following plot show some possibilities in green dashed lines, while each possiblity is denoted by $(b, w)$. Then the problem is reworded as to find the best pair $(b, w)$.\n\n::: {#d4b3ae45 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-3-output-1.png){width=573 height=421}\n:::\n:::\n\n\n### Loss function\nThe \"best\" is defined in the following way. The dataset is given $\\{(x_i, y_i)\\}$. If we choose a pair of parameters $(b,w)$, we will have an estimated regression line, as well as a set of estimated $\\hat{y_i}$. The idea is to let the difference between $y_i$ and $\\hat{y_i}$ is as small as possible. In other words, a **loss function** $J$ is defined as follows:\n\n$$\nJ_{\\{(x_i,y_i)\\}}(b,w)=\\frac1N\\sum_{i=1}^N(y_i-\\hat{y_i})^2=\\frac1N\\sum_{i=1}^N(y_i-b-wx_i)^2\n$$\nand we are expected to find the $(b,w)$ such that the loss function is minimized. The contour map of $J$ is shown below. \n\n::: {#06015841 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'w')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-4-output-2.png){width=574 height=441}\n:::\n:::\n\n\n### Gradient Descent\n\nWe use a technique called \"gradient descent\" to find the global minimal of $J$. We start from a random point. For example $(1.0, 1.5)$. Then we find a direction where the cost $J$ reduces the most, and move in that direction. This direction is computed by the gradient of the cost $J$, and this is the reason why the algorithm is called \"gradient descent\". After we get to a new point, we evaluate the new gradient and move in the new direction. The process is repeated and we are expected to get close to the minimal point after several iterations. Just like shown in the following plot.\n\n::: {#79c15c51 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-5-output-1.png){width=574 height=441}\n:::\n:::\n\n\nThe parameter updating rule is shown below. The $\\eta$ is called the **learning rate**. It is a hyperparameter that is used to control the learning process. \n\n::: {.callout-note collapse=\"true\"}\n# Learning rate $\\eta$\nGenerally speaking, larger $\\eta$ will move faster to the global minimal, but might be jumpy which cause it harder to converge. On the other side, smaller $\\eta$ moves in a more stable fashion, but may take a long time to converge. See the following examples.\n\n::: {#75c3f9c8 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-6-output-1.png){width=577 height=436}\n:::\n:::\n\n\nIn the first example, $\\eta$ is too small, that after 200 iterations it is not very close to the minimal. In the second example, $\\eta$ becomes large. Although it gets to somewhere near the minimal, the path is very jumpy. It is able to converge only because the problem is indeed an easy one.\n:::\n\nWe may record the curve of the cost function.\n\n::: {#5a853bb1 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nAfter 200 iterations, the parameters are (2.291241352364798, 1.203587494484257).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-7-output-2.png){width=593 height=436}\n:::\n:::\n\n\nThe cost is close to $0$ after 200 iterations and seems to be convergent. Therefore we believe that we are close to the minimal point. The point we get is (2.291241352364798, 1.203587494484257).\n\n\n\n\n\n### Summary\n\nLet us summarize the example above and generalize it to the general case. \n\n1. Let $\\{(X_i, y_i)\\}$ be a given dataset. Assume that $y=f_{\\Theta}(X)$ where $\\Theta$ is the set of all parameters. \n2. The cost function $J_{\\Theta, \\{(X_i, y_i)\\}}$ is defined. \n3. To find the minimal point of the cost function, the gradient descent is applied: \n   - Start from a random initial point $\\theta_0$.\n   - Compute the gradient $\\nabla J$ and update $\\theta_i=\\theta_{i-1}- \\eta \\nabla J$ and repeat the process multiple times.\n   - Draw the learning curve and determine when to stop. Then we get the estimated best parameters $\\hat{\\Theta}$.\n4. Our model under this setting is sovled. We then turn to evaluation phase.\n\n\n::: {.callout-note}\nThe above process can be further developped. We will discuss many of them in details later.\n\n1. The cost function is related to each concerte problem.\n2. To compute the gradient of the cost function, chain rule is usually used. In the setting of MLP which we will discuss later, the gradient computations with chain rule are summarized as the so-called **Back propagation**. \n3. We go through the data points to compute the graident. How many points do we use? What is the frenqucy to update the gradient? This belongs to the topic of **mini-batch**.\n4. Even when we know that the graident gives the best direction, sometimes we don't really want to go in that direction, but make some modifications for some reason. To modify the direction, as well as choosing the learning rate $\\eta$, is the subject of **optimizers**.\n:::\n\n\n## Linear regression (`numpy`)\n\n\n\n\n::: {.hidden}\n<!-- Constants and basic symbols -->\n\n$$\n\\require{physics}\n\\require{braket}\n$$\n\n$$\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n$$\n\n<!-- Probability -->\n\n$$\n \\newcommand{\\Exp}{\\operatorname{E}}\n \\newcommand{\\Var}{\\operatorname{Var}}\n \\newcommand{\\Mode}{\\operatorname{mode}}\n$$\n\n<!-- Distributions pdf -->\n\n$$\n \\newcommand{\\pdfbinom}{{\\tt binom}}\n \\newcommand{\\pdfbeta}{{\\tt beta}}\n \\newcommand{\\pdfpois}{{\\tt poisson}}\n \\newcommand{\\pdfgamma}{{\\tt gamma}}\n \\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n$$\n\n<!-- Distributions -->\n\n$$\n \\newcommand{\\distbinom}{\\operatorname{B}}\n \\newcommand{\\distbeta}{\\operatorname{Beta}}\n \\newcommand{\\distgamma}{\\operatorname{Gamma}}\n \\newcommand{\\distexp}{\\operatorname{Exp}}\n \\newcommand{\\distpois}{\\operatorname{Poisson}}\n \\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n$$\n:::\n\n\n\n\nWe will translate everything from the previous sections into codes.\n\n### Prepare the dataset\nWe first randomly generate a dataset `(X, y)` for the linear regression problem. \n\n::: {#dc1f1fde .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\n\nRANDOMSEED = 42\nnp.random.seed(RANDOMSEED)\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\n```\n:::\n\n\nWe set the seed to be 42 for reproducing the results. We will also split the dataset into training and test sets.\n\n::: {#8e32a623 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=RANDOMSEED)\n```\n:::\n\n\nWe will only focus only on the training set in this Chapter. \n\n\n### Compute gradient\nThe cost function is \n\n$$\nJ(b,w)=\\frac1N\\sum_{i=1}^N(y_i-b-wx_i)^2.\n$$\n\nSo the graident is \n$$\n\\begin{aligned}\n\\pdv{J}{b}&=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\\\\n\\pdv{J}{w}&=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-x_i).\n\\end{aligned}\n$$\n\n::: {#cd9d4fa4 .cell execution_count=9}\n``` {.python .cell-code}\ndef J(parameters, X, y):    \n    b = parameters[0]\n    w = parameters[1]\n    return ((y-b-w*X)**2).mean().item()\n\ndef dJ(parameters, X, y):\n    b = parameters[0]\n    w = parameters[1]\n    db = (2*(y-b-w*X)*(-1)).mean()\n    dw = (2*(y-b-w*X)*(-X)).mean()\n    return np.array([db, dw])\n```\n:::\n\n\n### Gradient descent {#sec-gradeientdescent_numpy_example}\n\nIn general we need to random select a starting point. Here for the purpose of comparing to what we get from previous section, we will use a manual selected starting point $(1, 1.5)$. We then follow the path and move for a few steps. Here we will use $\\eta=0.2$ as the learning rate. \n\n::: {#5820c22e .cell execution_count=10}\n``` {.python .cell-code}\np = np.array([1.0, 1.5])\neta = 0.2\n\nplist = []\nJlist = []\ndJlist = []\nfor _ in range(10):\n    J_i = J(p, X_train, y_train)\n    dJ_i = dJ(p, X_train, y_train)\n    plist.append(p)\n    Jlist.append(J_i)\n    dJlist.append(dJ_i)\n\n    p = p - eta * dJ_i\nplist = np.array(plist)\nJlist = np.array(Jlist)\ndJlist = np.array(dJlist)\nplist\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[1.        , 1.5       ],\n       [1.46212898, 1.70260448],\n       [1.70179135, 1.79494492],\n       [1.8284451 , 1.8316393 ],\n       [1.89762476, 1.84038829],\n       [1.93750825, 1.83523704],\n       [1.96239471, 1.8233032 ],\n       [1.97954219, 1.80819012],\n       [1.99263653, 1.79171854],\n       [2.00355121, 1.77480499]])\n```\n:::\n:::\n\n\n::: {#3e3a6fc1 .cell execution_count=11}\n``` {.python .cell-code}\nJlist\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([1.35302136, 0.39283316, 0.14356485, 0.07738847, 0.0584474 ,\n       0.05176925, 0.04834712, 0.04585734, 0.04369662, 0.04170475])\n```\n:::\n:::\n\n\n::: {#aadc821f .cell execution_count=12}\n``` {.python .cell-code}\ndJlist\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\narray([[-2.31064488, -1.01302241],\n       [-1.19831188, -0.4617022 ],\n       [-0.63326872, -0.1834719 ],\n       [-0.34589833, -0.04374494],\n       [-0.19941743,  0.02575624],\n       [-0.12443229,  0.05966924],\n       [-0.08573741,  0.07556538],\n       [-0.06547171,  0.08235793],\n       [-0.05457338,  0.08456773],\n       [-0.04844465,  0.08448733]])\n```\n:::\n:::\n\n\nYou may compare the answer with the `PyTorch` implementation in @sec-gradientdescent_pytorch_example.\n\n### Mini-batch\n\n\n## Linear regression (`PyTorch`)\n\n### Construct `torch.Tensor`\nThere are multiple ways to construct a tensor. I just discuss those confusing ones. \n\n- `torch.Tensor` is the `PyTorch` tensor data structure. Itself serves as the constructor of the class, therefore you may use `torch.Tensor(data)` to construct a tensor. This is relative basic, and will have a default `float` type.\n- `torch.tensor` is the recommendated function to construct a tensor from data. It has two benefits over `torch.Tensor`: it will automatically induce the datatype from data instead of always using `float`; and it is easier to change datatype with the argument `dtype`.\n- `torch.as_tensor` is a function to construct a tensor from data. If the original data is numpy array, this tensor shares data with it. This means that if one is changed, the other is changed as well.\n\n::: {#1dc97aa7 .cell execution_count=13}\n``` {.python .cell-code}\nimport numpy as np\nimport torch\n\nexample = np.array([1, 2])\nexample_tensor0 = torch.Tensor(example)\nexample_tensor1 = torch.tensor(example)\nexample_tensor2 = torch.as_tensor(example)\n\nprint(f'Tensor: dtype: {example_tensor0.dtype}, tensor: dtype: {example_tensor1.dtype}')\n\nprint(f'tensor: {example_tensor1}, as_tensor: {example_tensor2}, original: {example}')\n\nexample[0] = 0\nprint(f'tensor: {example_tensor1}, as_tensor: {example_tensor2}, original: {example}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensor: dtype: torch.float32, tensor: dtype: torch.int32\ntensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([1, 2], dtype=torch.int32), original: [1 2]\ntensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([0, 2], dtype=torch.int32), original: [0 2]\n```\n:::\n:::\n\n\nIn general, it is recommended to use `torch.as_tensor` over `torch.tensor` (since for large data to create a view is much faster than to create a copy) and to use `torch.tensor` over `torch.Tensor` (due to the benefits mentioned above).\n\n\n::: {.callout-note}\n# datatype  \nThe datatype in `PyTorch` is very strict. Many functions can work with only some of the datatypes. In most cases `float` and `double` are used. Other types may or may not be supported by a specific function. \n\nHowever, there are a lot of ways to play with types. For example, you may use `torch.tensor([1], dtype=torch.double)` to directly construct a `double` tensor, or use `torch.tensor([1]).double()` to first construct an `int` tensor and then cast it into a `double` tensor.\n:::\n\n\n::: {.callout-note}\n# Scalar\nA tensor with only one element is still a tensor in `PyTorch`. To use it as a scalar, you need to use `itme()` method.\n\n::: {#cf4174b8 .cell execution_count=14}\n``` {.python .cell-code}\na = torch.tensor(1)\na\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\ntensor(1)\n```\n:::\n:::\n\n\n::: {#1eda624b .cell execution_count=15}\n``` {.python .cell-code}\na.item()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n1\n```\n:::\n:::\n\n\nNote that for `numpy`, before 2.0 version an array with one element is considered as scalar. However after 2.0, it behaves very similar to `PyTorch`.\n:::\n\nWe now construct a `PyTorch` tensor version of the dataset we used in previous sections. The `device` part will be introduced in the next section.\n\n::: {#04d7c59f .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\n\nRANDOMSEED = 42\nnp.random.seed(RANDOMSEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=RANDOMSEED)\nX_tensor_train = torch.as_tensor(X_train, device=device)\ny_tensor_train = torch.as_tensor(y_train, device=device)\n```\n:::\n\n\n### devices\nWe coulde use `torch.cuda.is_available()` to check whether we have GPU/CUDA supported devices. If the answer is no, we don't need to change any codes and everything works fine but slow.\n\nIf we have GPU/CUDA supported devices, we could send our tensors to them and do computations there. Google Colab is a good place to play with it if we don't have our own hardware.\n\nIn most cases we use `to(device)` method to send a tensor to a device. Sometimes some function has `device=device` argument to automatically construct tensors in a device. Note that if one needs to compute the gradient of a tensor and send the tensor to a device, we need to manually set `requires_grad_(True)`. \n\nHere are some examples, although they only makes sense in a GPU environment.\n\n::: {#c6837780 .cell execution_count=17}\n``` {.python .cell-code}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nt1 = torch.tensor(1, dtype=float, device=device)\nt2 = torch.tensor(1, dtype=float)\nprint(f't1: {t1.type()}, t2: {t2.type()}')\n```\n:::\n\n\nIf you can see `cuda` in the output of `type`, it is a GPU tensor. Otherwise it is a CPU tensor. We may use `to` to convert a CPU tensor to be a GPU tensor. If this tensor requires gradient, we should set it manually.\n\n::: {#ab866254 .cell execution_count=18}\n``` {.python .cell-code}\nt3 = t2.to(device)\nt3 = t3.requires_grad_(True)\nt3\n```\n:::\n\n\nIt is usually recommended to write codes with `device` in mind like above, since the codes work for both CPU and GPU machines.\n\n### Gradient {#sec-gradientdescent_pytorch_example}\n`PyTorch` can use `autograd` to automatically compute the gradient of given formula. All computations are done within the context of tensors. The biggest difference between `PyTorch` tensor and `numpy` array is that `PyTorch` tensor carries gradient infomation on its own. \n\nThe step is very easy: first use `PyTorch` tensor to write a formula, enable gradients on correct tensors, and then use the `backward()` method.\n\n::: {#9fe37dd5 .cell execution_count=19}\n``` {.python .cell-code}\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\n\nloss = ((y_tensor_train - b - w * X_tensor_train)**2).mean()\nloss.backward()\nprint(f'db: {b.grad}, dw: {w.grad}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndb: -2.310644882519191, dw: -1.0130224137389\n```\n:::\n:::\n\n\nWe could manually compute the first few iterations and record the results. You may compare it with the `numpy` implementation in @sec-gradeientdescent_numpy_example. The answer is exactly the same.\n\n::: {#540a138f .cell execution_count=20}\n``` {.python .cell-code}\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\neta = 0.2\ndJlist = []\nplist = []\nfor _ in range(10):\n    loss = ((y_tensor_train - b - w * X_tensor_train)**2).mean()\n    loss.backward()\n    dJlist.append([b.grad.item(), w.grad.item()])\n    plist.append([b.item(), w.item()])\n    with torch.no_grad():\n        b -= eta * b.grad\n        w -= eta * w.grad\n    b.grad.zero_()\n    w.grad.zero_()\n    \nplist\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n[[1.0, 1.5],\n [1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945]]\n```\n:::\n:::\n\n\n::: {#26b79efd .cell execution_count=21}\n``` {.python .cell-code}\ndJlist\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n[[-2.310644882519191, -1.0130224137389],\n [-1.1983118800115005, -0.46170219928817446],\n [-0.6332687245365407, -0.18347189802882874],\n [-0.3458983296611412, -0.043744944608227095],\n [-0.19941742762115208, 0.025756235421503952],\n [-0.12443229080077543, 0.05966923641460931],\n [-0.08573740635824567, 0.07556538094598902],\n [-0.06547171164669269, 0.08235792644570186],\n [-0.05457338076787398, 0.08456773288435376],\n [-0.04844464903201605, 0.08448732917010579]]\n```\n:::\n:::\n\n\n## Rewrite in classes\n\ns\n\n\n## Exercises\n\n\n\n::: {exr-}\nTry to reconstruct all the plots in Section @sec-linearregression_math.\n:::\n\n",
    "supporting": [
      "intro_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}