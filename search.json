[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Fall 2024",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 4803/5803 Machine Learning Fall 2024 at ATU. If you have any comments/suggetions/concers about the notes please contact us at xxiao@atu.edu.\n\n\nReferences\n\n\n[1] Chollet, F.\n(2021). Deep\nlearning with python, second edition. MANNING PUBN.\n\n\n[2] Géron, A.\n(2019). Hands-on machine learning with scikit-learn, keras, and\nTensorFlow concepts, tools, and techniques to build intelligent systems:\nConcepts, tools, and techniques to build intelligent systems.\nO’Reilly Media.\n\n\n[3] Harrington, P.\n(2012). Machine\nlearning in action. Manning Publications.\n\n\n[4] Klosterman, S.\n(2021). Data\nscience projects with python: A case study approach to gaining valuable\ninsights from real data with machine learning. Packt\nPublishing, Limited.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/1/intro.html",
    "href": "contents/1/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Machine Learning?\nMachine Learning is the science (and art) of programming computers so they can learn from data [1].\nHere is a slightly more general definition:\nThis “without being explicitly programmed to do so” is the essential difference between Machine Learning and usual computing tasks. The usual way to make a computer do useful work is to have a human programmer write down rules — a computer program — to be followed to turn input data into appropriate answers. Machine Learning turns this around: the machine looks at the input data and the expected task outcome, and figures out what the rules should be. A Machine Learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task [2].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#what-is-machine-learning",
    "href": "contents/1/intro.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "",
    "text": "[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n                                                   -- Arthur Samuel, 1959",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#types-of-machine-learning-systems",
    "href": "contents/1/intro.html#types-of-machine-learning-systems",
    "title": "1  Introduction",
    "section": "1.2 Types of Machine Learning Systems",
    "text": "1.2 Types of Machine Learning Systems\nThere are many different types of Machine Learning systems that it is useful to classify them in braod categories, based on different criteria. These criteria are not exclusive, and you can combine them in any way you like.\nThe most popular criterion for Machine Learning classification is the amount and type of supervision they get during training. In this case there are four major types.\nSupervised Learning The training set you feed to the algorithm includes the desired solutions. The machines learn from the data to alter the model to get the desired output. The main task for Supervised Learning is classification and regression.\nUnsupervised Learning In Unsupervised Learning, the data provided doesn’t have class information or desired solutions. We just want to dig some information directly from those data themselves. Usually Unsupervised Learning is used for clustering and dimension reduction.\nReinforcement Learning In Reinforcement Learning, there is a reward system to measure how well the machine performs the task, and the machine is learning to find the strategy to maximize the rewards. Typical examples here include gaming AI and walking robots.\nSemisupervised Learning This is actually a combination of Supervised Learning and Unsupervised Learning, that it is usually used to deal with data that are half labelled.\n\n1.2.1 Tasks for Supervised Learning\nAs mentioned above, for Supervised Learning, there are two typical types of tasks:\nClassification It is the task of predicting a discrete class labels. A typical classification problem is to see an handwritten digit image and recognize it.\nRegression It is the task of predicting a continuous quantity. A typical regression problem is to predict the house price based on various features of the house.\nThere are a lot of other tasks that are not directly covered by these two, but these two are the most classical Supervised Learning tasks.\n\n\n\n\n\n\nNote\n\n\n\nIn this course we will mainly focus on Supervised Classification problems.\n\n\n\n\n1.2.2 Classification based on complexity\nAlong with the popularity boost of deep neural network, there comes another classificaiton: shallow learning vs. deep learning. Basically all but deep neural network belongs to shallow learning. Although deep learning can do a lot of fancy stuffs, shallow learning is still very good in many cases. When the performance of a shallow learning model is good enough comparing to that of a deep learning model, people tend to use the shallow learning since it is usually faster, easier to understand and easier to modify.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "href": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "title": "1  Introduction",
    "section": "1.3 Basic setting for Machine learning problems",
    "text": "1.3 Basic setting for Machine learning problems\n\n\n\n\n\n\nNote\n\n\n\nWe by default assume that we are dealing with a Supervised Classification problem.\n\n\n\n1.3.1 Input and output data structure\nSince we are dealing with Supervised Classification problems, the desired solutions are given. These desired solutions in Classification problems are also called labels. The properties that the data are used to describe are called features. Both features and labels are usually organized as row vectors.\n\nExample 1.1 The example is extracted from [3]. There are some sample data shown in the following table. We would like to use these information to classify bird species.\n\n\n\n\nTable 1.1: Bird species classification based on four features\n\n\n\n\n\n\n\n\nWeight (g)\nWingspan (cm)\nWebbed feet?\nBack color\nSpecies\n\n\n\n\n1000.100000\n125.000000\nNo\nBrown\nButeo jamaicensis\n\n\n3000.700000\n200.000000\nNo\nGray\nSagittarius serpentarius\n\n\n3300.000000\n220.300000\nNo\nGray\nSagittarius serpentarius\n\n\n4100.000000\n136.000000\nYes\nBlack\nGavia immer\n\n\n3.000000\n11.000000\nNo\nGreen\nCalothorax lucifer\n\n\n570.000000\n75.000000\nNo\nBlack\nCampephilus principalis\n\n\n\n\n\n\n\n\nThe first four columns are features, and the last column is the label. The first two features are numeric and can take on decimal values. The third feature is binary that can only be \\(1\\) (Yes) or \\(0\\) (No). The fourth feature is an enumeration over the color palette. You may either treat it as categorical data or numeric data, depending on how you want to build the model and what you want to get out of the data. In this example we will use it as categorical data that we only choose it from a list of colors (\\(1\\) — Brown, \\(2\\) — Gray, \\(3\\) — Black, \\(4\\) — Green).\nThen we are able to transform the above data into the following form:\n\n\n\nTable 1.2: Vectorized Bird species data\n\n\n\n\n\nFeatures\nLabels\n\n\n\n\n\\(\\begin{bmatrix}1001.1 & 125.0 & 0 & 1 \\end{bmatrix}\\)\n\\(1\\)\n\n\n\\(\\begin{bmatrix}3000.7 & 200.0 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}3300.0 & 220.3 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}4100.0 & 136.0 & 1 & 3 \\end{bmatrix}\\)\n\\(3\\)\n\n\n\\(\\begin{bmatrix}3.0 & 11.0 & 0 & 4 \\end{bmatrix}\\)\n\\(4\\)\n\n\n\\(\\begin{bmatrix}570.0 & 75.0 & 0 & 3 \\end{bmatrix}\\)\n\\(5\\)\n\n\n\n\n\n\nThen the Supervised Learning problem is stated as follows: Given the features and the labels, we would like to find a model that can classify future data.\n\n\n\n1.3.2 Parameters and hyperparameters\nA model parameter is internal to the model and its value is learned from the data.\nA model hyperparameter is external to the model and its value is set by people.\nFor example, assume that we would like to use Logistic regression to fit the data. We set the learning rate is 0.1 and the maximal iteration is 100. After the computations are done, we get a the model\n\\[\ny = \\sigma(0.8+0.7x).\n\\] The two cofficients \\(0.8\\) and \\(0.7\\) are the parameters of the model. The model Logistic regression, the learning rate 0.1 and the maximal iteration 100 are all hyperparametrs. If we change to a different set of hyperparameters, we may get a different model, with a different set of parameters.\nThe details of Logistic regression will be discussed later.\n\n\n1.3.3 Evaluate a Machine Learning model\nOnce the model is built, how do we know that it is good or not? The naive idea is to test the model on some brand new data and check whether it is able to get the desired results. The usual way to achieve it is to split the input dataset into three pieces: training set, validation set and test set.\nThe model is initially fit on the training set, with some arbitrary selections of hyperparameters. Then hyperparameters will be changed, and new model is fitted over the training set. Which set of hyperparameters is better? We then test their performance over the validation set. We could run through a lot of different combinations of hyperparameters, and find the best performance over the validation set. After we get the best hyperparameters, the model is selcted, and we fit it over the training set to get our model to use.\nTo compare our model with our models, either our own model using other algorithms, or models built by others, we need some new data. We can no longer use the training set and the validation set since all data in them are used, either for training or for hyperparameters tuning. We need to use the test set to evaluate the “real performance” of our data.\nTo summarize:\n\nTraining set: used to fit the model;\nValidation set: used to tune the hyperparameters;\nTest set: used to check the overall performance of the model.\n\nThe validation set is not always required. If we use cross-validation technique for hyperparameters tuning, like sklearn.model_selection.GridSearchCV(), we don’t need a separated validation set. In this case, we will only need the training set and the test set, and run GridSearchCV over the training set. The cross-validation will be discussed in {numref}Section %s&lt;section-cross-validation&gt;.\nThe sizes and strategies for dataset division depends on the problem and data available. It is often recommanded that more training data should be used. The typical distribution of training, validation and test is \\((6:3:1)\\), \\((7:2:1)\\) or \\((8:1:1)\\). Sometimes validation set is discarded and only training set and test set are used. In this case the distribution of training and test set is usually \\((7:3)\\), \\((8:2)\\) or \\((9:1)\\).\n\n\n1.3.4 Workflow in developing a machine learning application\nThe workflow described below is from [3].\n\nCollect data.\nPrepare the input data.\nAnalyze the input data.\nTrain the algorithm.\nTest the algorithm.\nUse it.\n\nIn this course, we will mainly focus on Step 4 as well Step 5. These two steps are where the “core” algorithms lie, depending on the algorithm. We will start from the next Chapter to talk about various Machine Learning algorithms and examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#python-quick-guide",
    "href": "contents/1/intro.html#python-quick-guide",
    "title": "1  Introduction",
    "section": "1.4 Python quick guide",
    "text": "1.4 Python quick guide\n\n1.4.1 Python Notebook\nWe mainly use Python Notebook (.ipynb) to write documents for this course. Currently all main stream Python IDE support Python Notebook. All of them are not entirely identical but the differences are not huge and you may choose any you like.\nOne of the easiest ways to use Python Notebook is through JupyterLab. The best part about it is that you don’t need to worry about installation and configuration in the first place, and you can directly start to code.\nClick the above link and choose JupyterLab. Then you will see the following page.\n\nThe webapp you just started is called JupyterLite. This is a demo version. The full JupyterLab installation instruction can also be found from the link.\nThere is a small button + under the tab bar. This is the place where you click to start a new cell. You may type codes or markdown documents or raw texts in the cell according to your needs. The drag-down menu at the end of the row which is named Code or Markdown or Raw can help you make the switch. Markdown is a very simple light wighted language to write documents. In most cases it behaves very similar to plain texts. Codes are just regular Python codes (while some other languages are supported). You may either use the triangle button in the menu to execute the codes, or hit shift + enter.\n\nJupyterLite contains a few popular packages. Therefore it is totally ok if you would like to play with some simple things. However since it is an online evironment, it has many limitations. Therefore it is still recommended to set up a local environment once you get familiar with Python Notebook. Please check the following links for some popular choices for notebooks and Python installations in general, either local and online.\n\nJupyter Notebook / JupyterLab\nVS Code\nPyCharm\nGoogle Colab\nAnaconda\n\n\n\n1.4.2 Python fundamentals\nWe will put some very basic Python commands here for you to warm up. More advanced Python knowledge will be covered during the rest of the semester. The main reference for this part is [3]. Another referenece is My notes.\n\n1.4.2.1 Indentation\nPython is using indentation to denote code blocks. It is not convienent to write in the first place, but it forces you to write clean, readable code.\nBy the way, the if and for block are actually straightforward.\nif jj &lt; 3:\n    jj = jj \n    print(\"It is smaller than 3.\")\nif jj &lt; 3:\n    jj = jj\nprint(\"It is smaller than 3.\")\nfor i in range(3):\n    i = i + 1\n    print(i)\nfor i in range(3):\n    i = i + 1\nprint(i)\n\n\n\n\n\n\n\n\nPlease tell the differences between the above codes.\n\n\n1.4.2.2 list and dict\nHere are some very basic usage of lists of dictionaries in Python.\n\nnewlist = list()\nnewlist.append(1)\nnewlist.append('hello')\nnewlist\n\n[1, 'hello']\n\n\n\nnewlisttwo = [1, 'hello']\nnewlisttwo\n\n[1, 'hello']\n\n\n\nnewdict = dict()\nnewdict['one'] = 'good'\nnewdict[1] = 'yes'\nnewdict\n\n{'one': 'good', 1: 'yes'}\n\n\n\nnewdicttwo = {'one': 'good', 1: 'yes'}\nnewdicttwo\n\n{'one': 'good', 1: 'yes'}\n\n\n\n\n1.4.2.3 Loop through lists\nWhen creating for loops we may let Python directly loop through lists. Here is an example. The code is almost self-explained.\n\nalist = ['one', 2, 'three', 4]\n\nfor item in alist:\n    print(item)\n\none\n2\nthree\n4\n\n\n\n\n1.4.2.4 Reading files\nThere are a lot of functions that can read files. The basic one is to read any files as a big string. After we get the string, we may parse it based on the structure of the data.\nThe above process sounds complicated. That’s why we have so many different functions reading files. Usually they focus on a certain types of files (e.g. spreadsheets, images, etc..), parse the data into a particular data structure for us to use later.\nI will mention a few examples.\n\ncsv files and excel files Both of them are spreadsheets format. Usually we use pandas.read_csv and pandas.read_excel both of which are from the package pandas to read these two types of files.\nimages Images can be treated as matrices, that each entry represents one pixel. If the image is black/white, it is represented by one matrix where each entry represents the gray value. If the image is colored, it is represented by three matrices where each entry represents one color. To use which three colors depends on the color map. rgb is a popular choice.\nIn this course when we need to read images, we usually use matplotlib.pyplot.imread from the package matplotlib or cv.imread from the package opencv.\n.json files .json is a file format to store dictionary type of data. To read a json file and parse it as a dictionary, we need json.load from the package json.\n\n\n\n1.4.2.5 Writing files\n\npandas.DataFrame.to_csv\npandas.DataFrame.to_excel\nmatplotlib.pyplot.imsave\ncv.imwrite\njson.dump\n\n\n\n1.4.2.6 Relative paths\nIn this course, when reading and writing files, please keep all the files using relative paths. That is, only write the path starting from the working directory.\n\nExample 1.2 Consider the following tasks:\n\nYour working directory is C:/Users/Xinli/projects/.\nWant to read a file D:/Files/example.csv.\nWant to generate a file whose name is result.csv and put it in a subfoler named foldername.\n\nTo do the tasks, don’t directly run the code pd.read_csv('D:/Files/example.csv'). Instead you should first copy the file to your working directory C:/Users/Xinli/projects/, and then run the following code.\n\nimport pandas as pd\n\ndf = pd.read_csv('example.csv')\ndf.to_csv('foldername/result.csv')\n\nPlease pay attention to how the paths are written.\n\n\n\n1.4.2.7 .\n\nclass and packages.\nGet access to attributes and methods\nChaining dots.\n\n\n\n\n1.4.3 Some additional topics\nYou may read about these parts from the appendices of My notes.\n\n1.4.3.1 Package management and Virtual environment\n\nconda\n\nconda create\n\nconda create --name myenv\nconda create --name myenv python=3.9\nconda create --name myenv --file spec-file.txt\n\nconda install\n\nconda install -c conda-forge numpy\n\nconda activate myenv\nconda list\n\nconda list numpy\nconda list --explicit &gt; spec-file.txt\n\nconda env list\n\npip / venv\n\npython -m venv newenv\nnewenv\\Scripts\\activate\npip install\npip freeze &gt; requirements.txt\npip install -r /path/to/requirements.txt\ndeactivate\n\n\n\n\n1.4.3.2 Version Control\n\nGit\n\nInstall\ngit config --list\ngit config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n\nGitHub",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#exercises",
    "href": "contents/1/intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nThese exercises are from [4], [1] and [3].\n\n1.5.1 Python Notebook\n\nExercise 1.1 (Hello World!) Please set up a Python Notebook environment and type print('Hello World!').\n\n\nExercise 1.2 Please set up a Python Notebook and start a new virtual environment and type print('Hello World!').\n\n\n\n1.5.2 Basic Python\n\nExercise 1.3 (Play with lists) Please complete the following tasks.\n\nWrite a for loop to print values from 0 to 4.\nCombine two lists ['apple', 'orange'] and ['banana'] using +.\nSort the list ['apple', 'orange', 'banana'] using sorted().\n\n\n\n\nExercise 1.4 (Play with list, dict and pandas.) Please complete the following tasks.\n\nCreate a new dictionary people with two keys name and age. The values are all empty list.\nAdd Tony to the name list in people.\nAdd Harry to the name list in people.\nAdd number 100 to the age list in people.\nAdd number 10 to the age list in people.\nFind all the keys of people and save them into a list namelist.\nConvert the dictionary people to a Pandas DataFrame df.\n\n\n\n\nExercise 1.5 (The dataset iris)  \n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nPlease explore this dataset.\n\nPlease get the features for iris and save it into X as an numpy array.\nWhat is the meaning of these features?\nPlease get the labels for iris and save it into y as an numpy array.\nWhat is the meaning of labels?\n\n\n\n\nExercise 1.6 (Play with Pandas) Please download the Titanic data file from here. Then follow the instructions to perform the required tasks.\n\nUse pandas.read_csv to read the dataset and save it as a dataframe object df.\nChange the values of the Sex column that male is 0 and female is 1.\nPick the columns Pclass, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare and transform them into a 2-dimensional numpy.ndarray, and save it as X.\nPick the column Survived and transform it into a 1-dimensional numpy.ndarray and save it as y.\n\n\n\n\n\n\n\n[1] Géron, A. (2019). Hands-on machine learning with scikit-learn, keras, and TensorFlow concepts, tools, and techniques to build intelligent systems: Concepts, tools, and techniques to build intelligent systems. O’Reilly Media.\n\n\n[2] Chollet, F. (2021). Deep learning with python, second edition. MANNING PUBN.\n\n\n[3] Harrington, P. (2012). Machine learning in action. Manning Publications.\n\n\n[4] Klosterman, S. (2021). Data science projects with python: A case study approach to gaining valuable insights from real data with machine learning. Packt Publishing, Limited.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html",
    "href": "contents/2/intro.html",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1 k-Nearest Neighbors Algorithm (k-NN)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "href": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1.1 Ideas\nAssume that we have a set of labeled data \\(\\{(X_i, y_i)\\}\\) where \\(y_i\\) denotes the label. Given a new data \\(X\\), how do we determine the label of it?\nk-NN algorithm starts from a very straightforward idea. We use the distances from the new data point \\(X\\) to the known data points to identify the label. If \\(X\\) is closer to \\(y_i\\) points, then we will label \\(X\\) as \\(y_i\\).\nLet us take cities and countries as an example. New York and Los Angeles are U.S cities, and Beijing and Shanghai are Chinese cities. Now we would like to consider Tianjin and Russellville. Do they belong to China or U.S? We calculate the distances from Tianjin (resp. Russellville) to all four known cities. Since Tianjin is closer to Beijing and Shanghai comparing to New York and Los Angeles, we classify Tianjin as a Chinese city. Similarly, since Russellville is closer to New York and Los Angeles comparing to Beijing and Shanghai, we classify it as a U.S. city.\n\n\n\n\n\n\n\nG\n\n\n\nBeijing\n\nBeijing\n\n\n\nShanghai\n\nShanghai\n\n\n\nTianjin\n\nTianjin\n\n\n\nTianjin-&gt;Beijing\n\n\ncloser\n\n\n\nTianjin-&gt;Shanghai\n\n\ncloser   \n\n\n\nNew York\n\nNew York\n\n\n\nTianjin-&gt;New York\n\n\n far away\n\n\n\nLos Angelis\n\nLos Angelis\n\n\n\nTianjin-&gt;Los Angelis\n\n\n far away\n\n\n\nRussellville\n\nRussellville\n\n\n\nRussellville-&gt;Beijing\n\n\nfar away \n\n\n\nRussellville-&gt;Shanghai\n\n\nfar away\n\n\n\nRussellville-&gt;New York\n\n\ncloser  \n\n\n\nRussellville-&gt;Los Angelis\n\n\ncloser\n\n\n\n\n\n\n\n\nThis naive example explains the idea of k-NN. Here is a more detailed description of the algorithm.\n\n\n2.1.2 The Algorithm\n\n\n\n\n\n\nk-NN Classifier\n\n\n\nInputs: Given the training data set \\(\\{(X_i, y_i)\\}\\) where \\(X_i=(x_i^1,x_i^2,\\ldots,x_i^n)\\) represents \\(n\\) features and \\(y_i\\) represents labels. Given a new data point \\(\\tilde{X}=(\\tilde{x}^1,\\tilde{x}^2,\\ldots,\\tilde{x}^n)\\).\nOutputs: Want to find the best label for \\(\\tilde{X}\\).\n\nCompute the distance from \\(\\tilde{X}\\) to each \\(X_i\\).\nSort all these distances from the nearest to the furthest.\nFind the nearest \\(k\\) data points.\nDetermine the labels for each of these \\(k\\) nearest points, and compute the frenqucy of each labels.\nThe most frequent label is considered to be the label of \\(\\tilde{X}\\).\n\n\n\n\n\n2.1.3 Details\n\nThe distance between two data points are defined by the Euclidean distance:\n\n\\[\ndist\\left((x^j_i)_{j=1}^n, (\\tilde{x}^j)_{j=1}^n\\right)=\\sqrt{\\sum_{j=1}^n(x^j_i-\\tilde{x}^j)^2}.\n\\]\n\nUsing linear algebra notations:\n\n\\[\ndist(X_i,\\tilde{X})=\\sqrt{(X_i-\\tilde{X})\\cdot(X_i-\\tilde{X})}.\n\\]\n\nAll the distances are stored in a \\(1\\)-dim numpy array, and we will combine it together with another \\(1\\)-dim array that store the labels of each point.\n\n\n\n2.1.4 The codes\n\n\nThis part is optional.\n\n\nargsort\nunique\nargmax\n\n\nimport numpy as np\n\ndef classify_kNN(inX, X, y, k=5):\n    # compute the distance between each row of X and Xmat\n    Dmat = np.sqrt(((inX - X)**2).sum(axis=1))\n    # sort by distance\n    k = min(k, Dmat.shape[0])\n    argsorted = Dmat.argsort()[:k]\n    relatedy = y[argsorted]\n    # count the freq. of the first k labels\n    labelcounts = np.unique(relatedy, return_counts=True)\n    # find the label with the most counts\n    label = labelcounts[0][labelcounts[1].argmax()]\n    return label\n\n\n\n\n2.1.5 sklearn packages\nYou may also directly use the kNN function KNeighborsClassifier packaged in sklearn.neighbors. You may check the description of the function online from here.\nThere are many ways to modify the kNN algorithm. What we just mentioned is the simplest idea. It is correspondent to the argument weights='uniform', algorithm='brute and metric='euclidean'. However due to the implementation details, the results we got from sklearn are still a little bit different from the results produced by our naive codes.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=10, weights='uniform',\n                           algorithm='brute', metric='euclidean')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\n2.1.6 Normalization\nDifferent features may have different scales. It might be unfair for those features that have small scales. Therefore usually it is better to rescale all the features to make them have similar scales. After examining all the data, we find the minimal value minVal and the range ranges for each column. The normalization formula is:\n\\[\nX_{norm} = \\frac{X_{original}-minVal}{ranges}.\n\\]\nWe could also convert the normalized number back to the original value by\n\\[\nX_{original} = X_{norm} \\times ranges + minVal.\n\\]\n\n\nThe sample codes are listed below. This part is optional.\n\n\nimport numpy as np\n\ndef encodeNorm(X, parameters=None):\n    # parameters contains minVals and ranges\n    if parameters is None:\n        minVals = np.min(X, axis=0)\n        maxVals = np.max(X, axis=0)\n        ranges = np.maximum(maxVals - minVals, np.ones(minVals.size))\n        parameters = {'ranges': ranges, 'minVals': minVals}\n    else:\n        minVals = parameters['minVals']\n        ranges = parameters['ranges']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xnorm = (X - Nmat)/ranges\n    return (Xnorm, parameters)\n\n\ndef decodeNorm(X, parameters):\n    # parameters contains minVals and ranges\n    ranges = parameters['ranges']\n    minVals = parameters['minVals']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xoriginal = X * ranges + Nmat\n    return Xoriginal\n\n\nYou could use MinMaxScaler from sklearn.preprocessing to achive the goal. The API is very similar to an estimator.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\nmm.fit(X)\nX_norm = mm.transform(X)\n\nThe last two lines can be combined into\n\nX_norm = mm.fit_transform(X)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "href": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.2 k-NN Project 1: iris Classification",
    "text": "2.2 k-NN Project 1: iris Classification\nThis data is from sklearn.datasets. This dataset consists of 3 different types of irises’ petal / sepal length / width, stored in a \\(150\\times4\\) numpy.ndarray. We already explored the dataset briefly in the previous chapter. This time we will try to use the feature provided to predict the type of the irises. For the purpose of plotting, we will only use the first two features: sepal length and sepal width.\n\n2.2.1 Explore the dataset\nWe first load the dataset.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nThen we would like to split the dataset into trainning data and test data. Here we are going to use sklearn.model_selection.train_test_split function. Besides the dataset, we should also provide the propotion of the test set comparing to the whole dataset. We will choose test_size=0.1 here, which means that the size of the test set is 0.1 times the size of the whole dataset. stratify=y means that when split the dataset we want to split respects the distribution of labels in y.\nThe split will be randomly. You may set the argument random_state to be a certain number to control the random process. If you set a random_state, the result of the random process will stay the same. This is for reproducible output across multiple function calls.\nAfter we get the training set, we should also normalize it. All our normalization should be based on the training set. When we want to use our model on some new data points, we will use the same normalization parameters to normalize the data points in interests right before we apply the model. Here since we mainly care about the test set, we could normalize the test set at this stage.\nNote that in the following code, we mainly use the implementation from sklearn.  \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n\nmm = MinMaxScaler()\nX_train_norm = mm.fit_transform(X_train)\nX_test_norm = mm.transform(X_test)\n\nBefore we start to play with k-NN, let us look at the data first. Since we only choose two features, it is able to plot these data points on a 2D plane, with different colors representing different classes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the scatter plot.\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n# Generate legends.\nlabels = ['setosa', 'versicolor', 'virginica']\n_ = fig.legend(handles=scatter.legend_elements()[0], labels=labels,\n               loc=\"right\", title=\"Labels\")\n\n\n\n\n\n\n\n\n\n\n2.2.2 Apply our k-NN model\nNow let us apply k-NN to this dataset. Since our data is prepared, what we need to do is directly call the functions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 10\nclf = KNeighborsClassifier(n_neighbors, weights=\"uniform\", metric=\"euclidean\",\n                           algorithm='brute')\nclf.fit(X_train_norm, y_train)\ny_pred_sk = clf.predict(X_test_norm)\n\nacc = np.mean(y_pred_sk == y_test)\nacc\n\n0.7333333333333333\n\n\n\n\n2.2.3 Using data pipeline\nWe may organize the above process in a neater way. After we get a data, the usual process is to apply several transforms to the data before we really get to the model part. Using terminolgies from sklearn, the former are called transforms, and the latter is called an estimator. In this example, we have exactly one tranform which is the normalization. The estimator here we use is the k-NN classifier.\nsklearn provides a standard way to write these codes, which is called pipeline. We may chain the transforms and estimators in a sequence and let the data go through the pipeline. In this example, the pipeline contains two steps: 1. The normalization transform sklearn.preprocessing.MinMaxScaler. 2. The k-NN classifier sklearn.neighbors.KNeighborsClassifier. This is the same one as we use previously.\nThe code is as follows. It is a straightforward code. Note that the () after the class in each step of steps is very important. The codes cannot run if you miss it.\nAfter we setup the pipeline, we may use it as other estimators since it is an estimator. Here we may also use the accuracy function provided by sklearn to perform the computation. It is essentially the same as our acc computation.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nn_neighbors = 10\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.7333333333333333\n\n\nOnce a pipeline is set, you may use step name with TWO underscores __ with parameter name to get access to a specific parameter. Please check the following code.\n\npipe.get_params()['knn__n_neighbors']\n\n10\n\n\n\npipe.set_params(knn__n_neighbors=5)\npipe.get_params()['knn__n_neighbors']\n\n5\n\n\n\n\n2.2.4 Visualize the Decision boundary\n\n\nThis section is optional.\n\nUsing the classifier we get above, we are able to classify every points on the plane. This enables us to draw the following plot, which is called the Decision boundary. It helps us to visualize the relations between features and the classes.\nWe use DecisionBoundaryDisplay from sklearn.inspection to plot the decision boundary. The function requires us to have a fitted classifier. We may use the classifier pipe we got above. Note that this classifier should have some build-in structures that our classify_kNN function doesn’t have. We may rewrite our codes to make it work, but this goes out of the scope of this section. This is supposed to be Python programming exercise. We will talk about it in the future if we have enough time.\nWe first plot the dicision boundary using DecisionBoundaryDisplay.from_estimator. Then we plot the points from X_test. From the plot it is very clear which points are misclassified.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n            pipe, \n            X_train,\n            response_method=\"predict\",\n            plot_method=\"pcolormesh\",\n            xlabel=iris.feature_names[0],\n            ylabel=iris.feature_names[1],\n            alpha=0.5)\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\")\ndisp.figure_.set_size_inches((10,7))\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 k-Fold Cross-Validation\nPreviously we perform a random split and test our model in this case. What would happen if we fit our model on another split? We might get a different accuracy score. So in order to evaluate the performance of our model, it is natual to consider several different split and compute the accuracy socre for each case, and combine all these socres together to generate an index to indicate whehter our model is good or bad. This naive idea is called k-Fold Cross-Validation.\nThe algorithm is described as follows. We first randomly split the dataset into k groups of the same size. We use one of them as the test set, and the rest together forming the training set, and use this setting to get an accuracy score. We did this for each group to be chosen as the test set. Then the final score is the mean.\n\n\n\n\n\n\nKFold\n\n\n\n\n\nKFold from sklearn.model.selection is used to split the dataset into k groups and in each iteration to chooose one as the validation set.\n\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5)\n\nfor train_idx, val_idx in kf.split(range(10)):\n    print(train_idx, val_idx)\n\n[2 3 4 5 6 7 8 9] [0 1]\n[0 1 4 5 6 7 8 9] [2 3]\n[0 1 2 3 6 7 8 9] [4 5]\n[0 1 2 3 4 5 8 9] [6 7]\n[0 1 2 3 4 5 6 7] [8 9]\n\n\n\nI only put range(10) in kf.split since it only needs to work with the index. If a dataset is put there, the output is still the index of which data is in the training set and which is in the validation set.\nIf you want to randomize the selection, when set up KFold we could add an argument shuffle=True. In this case, we may use random_state to control the outcome provide reproducing ability.\n\nLet us see an example for our data.\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.base import clone\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\ncv_scores = []\nfor train_idx, val_idx in kf.split(X):\n    pipe_tmp = clone(pipe)\n    pipe_tmp.fit(X[train_idx], y[train_idx])\n    cv_scores.append(pipe_tmp.score(X[val_idx], y[val_idx]))\ncv_scores\n\n[0.8333333333333334,\n 0.7333333333333333,\n 0.7333333333333333,\n 0.7,\n 0.7666666666666667]\n\n\n\nnp.mean(cv_scores)\n\n0.7533333333333333\n\n\nNote that here sklearn.base.clone is used to initialize an unfitted model which has the same hyperpamaters as pipe.\n\n\n\n\n\n\n\n\n\ncross_validate\n\n\n\n\n\nKFold is too “manual”. We may use cross_validate to autmate the above process. Note that depending on the arguments given cross_validate may be implemented by KFold.\n\nfrom sklearn.model_selection import cross_validate\n\ncv_result = cross_validate(pipe, X, y, cv=5, scoring='accuracy')\ncv_result\n\n{'fit_time': array([0.00200152, 0.00251961, 0.00351   , 0.00199413, 0.00100112]),\n 'score_time': array([0.11333275, 0.11545825, 0.0934999 , 0.12822008, 0.13224006]),\n 'test_score': array([0.73333333, 0.8       , 0.76666667, 0.9       , 0.73333333])}\n\n\nAnd you may only see the scores if this is the only thing that interests you.\n\ncv_result['test_score']\n\narray([0.73333333, 0.8       , 0.76666667, 0.9       , 0.73333333])\n\n\n\nYou may choose different scoring methods. More info can be found in the document.\nIf cv=5, KFold(5, shuffle=False) is applied here. If you prefer random split, you may directly use KFold here.\n\n\ncv_result = cross_validate(pipe, X, y, scoring='accuracy',\n                           cv=KFold(5, shuffle=True, random_state=1))\ncv_result['test_score']\n\narray([0.83333333, 0.73333333, 0.73333333, 0.7       , 0.76666667])\n\n\nYou may compare this result with the previous one and the one in KFold section. Of course, the cv score is usually the mean of all the scores.\n\ncv_result['test_score'].mean()\n\n0.7533333333333333\n\n\n\n\n\n\n\n\n\n\n\ncross_val_score\n\n\n\n\n\nThis is a faster way to directly get cv_result['test_score'] in cross_validate section. The argument about cv and scoring are the same as cross_validate.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(pipe, X, y, cv=KFold(5, shuffle=True, random_state=1))\ncv_scores\n\narray([0.83333333, 0.73333333, 0.73333333, 0.7       , 0.76666667])\n\n\n\ncv_scores.mean()\n\n0.7533333333333333\n\n\n\n\n\n\n\n2.2.6 Choosing a k value\nIn the previous example we choose k to be 10 as an example. To choose a k value we usually run some test by trying different k and choose the one with the best performance. In this case, best performance means the highest cross-validation score.\n\n\n\n\n\n\nGrid search\n\n\n\n\n\nsklearn.model_selection.GridSearchCV provides a way to do this directly. We only need to setup the esitimator, the metric (which is the cross-validation score in this case), and the hyperparameters to be searched through, and GridSearchCV will run the search automatically.\nWe let k go from 1 to 100. The code is as follows.\nNote that parameters is where we set the search space. It is a dictionary. The key is the name of the estimator plus double _ and then plus the name of the parameter.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n31\n\n\nAfter we fit the data, the best_estimator_.get_params() can be printed. It tells us that it is best to use 31 neibhours for our model. We can directly use the best estimator by calling clf.best_estimator_.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nnp.mean(cv_scores)\n\n0.8200000000000001\n\n\nThe cross-validation score using k=31 is calculated. This serves as a benchmark score and we may come back to dataset using other methods and compare the scores.\n\n\n\n\n\n\n\n\n\nPlot the curve\n\n\n\n\n\nGrid search can only give us a single number that has the best cross validation score. However there are many cases that the number might not be really the best. So usually we also want to see the result for all k. The best way to display all results simutanously is to plot the curve.\n\nimport matplotlib.pyplot as plt\n\nn_list = list(range(1, 101))\ncv_scores = []\nfor k in n_list:\n    pipe_tmp = clone(pipe)\n    pipe_tmp.set_params(knn__n_neighbors=k)\n    cv_scores.append(cross_val_score(pipe_tmp, X, y, cv=5).mean())\n\nplt.plot(cv_scores)\n\n\n\n\n\n\n\n\nFrom this plot, combining with the best cv score happens at k=31, we could make our final decision about which k to choose.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "href": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.3 k-NN Project 2: Dating Classification",
    "text": "2.3 k-NN Project 2: Dating Classification\nThe data can be downloaded from here.\n\n2.3.1 Background\nHelen dated several people and rated them using a three-point scale: 3 is best and 1 is worst. She also collected data from all her dates and recorded them in the file attached. These data contains 3 features:\n\nNumber of frequent flyer miles earned per year\nPercentage of time spent playing video games\nLiters of ice cream consumed per week\n\nWe would like to predict her ratings of new dates when we are given the three features.\nThe data contains four columns, while the first column refers to Mileage, the second Gamingtime, the third Icecream and the fourth Rating.\n\n\n2.3.2 Look at Data\nWe first load the data and store it into a DataFrame.\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('datingTestSet2.txt', sep='\\t', header=None)\ndf.head()\n\nTo make it easier to read, we would like to change the name of the columns.\n\ndf = df.rename(columns={0: \"Mileage\", 1: \"Gamingtime\", 2: 'Icecream', 3: 'Rating'})\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nGamingtime\nIcecream\nRating\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\nSince now we have more than 2 features, it is not suitable to directly draw scatter plots. We use seaborn.pairplot to look at the pairplot. From the below plots, before we apply any tricks, it seems that Milegae and Gamingtime are better than Icecream to classify the data points.\n\nimport seaborn as sns\nsns.pairplot(data=df, hue='Rating')\n\n\n\n\n\n\n\n\n\n\n2.3.3 Applying kNN\nSimilar to the previous example, we will apply both methods for comparisons.\n\nfrom sklearn.model_selection import train_test_split\nX = np.array(df[['Mileage', 'Gamingtime', 'Icecream']])\ny = np.array(df['Rating'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40, stratify=y)\n\n\n# Using sklearn.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.93\n\n\n\n\n2.3.4 Choosing k Value\nSimilar to the previous section, we can run tests on k value to choose one to be used in our model using GridSearchCV.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters, cv=5)\nclf.fit(X_train, y_train)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n12\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\n\nn_list = list(range(1, 101))\ncv_scores = []\nfor k in n_list:\n    pipe_tmp = clone(pipe)\n    pipe_tmp.set_params(knn__n_neighbors=k)\n    cv_scores.append(cross_val_score(pipe_tmp, X_train, y_train, cv=5).mean())\nplt.plot(cv_scores)\n\n\n\n\n\n\n\n\nFrom this result, in this case the best k is 12. The corresponding test score is\n\nclf.score(X_test, y_test)\n\n0.93",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "href": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.4 k-NN Project 3: Handwritten recognition",
    "text": "2.4 k-NN Project 3: Handwritten recognition\nWe would like to let the machine recognize handwritten digits. The dataset is MNIST comeing from the MNIST database. Now we apply kNN algrotithm to it.\n\n2.4.1 Dataset description\nEvery digit is stored as a \\(28\\times28\\) picture. This is a \\(28\\times28\\) matrix. Every entry represents a gray value of the corresponding pixel, whose value is from 0 to 255. The label of each matrix is the digit it represents. Note that the dataset provided is already splitted into a training set and a test set.\nThe dataset can be loaded following the instruction.\n\nfrom datasets import load_dataset\nimport numpy as np\nimport itertools\n\ndef stream_to_array(streaming, max=None):\n    pic_list = []\n    label_list =[]\n    if max is None:\n        generator = streaming\n    else:\n        generator = itertools.islice(streaming, max)\n    for data in generator:\n        pic_list.append(np.array(data['image']).reshape(-1))\n        label_list.append(data['label'])\n    return np.array(pic_list), np.array(label_list)\n\nmnist_train = load_dataset(\"ylecun/mnist\", split='train', streaming=True)\nmnist_test = load_dataset(\"ylecun/mnist\", split='test', streaming=True)\n\nX_train, y_train = stream_to_array(mnist_train, max=600)\nX_test, y_test = stream_to_array(mnist_test, max=100)\n\nNote that one of the purpose to load the data in streaming mode is that the dataset is big and it is not wise to load everything all together. However this is the only way to train a KNN model since all it does is to memorize everything. In the future with other models we may want to load the image one by one with the streaming mode.\nAlso due to the issue of large dataset, I only choose the first 600/100 images from the original dataset. The itertools.islice is used to only choose the first few items from a generator.\n\n\n2.4.2 Apply k-NN\nLike the previous two examples, we now try to apply the k-NN algorithm to classify these handwritten digits. Note that the original dataset is huge and the processing time is very slow. However since we only choose 600/100 images, we could still run all our tricks.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors=5))]\npipe = Pipeline(steps=steps)\nn_list = list(range(1, 11))\n\ncv_score = []\nfor k in n_list:\n    pipe_tmp = clone(pipe)\n    pipe_tmp.set_params(knn__n_neighbors=k)\n    cv_score.append(cross_val_score(pipe_tmp, X_train, y_train, cv=5).mean())\nplt.plot(n_list, cv_score)\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngs = GridSearchCV(pipe, param_grid=dict(knn__n_neighbors=n_list), cv=5)\ngs.fit(X_train, y_train)\ngs.best_params_\n\n{'knn__n_neighbors': 3}\n\n\nThe best k=3 for thi degenerated dataset. The corresponding test score is\n\ngs.score(X_test, y_test)\n\n0.82",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#exercises-and-projects",
    "href": "contents/2/intro.html#exercises-and-projects",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.5 Exercises and Projects",
    "text": "2.5 Exercises and Projects\n\nExercise 2.1 Handwritten example :label: ex2handwritten Consider the 1-dimensional data set shown below.\n\n\n\n\n\n\n\nx\n1.5\n2.5\n3.5\n4.5\n5.0\n5.5\n5.75\n6.5\n7.5\n10.5\n\n\n\n\ny\n+\n+\n-\n-\n-\n+\n+\n-\n+\n+\n\n\n\n\n\nPlease use the data to compute the class of \\(x=5.5\\) according to \\(k=1\\), \\(3\\), \\(6\\) and \\(9\\). Please compute everything by hand.\n\n\nExercise 2.2 (Titanic) Please download the titanic dataset from here. This is the same dataset from what you dealt with in Chapter 1 Exercises. Therefore you may use the same way to prepare the data.\nPlease analyze the dataset and build a k-NN model to predict whether someone is survived or not. Note that you have to pick k at the end.\n\n\n\n\n\n[1] Harrington, P. (2012). Machine learning in action. Manning Publications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html",
    "href": "contents/3/intro.html",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1 Gini impurity\nTo split a dataset, we need a metric to tell whether the split is good or not. The two most popular metrics that are used are Gini impurity and Entropy. The two metrics don’t have essential differences, that the results obtained by applying each metric are very similar to each other. Therefore we will only focus on Gini impurity since it is slightly easier to compute and slightly easier to explain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#gini-impurity",
    "href": "contents/3/intro.html#gini-impurity",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1.1 Motivation and Definition\nAssume that we have a dataset of totally \\(n\\) objects, and these objects are divided into \\(k\\) classes. The \\(i\\)-th class has \\(n_i\\) objects. Then if we randomly pick an object, the probability to get an object belonging to the \\(i\\)-th class is\n\\[\np_i=\\frac{n_i}{n}\n\\]\nIf we then guess the class of the object purely based on the distribution of each class, the probability that our guess is incorrect is\n\\[\n1-p_i = 1-\\frac{n_i}{n}.\n\\]\nTherefore, if we randomly pick an object that belongs to the \\(i\\)-th class and randomly guess its class purely based on the distribution but our guess is wrong, the probability that such a thing happens is\n\\[\np_i(1-p_i).\n\\]\nConsider all classes. The probability at which any object of the dataset will be mislabelled when it is randomly labeled is the sum of the probability described above:\n\\[\n\\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2.\n\\]\nThis is the definition formula for the Gini impurity.\n\nDefinition 3.1 The Gini impurity is calculated using the following formula\n\\[\nGini = \\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2,\n\\] where \\(p_i\\) is the probability of class \\(i\\).\n\nThe way to understand Gini impurity is to consider some extreme examples.\n\nExample 3.1 Assume that we only have one class. Therefore \\(k=1\\), and \\(p_1=1\\). Then the Gini impurity is\n\\[\nGini = 1-1^2=0.\n\\] This is the minimum possible Gini impurity. It means that the dataset is pure: all the objects contained are of one unique class. In this case, we won’t make any mistakes if we randomly guess the label.\n\n\nExample 3.2 Assume that we have two classes. Therefore \\(k=2\\). Consider the distribution \\(p_1\\) and \\(p_2\\). We know that \\(p_1+p_2=1\\). Therefore \\(p_2=1-p_1\\). Then the Gini impurity is\n\\[\nGini(p_1) = 1-p_1^2-p_2^2=1-p_1^2-(1-p_1)^2=2p_1-2p_1^2.\n\\] When \\(0\\leq p_1\\leq 1\\), this function \\(Gini(p_1)\\) is between \\(0\\) and \\(0.5\\). - It gets \\(0\\) when \\(p_1=0\\) or \\(1\\). In these two cases, the dataset is still a one-class set since the size of one class is \\(0\\). - It gets \\(0.5\\) when \\(p_1=0.5\\). This means that the Gini impurity is maximized when the size of different classes are balanced.\n\n\n\n3.1.2 Algorithm\n\n\n\n\n\n\nAlgorithm: Gini impurity\n\n\n\nInputs A dataset \\(S=\\{data=[features, label]\\}\\) with labels.\nOutputs The Gini impurity of the dataset.\n\nGet the size \\(n\\) of the dataset.\nGo through the label list, and find all unique labels: \\(uniqueLabelList\\).\nGo through each label \\(l\\) in \\(uniqueLabelList\\) and count how many elements belonging to the label, and record them as \\(n_l\\).\nUse the formula to compute the Gini impurity:\n\\[\nGini = 1-\\sum_{l\\in uniqueLabelList}\\left(\\frac{n_l}{n}\\right)^2.\n\\]\n\n\n\nThe sample codes are listed below:\n\nimport pandas as pd\ndef gini(S):\n    N = len(S)\n    y = S[:, -1].reshape(N)\n    gini = 1 - ((pd.Series(y).value_counts()/N)**2).sum()\n    return gini",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#cart-algorithms",
    "href": "contents/3/intro.html#cart-algorithms",
    "title": "3  Decision Trees",
    "section": "3.2 CART Algorithms",
    "text": "3.2 CART Algorithms\n\n3.2.1 Ideas\nConsider a labeled dataset \\(S\\) with totally \\(m\\) elements. We use a feature \\(k\\) and a threshold \\(t_k\\) to split it into two subsets: \\(S_l\\) with \\(m_l\\) elements and \\(S_r\\) with \\(m_r\\) elements. Then the cost function of this split is\n\\[\nJ(k, t_k)=\\frac{m_l}mGini(S_l)+\\frac{m_r}{m}Gini(S_r).\n\\] It is not hard to see that the more pure the two subsets are the lower the cost function is. Therefore our goal is find a split that can minimize the cost function.\n\n\n\n\n\n\nAlgorithm: Split the Dataset\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\).\nOutputs A best split \\((k, t_k)\\).\n\nFor each feature \\(k\\):\n\nFor each value \\(t\\) of the feature:\n\nSplit the dataset \\(S\\) into two subsets, one with \\(k\\leq t\\) and one with \\(k&gt;t\\).\nCompute the cost function \\(J(k,t)\\).\nCompare it with the current smallest cost. If this split has smaller cost, replace the current smallest cost and pair with \\((k, t)\\).\n\n\nReturn the pair \\((k,t_k)\\) that has the smallest cost function.\n\n\n\nWe then use this split algorithm recursively to get the decision tree.\n\n\n\n\n\n\nClassification and Regression Tree, CART\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\) and a maximal depth max_depth.\nOutputs A decision tree.\n\nStarting from the original dataset \\(S\\). Set the working dataset \\(G=S\\).\nConsider a dataset \\(G\\). If \\(Gini(G)\\neq0\\), split \\(G\\) into \\(G_l\\) and \\(G_r\\) to minimize the cost function. Record the split pair \\((k, t_k)\\).\nNow set the working dataset \\(G=G_l\\) and \\(G=G_r\\) respectively, and apply the above two steps to each of them.\nRepeat the above steps, until max_depth is reached.\n\n\n\nHere are the sample codes.\n\ndef split(G):\n    m = G.shape[0]\n    gmini = gini(G)\n    pair = None\n    if gini(G) != 0:\n        numOffeatures = G.shape[1] - 1\n        for k in range(numOffeatures):\n            for t in range(m):\n                Gl = G[G[:, k] &lt;= G[t, k]]\n                Gr = G[G[:, k] &gt; G[t, k]]\n                gl = gini(Gl)\n                gr = gini(Gr)\n                ml = Gl.shape[0]\n                mr = Gr.shape[0]\n                g = gl*ml/m + gr*mr/m\n                if g &lt; gmini:\n                    gmini = g\n                    pair = (k, G[t, k])\n                    Glm = Gl\n                    Grm = Gr\n        res = {'split': True,\n               'pair': pair,\n               'sets': (Glm, Grm)}\n    else:\n        res = {'split': False,\n               'pair': pair,\n               'sets': G}\n    return res\n\nFor the purpose of counting labels, we also write a code to do so.\n\nimport pandas as pd\ndef countlabels(S):\n    y = S[:, -1].reshape(S.shape[0])\n    labelCount = dict(pd.Series(y).value_counts())\n    return labelCount",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "href": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "title": "3  Decision Trees",
    "section": "3.3 Decision Tree Project 1: the iris dataset",
    "text": "3.3 Decision Tree Project 1: the iris dataset\nWe are going to use the Decision Tree model to study the iris dataset. This dataset has already studied previously using k-NN. Again we will only use the first two features for visualization purpose.\n\n3.3.1 Initial setup\nSince the dataset will be splitted, we will put X and y together as a single variable S. In this case when we split the dataset by selecting rows, the features and the labels are still paired correctly.\nWe also print the labels and the feature names for our convenience.\n\nfrom sklearn.datasets import load_iris\nimport numpy as np\nfrom assests.codes.dt import gini, split, countlabels\n\niris = load_iris()\nX = iris.data[:, 2:]\ny = iris.target\ny = y.reshape((y.shape[0],1))\nS = np.concatenate([X,y], axis=1)\n\nprint(iris.target_names)\nprint(iris.feature_names)\n\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\n\n\n3.3.2 Apply CART manually\nWe apply split to the dataset S.\n\nr = split(S)\nif r['split'] is True:\n    Gl, Gr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Gl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gr)),\n          ' and its label counts is {d}'.format(d=countlabels(Gr)))\n\n(0, 1.9)\nThe left subset's Gini impurity is 0.00,  and its label counts is {0.0: 50}\nThe right subset's Gini impurity is 0.50,  and its label counts is {1.0: 50, 2.0: 50}\n\n\nThe results shows that S is splitted into two subsets based on the 0-th feature and the split value is 1.9.\nThe left subset is already pure since its Gini impurity is 0. All elements in the left subset is label 0 (which is setosa). The right one is mixed since its Gini impurity is 0.5. Therefore we need to apply split again to the right subset.\n\nr = split(Gr)\nif r['split'] is True:\n    Grl, Grr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Grl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grr)),\n          ' and its label counts is {d}'.format(d=countlabels(Grr)))\n\n(1, 1.7)\nThe left subset's Gini impurity is 0.17,  and its label counts is {1.0: 49, 2.0: 5}\nThe right subset's Gini impurity is 0.04,  and its label counts is {2.0: 45, 1.0: 1}\n\n\nThis time the subset is splitted into two more subsets based on the 1-st feature and the split value is 1.7. The total Gini impurity is minimized using this split.\nThe decision we created so far can be described as follows:\n\nCheck the first feature sepal length (cm) to see whether it is smaller or equal to 1.9.\n\nIf it is, classify it as lable 0 which is setosa.\nIf not, continue to the next stage.\n\nCheck the second feature sepal width (cm) to see whether it is smaller or equal to 1.7.\n\nIf it is, classify it as label 1 which is versicolor.\nIf not, classify it as label 2 which is virginica.\n\n\n\n\n3.3.3 Use package sklearn\nNow we would like to use the decision tree package provided by sklearn. The process is straightforward. The parameter random_state=40 will be discussed {ref}later&lt;note-random_state&gt;, and it is not necessary in most cases.\n\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth=2, random_state=40)\nclf.fit(X, y)\n\nDecisionTreeClassifier(max_depth=2, random_state=40)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=2, random_state=40)\n\n\nsklearn provide a way to automatically generate the tree view of the decision tree. The code is as follows.\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(2, 2), dpi=200)\ntree.plot_tree(clf, filled=True, impurity=True)\n\n[Text(0.4, 0.8333333333333334, 'x[1] &lt;= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.2, 0.5, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.6, 0.5, 'x[1] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.4, 0.16666666666666666, 'gini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.8, 0.16666666666666666, 'gini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]')]\n\n\n\n\n\n\n\n\n\nSimilar to k-NN, we may use sklearn.inspection.DecisionBoundaryDisplay to visualize the decision boundary of this decision tree.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap='coolwarm',\n    response_method=\"predict\",\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n)\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=15)\n\n\n\n\n\n\n\n\n\n\n3.3.4 Analyze the differences between the two methods\nThe tree generated by sklearn and the tree we got manually is a little bit different. Let us explore the differences here.\nTo make it easier to split the set, we could convert the numpy.ndarray to pandas.DataFrame.\n\nimport pandas as pd\n\ndf = pd.DataFrame(X)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.4\n0.2\n\n\n1\n1.4\n0.2\n\n\n2\n1.3\n0.2\n\n\n3\n1.5\n0.2\n\n\n4\n1.4\n0.2\n\n\n\n\n\n\n\nNow based on our tree, we would like to get all data points that the first feature (which is marked as 0) is smaller or equal to 1.9. We save it as df1. Similarly based on the tree gotten from sklearn, we would like to get all data points taht the second feature (which is marked as 1) is smaller or equal to 0.8 and save it to df2.\n\ndf1 = df[df[0]&lt;=1.9]\ndf2 = df[df[1]&lt;=0.8]\n\nThen we would like to compare these two dataframes. What we want is to see whether they are the same regardless the order. One way to do this is to sort the two dataframes and then compare them directly.\nTo sort the dataframe we use the method DataFrame.sort_values. The details can be found here. Note that after sort_values we apply reset_index to reset the index just in case the index is massed by the sort operation.\nThen we use DataFrame.equals to check whether they are the same.\n\ndf1sorted = df1.sort_values(by=df1.columns.tolist()).reset_index(drop=True)\ndf2sorted = df2.sort_values(by=df2.columns.tolist()).reset_index(drop=True)\nprint(df1sorted.equals(df2sorted))\n\nTrue\n\n\nSo these two sets are really the same. The reason this happens can be seen from the following two graphs.\n\n\n\n\n\n\n\n\n\nFrom our code\n\n\n\n\n\n\n\nFrom sklearn\n\n\n\n\n\nSo you can see that either way can give us the same classification. This means that given one dataset the construction of the decision tree might be random at some points.\n\n\n\n\n\n\nnote-random_state\n\n\n\nSince the split is random, when using sklearn.DecisionTreeClassifier to construct decision trees, sometimes we might get the same tree as what we get from our naive codes.\nTo illustrate this phenomenaon I need to set the random state in case it will generate the same tree as ours when I need it to generate a different tree. The parameter random_state=40 mentioned before is for this purpose.\n\n\nAnother difference is the split value of the second branch. In our case it is 1.7 and in sklearn case it is 1.75. So after we get the right subset from the first split (which is called dfr), we would split it into two sets based on whether the second feature is above or below 1.7.\n\ndfr = df[df[0]&gt;1.9]\ndf2a = dfr[dfr[1]&gt;1.7]\ndf2b = dfr[dfr[1]&lt;=1.7]\nprint(df2b[1].max())\nprint(df2a[1].min())\n\n1.7\n1.8\n\n\nNow you can see where the split number comes from. In our code, when we found a split, we will directly use that number as the cut. In this case it is 1.7.\nIn sklearn, when it finds a split, the algorithm will go for the middle of the gap as the cut. In this case it is (1.7+1.8)/2=1.75.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-2-make_moons-dataset",
    "href": "contents/3/intro.html#decision-tree-project-2-make_moons-dataset",
    "title": "3  Decision Trees",
    "section": "3.4 Decision Tree Project 2: make_moons dataset",
    "text": "3.4 Decision Tree Project 2: make_moons dataset\nsklearn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. We are going to use make_moons in this section. More details can be found here.\nmake_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. make_moons produces two interleaving half circles. It is useful for visualization.\nLet us explorer the dataset first.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\n\n\n\n\n\n\n\n\nNow we are applying sklearn.DecisionTreeClassifier to construct the decision tree. The steps are as follows.\n\nSplit the dataset into training data and test data.\nConstruct the pipeline. Since we won’t apply any transformers there for this problem, we may just use the classifier sklearn.DecisionTreeClassifier directly without really construct the pipeline object.\nConsider the hyperparameter space for grid search. For this problme we choose min_samples_split and max_leaf_nodes as the hyperparameters we need. We will let min_samples_split run through 2 to 5, and max_leaf_nodes run through 2 to 50. We will use grid_search_cv to find the best hyperparameter for our model. For cross-validation, the number of split is set to be 3 which means that we will run trainning 3 times for each pair of hyperparameters.\nRun grid_search_cv. Find the best hyperparameters and the best estimator. Test it on the test set to get the accuracy score.\n\n\n# Step 1\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Step 3\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nparams = {'min_samples_split': list(range(2, 5)),\n          'max_leaf_nodes': list(range(2, 50))}\ngrid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), \n                              params, verbose=1, cv=3)\ngrid_search_cv.fit(X_train, y_train)\n\nFitting 3 folds for each of 144 candidates, totalling 432 fits\n\n\nGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)estimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=42)DecisionTreeClassifierDecisionTreeClassifier(random_state=42)\n\n\n\n# Step 4\nfrom sklearn.metrics import accuracy_score\n\nclf = grid_search_cv.best_estimator_\nprint(grid_search_cv.best_params_)\ny_pred = clf.predict(X_test)\naccuracy_score(y_pred, y_test)\n\n{'max_leaf_nodes': 17, 'min_samples_split': 2}\n\n\n0.8695\n\n\nNow you can see that for this make_moons dataset, the best decision tree should have at most 17 leaf nodes and the minimum number of samples required to be at a leaft node is 2. The fitted decision tree can get 86.95% accuracy on the test set.\nNow we can plot the decision tree and the decision surface.\n\nfrom sklearn import tree\nplt.figure(figsize=(15, 15), dpi=300)\ntree.plot_tree(clf, filled=True)\n\n[Text(0.5340909090909091, 0.9375, 'x[1] &lt;= 0.296\\ngini = 0.5\\nsamples = 8000\\nvalue = [3987, 4013]'),\n Text(0.25, 0.8125, 'x[0] &lt;= -0.476\\ngini = 0.367\\nsamples = 4275\\nvalue = [1036, 3239]'),\n Text(0.09090909090909091, 0.6875, 'x[0] &lt;= -0.764\\ngini = 0.183\\nsamples = 472\\nvalue = [424, 48]'),\n Text(0.045454545454545456, 0.5625, 'gini = 0.035\\nsamples = 333\\nvalue = [327, 6]'),\n Text(0.13636363636363635, 0.5625, 'x[1] &lt;= 0.047\\ngini = 0.422\\nsamples = 139\\nvalue = [97, 42]'),\n Text(0.09090909090909091, 0.4375, 'gini = 0.496\\nsamples = 70\\nvalue = [38, 32]'),\n Text(0.18181818181818182, 0.4375, 'gini = 0.248\\nsamples = 69\\nvalue = [59, 10]'),\n Text(0.4090909090909091, 0.6875, 'x[1] &lt;= -0.062\\ngini = 0.27\\nsamples = 3803\\nvalue = [612, 3191]'),\n Text(0.3181818181818182, 0.5625, 'x[1] &lt;= -0.371\\ngini = 0.147\\nsamples = 2426\\nvalue = [194, 2232]'),\n Text(0.2727272727272727, 0.4375, 'gini = 0.079\\nsamples = 1336\\nvalue = [55, 1281]'),\n Text(0.36363636363636365, 0.4375, 'gini = 0.223\\nsamples = 1090\\nvalue = [139, 951]'),\n Text(0.5, 0.5625, 'x[0] &lt;= 1.508\\ngini = 0.423\\nsamples = 1377\\nvalue = [418, 959]'),\n Text(0.45454545454545453, 0.4375, 'x[0] &lt;= 0.503\\ngini = 0.48\\nsamples = 1013\\nvalue = [404, 609]'),\n Text(0.36363636363636365, 0.3125, 'x[0] &lt;= -0.162\\ngini = 0.417\\nsamples = 469\\nvalue = [139, 330]'),\n Text(0.3181818181818182, 0.1875, 'gini = 0.5\\nsamples = 120\\nvalue = [61, 59]'),\n Text(0.4090909090909091, 0.1875, 'gini = 0.347\\nsamples = 349\\nvalue = [78, 271]'),\n Text(0.5454545454545454, 0.3125, 'x[0] &lt;= 1.1\\ngini = 0.5\\nsamples = 544\\nvalue = [265, 279]'),\n Text(0.5, 0.1875, 'x[1] &lt;= 0.129\\ngini = 0.49\\nsamples = 339\\nvalue = [193, 146]'),\n Text(0.45454545454545453, 0.0625, 'gini = 0.498\\nsamples = 178\\nvalue = [84, 94]'),\n Text(0.5454545454545454, 0.0625, 'gini = 0.437\\nsamples = 161\\nvalue = [109, 52]'),\n Text(0.5909090909090909, 0.1875, 'gini = 0.456\\nsamples = 205\\nvalue = [72, 133]'),\n Text(0.5454545454545454, 0.4375, 'gini = 0.074\\nsamples = 364\\nvalue = [14, 350]'),\n Text(0.8181818181818182, 0.8125, 'x[0] &lt;= 1.452\\ngini = 0.329\\nsamples = 3725\\nvalue = [2951, 774]'),\n Text(0.7272727272727273, 0.6875, 'x[1] &lt;= 0.757\\ngini = 0.232\\nsamples = 3355\\nvalue = [2905, 450]'),\n Text(0.6818181818181818, 0.5625, 'x[0] &lt;= -0.588\\ngini = 0.349\\nsamples = 1629\\nvalue = [1262, 367]'),\n Text(0.6363636363636364, 0.4375, 'gini = 0.07\\nsamples = 384\\nvalue = [370, 14]'),\n Text(0.7272727272727273, 0.4375, 'x[1] &lt;= 0.439\\ngini = 0.406\\nsamples = 1245\\nvalue = [892, 353]'),\n Text(0.6818181818181818, 0.3125, 'gini = 0.477\\nsamples = 420\\nvalue = [255, 165]'),\n Text(0.7727272727272727, 0.3125, 'gini = 0.352\\nsamples = 825\\nvalue = [637, 188]'),\n Text(0.7727272727272727, 0.5625, 'gini = 0.092\\nsamples = 1726\\nvalue = [1643, 83]'),\n Text(0.9090909090909091, 0.6875, 'x[0] &lt;= 1.782\\ngini = 0.218\\nsamples = 370\\nvalue = [46, 324]'),\n Text(0.8636363636363636, 0.5625, 'gini = 0.416\\nsamples = 132\\nvalue = [39, 93]'),\n Text(0.9545454545454546, 0.5625, 'gini = 0.057\\nsamples = 238\\nvalue = [7, 231]')]\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap=plt.cm.RdYlBu,\n    response_method=\"predict\"\n)\nplt.scatter(\n    X[:, 0],\n    X[:, 1],\n    c=y,\n    cmap='gray',\n    edgecolor=\"black\",\n    s=15,\n    alpha=.15)\n\n\n\n\n\n\n\n\nSince it is not very clear what the boundary looks like, I will draw the decision surface individually below.\n\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap=plt.cm.RdYlBu,\n    response_method=\"predict\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#exercises-and-projects",
    "href": "contents/3/intro.html#exercises-and-projects",
    "title": "3  Decision Trees",
    "section": "3.5 Exercises and Projects",
    "text": "3.5 Exercises and Projects\n\nExercise 3.1 The dataset and its scattering plot is given below.\n\nPlease calculate the Gini impurity of the whole set by hand.\nPlease apply CART to create the decision tree by hand.\nPlease use the tree you created to classify the following points:\n\n\\((0.4, 1.0)\\)\n\\((0.6, 1.0)\\)\n\\((0.6, 0)\\)\n\n\nThe following code is for ploting. You may also get the precise data points by reading the code. You don’t need to write codes to solve the problem.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.2 CHOOSE ONE: Please apply the Decision Tree to one of the following datasets.\n\ndating dataset (in Chpater 2).\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the accuracy (or cross-validation score) of your model and compare it with the models you learned before (kNN).\nPlease find the two most important features and explane your reason.\n(Optional) Use the two most important features to draw the Decision Boundary if possible.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html",
    "href": "contents/4/intro.html",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1 Bootstrap aggregating",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#bootstrap-aggregating",
    "href": "contents/4/intro.html#bootstrap-aggregating",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1.1 Basic bagging\nOne approach to get many estimators is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.\nConsider the following example. The dataset is the one we used in Chpater 3: make_moon. We split the dataset into training and test sets.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\n\n\n\n\n\n\n\nWe would like to sample from the dataset to get some smaller minisets. We will use sklearn.model_selection.ShuffleSplit to perform the action.\nThe output of ShuffleSplit is a generator. To get the index out of it we need a for loop. You may check out the following code.\nNote that ShuffleSplit is originally used to shuffle data into training and test sets. We would only use the shuffle function out of it, so we will set test_size to be 1 and use _ later in the for loop since we won’t use that part of the information.\nWhat we finally get is a generator rs that produces indexes of subsets of X_train and y_train.\n\nfrom sklearn.model_selection import ShuffleSplit\nn_trees = 1000\nn_instances = 100\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\n\nNow we would like to generate a list of Decision Trees. We could use the hyperparameters we get from Chapter 3. We train each tree over a certain mini set, and then evaluate the trained model over the test set. The average accuracy is around 80%.\nNote that rs is a generator. We put it in a for loop, and during each loop it will produce a list of indexes which gives a subset. We will directly train our model over the subset and use it to predict the test set. The result of each tree is put in the list y_pred_list and the accuracy is stored in the list acc_list. The mean of the accuracy is then computed by np.mean(acc_list).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(min_samples_split=2, max_leaf_nodes=17)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\n\nnp.mean(acc_list)\n\n0.7982400000000001\n\n\nNow for each test data, we actually have n_trees=1000 predicted results. We can treat it as the options from 1000 exports and would like to use the majority as our result. For this purpose we would like to use mode() which will find the most frequent entry.\n\nfrom scipy.stats import mode\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\n\nSince the output of mode is a tuple where the first entry is a 2D array, we need to reshape y_pred_mode. This is the result using this voting system. Then we are able to compute the accuracy, and find that it is increased from the previous prediction.\n\naccuracy_score(y_pred_mode, y_test)\n\n0.8646666666666667\n\n\n\n\n4.1.2 Some rough analysis\nThe point of Bagging is to let every classifier study part of the data, and then gather the opinions from everyone. If the performance are almost the same between individual classifers and the Bagging classifiers, this means that the majority of the individual classifiers have the same opinions. One possible reason is that the randomized subsets already catch the main features of the dataset that every individual classifiers behave similar.\n\n4.1.2.1 Case 1\nLet us continue with the previous dataset. We start from using Decision Tree with max_depth=1. In other words each tree only split once.\n\nn_trees = 500\nn_instances = 1000\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.7704213333333334\nThe accuracy of the bagging classifier: 0.772\n\n\nThe two accuracy has some differences, but not much. This is due to the fact that the sample size of the subset is too large: 1000 can already help the individual classifers to capture the major ideas of the datasets. Let us see the first 1000 data points. The scattering plot is very similar to that of the whole dataset shown above.\n\nNpiece = 1000\nplt.scatter(x=X[:Npiece, 0], y=X[:Npiece, 1], c=y[:Npiece])\n\n\n\n\n\n\n\n\n\n\n4.1.2.2 Case 2\nIf we reduce the sample size to be very small, for example, 20, the sampled subset will lose a lot of information and it will be much harder to capture the idea of the original dataset. See the scattering plot of the first 20 data points.\n\nNpiece = 20\nplt.scatter(x=X[:Npiece, 0], y=X[:Npiece, 1], c=y[:Npiece])\n\n\n\n\n\n\n\n\nIn this case, let us see the performance comparison between multiple decision trees and the bagging classifier.\n\nn_trees = 500\nn_instances = 20\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n    clf_ind.fit(X_subset, y_subset)\n    y_pred = clf_ind.predict(X_test)\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.7273333333333334\nThe accuracy of the bagging classifier: 0.814\n\n\nThis time you may see a significant increase in the performance.\n\n\n\n4.1.3 Using sklearn\nsklearn provides BaggingClassifier to directly perform bagging or pasting. The code is as follows.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(),\n                            n_estimators=1000,\n                            max_samples=100,\n                            bootstrap=True)\n\nIn the above code, bag_clf is a bagging classifier, made of 500 DecisionTreeClassifers, and is trained over subsets of size 100. The option bootstrap=True means that it is bagging. If you would like to use pasting, the option is bootstrap=False.\nThis bag_clf also has .fit() and .predict() methods. It is used the same as our previous classifiers. Let us try the make_moon dataset.\n\nbag_clf.fit(X_train, y_train)\ny_pred_bag = bag_clf.predict(X_test)\naccuracy_score(y_pred_bag, y_test)\n\n0.8606666666666667\n\n\n\n\n4.1.4 OOB score\nWhen we use bagging, it is possible that some of the training data are not used. In this case, we could record which data are not used, and just use them as the test set, instead of providing extra data for test. The data that are not used is called out-of-bag instances, or oob for short. The accuracy over the oob data is called the oob score.\nWe could set oob_score=True to enable the function when creating a BaggingClassifier, and use .oob_score_ to get the oob score after training.\n\nbag_clf_oob = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=1000,\n                                max_samples=100,\n                                bootstrap=True,\n                                oob_score=True)\nbag_clf_oob.fit(X_train, y_train)\nbag_clf_oob.oob_score_\n\n0.8649411764705882\n\n\n\n\n4.1.5 Random Forests\nWhen the classifiers used in a bagging classifier are all Decision Trees, the bagging classifier is called a random forest. sklearn provide RandomForestClassifier class. It is almost the same as BaggingClassifier + DecisionTreeClassifer.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\ny_pred_rnd = rnd_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.8613333333333333\n\n\nWhen we use the Decision Tree as our base estimators, the class RandomForestClassifier provides more control over growing the random forest, with a certain optimizations. If you would like to use other estimators, then BaggingClassifier should be used.\n\n\n4.1.6 Extra-trees\nWhen growing a Decision Tree, our method is to search through all possible ways to find the best split point that get the lowest Gini impurity. Anohter method is to use a random split. Of course a random tree performs much worse, but if we use it to form a random forest, the voting system can help to increase the accuracy. On the other hand, random split is much faster than a regular Decision Tree.\nThis type of forest is called Extremely Randomized Trees, or Extra-Trees for short. We could modify the above random forest classifier code to implement the extra-tree algorithm. The key point is that we don’t apply the Decision Tree algorithm to X_subset. Instead we perform a random split.\n\nn_trees = 500\nn_instances = 20\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\ny_pred_list = list()\nacc_list = list()\nfor mini_train_index, _ in rs:\n    X_subset = X_train[mini_train_index]\n    y_subset = y_train[mini_train_index]\n    clf_ind = DecisionTreeClassifier(max_depth=1)\n# random split\n    i = np.random.randint(0, X_subset.shape[0])\n    j = np.random.randint(0, X_subset.shape[1])\n    split_threshold = X_subset[i, j]\n    lsetindex = np.where(X_subset[:, j]&lt;split_threshold)[0]\n\n    if len(lsetindex) == 0:\n        rsetindex = np.where(X_subset[:, j]&gt;=split_threshold)\n        rmode, _ = mode(y_subset[rsetindex], keepdims=True)\n        rmode = rmode[0]\n        lmode = 1 - rmode\n    else:\n        lmode, _ = mode(y_subset[lsetindex], keepdims=True)\n        lmode = lmode[0]\n        rmode = 1 - lmode\n    y_pred = np.where(X_test[:, j] &lt; split_threshold, lmode, rmode).reshape(-1)\n# The above code is used to use the random split to classify the data points\n    y_pred_list.append(y_pred)\n    acc_list.append(accuracy_score(y_pred, y_test))\nprint('The mean of individual accuracy: {}'.format(np.mean(acc_list)))\n\nvoting = np.array(y_pred_list)\ny_pred_mode, _ = mode(voting, axis=0, keepdims=False)\nprint('The accuracy of the bagging classifier: {}'.format(accuracy_score(y_pred_mode, y_test)))\n\nThe mean of individual accuracy: 0.6313213333333333\nThe accuracy of the bagging classifier: 0.8273333333333334\n\n\nFrom the above example, you may find a significant increase in the performace from the mean individual accuracy to the Extra-tree classifier accuracy. The accuracy of the Extra-tree classifier is also very close to what we get from the original data points, although its base classifier is much simpler.\nIn sklearn there is an ExtraTreesClassifier to create such a classifier. It is hard to say which random forest is better beforehand. What we can do is to test and calculate the cross-validation scores (with grid search for hyperparameters tuning).\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\ny_pred_rnd = ext_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.858\n\n\nIn the above example, RandomForestClassifier and ExtraTreesClassifier get similar accuracy. However from the code below, you will see that in this example ExtraTreesClassifier is much faster than RandomForestClassifier.\n\nfrom time import time\nt0 = time()\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\nt1 = time()\nprint('Random Frorest: {}'.format(t1 - t0))\n\nt0 = time()\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\nt1 = time()\nprint('Extremely Randomized Trees: {}'.format(t1 - t0))\n\nRandom Frorest: 9.64048957824707\nExtremely Randomized Trees: 1.4413414001464844\n\n\n\n\n4.1.7 Gini importance\nAfter training a Decision Tree, we could look at each node. Each split is against a feature, which decrease the Gini impurity the most. In other words, we could say that the feature is the most important during the split.\nUsing the average Gini impurity decreased as a metric, we could measure the importance of each feature. This is called Gini importance. If the feature is useful, it tends to split mixed labeled nodes into pure single class nodes.\nIn the case of random forest, since there are many trees, we might compute the weighted average of the Gini importance across all trees. The weight depends on how many times the feature is used in a specific node.\nUsing RandomForestClassifier, we can directly get access to the Gini importance of each feature by .feature_importance_. Please see the following example.\n\nrnd_clf.fit(X_train, y_train)\nrnd_clf.feature_importances_\n\narray([0.43823588, 0.56176412])\n\n\nIn this example, you may see that the two features are relavely equally important, where the second feature is slightly more important since on average it decrease the Gini impurity a little bit more.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#voting-machine",
    "href": "contents/4/intro.html#voting-machine",
    "title": "4  Ensemble methods",
    "section": "4.2 Voting machine",
    "text": "4.2 Voting machine\n\n4.2.1 Voting classifier\nAssume that we have several trained classifiers. The easiest way to make a better classifer out of what we already have is to build a voting system. That is, each classifier give its own prediction, and it will be considered as a vote, and finally the highest vote will be the prediction of the system.\nIn sklearn, you may use VotingClassifier. It works as follows.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclfs = [('knn', KNeighborsClassifier(n_neighbors=5)),\n        ('dt', DecisionTreeClassifier(max_depth=2))]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\n\nAll classifiers are stored in the list clfs, whose elements are tuples. The syntax is very similar to Pipeline. What the classifier does is to train all listed classifiers and use the majority vote to predict the class of given test data. If each classifier has one vote, the voting method is hard. There is also a soft voting method. In this case, every classifiers not only can predict the classes of the given data, but also estimiate the probability of the given data that belongs to certain classes. On coding level, each classifier should have the predict_proba() method. In this case, the weight of each vote is determined by the probability computed. In our course we mainly use hard voting.\nLet us use make_moon as an example. We first load the dataset.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nWe would like to apply kNN model. As before, we build a data pipeline pipe to first apply MinMaxScaler and then KNeighborsClassifier.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline(steps=[('scalar', MinMaxScaler()),\n                       ('knn', KNeighborsClassifier())])\nparameters = {'knn__n_neighbors': list(range(1, 51))}\ngs_knn = GridSearchCV(pipe, param_grid=parameters) \ngs_knn.fit(X_train, y_train)\nclf_knn = gs_knn.best_estimator_\nclf_knn.score(X_test, y_test)\n\n0.8633333333333333\n\n\nThe resulted accuracy is shown above.\nWe then try it with the Decision Tree.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ngs_dt = GridSearchCV(DecisionTreeClassifier(), param_grid={'max_depth': list(range(1, 11)), 'max_leaf_nodes': list(range(10, 30))})\ngs_dt.fit(X_train, y_train)\nclf_dt = gs_dt.best_estimator_\nclf_dt.score(X_test, y_test)\n\n0.8606666666666667\n\n\nWe would also want to try Logistic regression method. This will be covered in the next Chapter. At current stage we just use the default setting without changing any hyperparameters.\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(X_train, y_train)\nclf_lr.score(X_test, y_test)\n\n0.8266666666666667\n\n\nNow we use a voting classifier to combine the results.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = [('knn', KNeighborsClassifier()),\n        ('dt', DecisionTreeClassifier()),\n        ('lr', LogisticRegression())]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n0.8346666666666667\n\n\nYou may compare the results of all these four classifiers. The voting classifier is not guaranteed to be better. It is just a way to form a model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#adaboost",
    "href": "contents/4/intro.html#adaboost",
    "title": "4  Ensemble methods",
    "section": "4.3 AdaBoost",
    "text": "4.3 AdaBoost\nThis is the first algorithm that successfully implements the boosting idea. AdaBoost is short for Adaptive Boosting.\n\n4.3.1 Weighted dataset\nWe firstly talk about training a Decision Tree on a weighted dataset. The idea is very simple. When building a Decision Tree, we use some method to determine the split. In this course the Gini impurity is used. There are at least two other methods: cross-entropy and misclassified rate. For all three, the count of the elemnts in some classes is the essnetial part. To train the model over the weighted dataset, we just need to upgrade the count of the elements by the weighted count.\n\nExample 4.1 Consider the following data:\n\n\n\n\n\n\n\nx0\nx1\ny\nWeight\n\n\n\n\n1.0\n2.1\n+\n0.5\n\n\n1.0\n1.1\n+\n0.125\n\n\n1.3\n1.0\n-\n0.125\n\n\n1.0\n1.0\n-\n0.125\n\n\n2.0\n1.0\n+\n0.125\n\n\n\n\n\nThe weighted Gini impurity is\n\\[\n\\text{WeightedGini}=1-(0.5+0.125+0.125)^2-(0.125+0.125)^2=0.375.\n\\]\nYou may see that the original Gini impurity is just the weighted Gini impurity with equal weights. Therefore the first tree we get from AdaBoost (see below) is the same tree we get from the Decision Tree model in Chpater 3.\n\n\n\n4.3.2 General process\nHere is the rough description of AdaBoost.\n\nAssign weights to each data point. At the begining we could assign weights equally.\nTrain a classifier based on the weighted dataset, and use it to predict on the training set. Find out all wrong answers.\nAdjust the weights, by inceasing the weights of data points that are done wrongly in the previous generation.\nTrain a new classifier using the new weighted dataset. Predict on the training set and record the wrong answers.\nRepeat the above process to get many classifiers. The training stops either by hitting \\(0\\) error rate, or after a specific number of rounds.\nThe final results is based on the weighted total votes from all classifiers we trained.\n\nNow let us talk about the details. Assume there are \\(N\\) data points. Then the inital weights are set to be \\(\\dfrac1N\\). There are 2 sets of weights. Let \\(w^{(i)}\\) be weights of the \\(i\\)th data points. Let \\(\\alpha_j\\) be the weights of the \\(j\\)th classifier. After training the \\(j\\)th classifier, the error rate is denoted by \\(e_j\\). Then we have\n\\[\ne_j=\\frac{\\text{the total weights of data points that are misclassified by the $j$th classifier}}{\\text{the total weights of data points}}\n\\]\n\\[\n\\alpha_j=\\eta\\ln\\left(\\dfrac{1-e_j}{e_j}\\right).\n\\]\n\\[\nw^{(i)}_{\\text{new}}\\leftarrow\\text{normalization} \\leftarrow w^{(i)}\\leftarrow\\begin{cases}w^{(i)}&\\text{if the $i$th data is correctly classified,}\\\\w^{(i)}\\exp(\\alpha_j)&\\text{if the $i$th data is misclassified.}\\end{cases}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe first tree is the same tree we get from the regular Decision Tree model. In the rest of the training process, more weights are put on the data that we are wrong in the previous iteration. Therefore the process is the mimic of “learning from mistakes”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe \\(\\eta\\) in computing \\(\\alpha_j\\) is called the learning rate. It is a hyperparameter that will be specified mannually. It does exactly what it appears to do: alter the weights of each classifier. The default is 1.0. When the number is very small (which is recommended although it can be any positive number), more iterations will be expected.\n\n\n\n\n4.3.3 Example 1: the iris dataset\nSimilar to all previous models, sklearn provides AdaBoostClassifier. The way to use it is similar to previous models. Note that although we are able to use any classifiers for AdaBoost, the most popular choice is Decision Tree with max_depth=1. This type of Decision Trees are also called Decision Stumps.\nIn the following examples, we initialize an AdaBoostClassifier with 500 Decision Stumps and learning_rate=0.5.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=1000,\n                             learning_rate=.5)\n\nWe will use the iris dataset for illustration. The cross_val_score is calculated as follows.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nscores = cross_val_score(ada_clf, X, y, cv=5)\nscores.mean()\n\n0.9533333333333334\n\n\n\n\n4.3.4 Example 2: the Horse Colic dataset\nThis dataset is from UCI Machine Learning Repository. The data is about whether horses survive if they get a disease called Colic. The dataset is preprocessed as follows. Note that there are a few missing values inside, and we replace them with 0.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\ndf = df.fillna(0)\nX = df.iloc[:, 1:].to_numpy().astype(float)\ny = df[0].to_numpy().astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nclf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.2)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.6222222222222222",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#exercises",
    "href": "contents/4/intro.html#exercises",
    "title": "4  Ensemble methods",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\nExercise 4.1 CHOOSE ONE: Please apply the random forest to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the cross-validation score and the OOB score of your model and compare it with the models you learned before (kNN, Decision Trees).\nPlease find some typical features (using the Gini importance) and draw the Decision Boundary against the features you choose.\n\n\n\nExercise 4.2 Please use the following code to get the mgq dataset.\n\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300,\n                                 n_features=2, n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, -y2 + 1))\n\nPlease build an AdaBoost model.\n\n\nExercise 4.3 Please use RandomForestClassifier, ExtraTreesClassifier and KNeighbourClassifier to form a voting classifier, and apply to the MNIST dataset.\n\n\n\n\n\n\n\nMNIST\n\n\n\nThis dataset can be loaded using the following code.\n\nimport numpy as np\nimport requests\nfrom io import BytesIO\nr = requests.get('https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz', stream = True) \ndata = np.load(BytesIO(r.raw.read()))\nX_train = data['x_train']\nX_test = data['x_test']\ny_train = data['y_train']\ny_test = data['y_test']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  }
]