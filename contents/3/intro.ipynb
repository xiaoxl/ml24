{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees\n",
        "\n",
        "\n",
        "<!-- ## Naive ideas -->\n",
        "\n",
        "Given a dataset with labels, the decision tree algorithm firstly trys to split the whole dataset into two different groups, based on some speicific features. Choose which feature to use and set the threshold for the split are done.\n",
        "\n",
        "\n",
        "\n",
        "## Gini impurity\n",
        "\n",
        "To split a dataset, we need a metric to tell whether the split is good or not. The two most popular metrics that are used are Gini impurity and Entropy. The two metrics don't have essential differences, that the results obtained by applying each metric are very similar to each other. Therefore we will only focus on Gini impurity since it is slightly easier to compute and slightly easier to explain.\n",
        "\n",
        "### Motivation and Definition\n",
        "Assume that we have a dataset of totally $n$ objects, and these objects are divided into $k$ classes. The $i$-th class has $n_i$ objects. Then if we randomly pick an object, the probability to get an object belonging to the $i$-th class is\n",
        "\n",
        "$$\n",
        "p_i=\\frac{n_i}{n}\n",
        "$$\n",
        "\n",
        "If we then guess the class of the object purely based on the distribution of each class, the probability that our guess is incorrect is \n",
        "\n",
        "$$\n",
        "1-p_i = 1-\\frac{n_i}{n}.\n",
        "$$\n",
        "\n",
        "Therefore, if we randomly pick an object that belongs to the $i$-th class and randomly guess its class purely based on the distribution but our guess is wrong, the probability that such a thing happens is \n",
        "\n",
        "$$\n",
        "p_i(1-p_i).\n",
        "$$\n",
        "\n",
        "Consider all classes. The probability at which any object of the dataset will be mislabelled when it is randomly labeled is the sum of the probability described above:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2.\n",
        "$$\n",
        "\n",
        "This is the definition formula for the *Gini impurity*. \n",
        "\n",
        "\n",
        "\n",
        "::: {#def-gini}\n",
        "The **Gini impurity** is calculated using the following formula\n",
        "\n",
        "$$\n",
        "Gini = \\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2,\n",
        "$$\n",
        "where $p_i$ is the probability of class $i$.\n",
        ":::\n",
        "\n",
        "The way to understand Gini impurity is to consider some extreme examples. \n",
        "\n",
        "\n",
        "::: {#exm-}\n",
        "\n",
        "Assume that we only have one class. Therefore $k=1$, and $p_1=1$. Then the Gini impurity is\n",
        "\n",
        "$$\n",
        "Gini = 1-1^2=0.\n",
        "$$\n",
        "This is the minimum possible Gini impurity. It means that the dataset is **pure**: all the objects contained are of one unique class. In this case, we won't make any mistakes if we randomly guess the label.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: {#exm-}\n",
        "Assume that we have two classes. Therefore $k=2$. Consider the distribution $p_1$ and $p_2$. We know that $p_1+p_2=1$. Therefore $p_2=1-p_1$. Then the Gini impurity is\n",
        "\n",
        "$$\n",
        "Gini(p_1) = 1-p_1^2-p_2^2=1-p_1^2-(1-p_1)^2=2p_1-2p_1^2.\n",
        "$$\n",
        "When $0\\leq p_1\\leq 1$, this function $Gini(p_1)$ is between $0$ and $0.5$. \n",
        "- It gets $0$ when $p_1=0$ or $1$. In these two cases, the dataset is still a one-class set since the size of one class is $0$. \n",
        "- It gets $0.5$ when $p_1=0.5$. This means that the Gini impurity is maximized when the size of different classes are balanced.\n",
        ":::\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "# Algorithm: Gini impurity\n",
        "\n",
        "**Inputs** A dataset $S=\\{data=[features, label]\\}$ with labels. \n",
        "\n",
        "**Outputs** The Gini impurity of the dataset.\n",
        "\n",
        "1. Get the size $n$ of the dataset.\n",
        "2. Go through the label list, and find all unique labels: $uniqueLabelList$.\n",
        "3. Go through each label $l$ in $uniqueLabelList$ and count how many elements belonging to the label, and record them as $n_l$.\n",
        "4. Use the formula to compute the Gini impurity:\n",
        "\n",
        "   $$\n",
        "    Gini = 1-\\sum_{l\\in uniqueLabelList}\\left(\\frac{n_l}{n}\\right)^2.\n",
        "   $$\n",
        ":::\n",
        "\n",
        "\n",
        "The sample codes are listed below:"
      ],
      "id": "b8933846"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "def gini(S):\n",
        "    N = len(S)\n",
        "    y = S[:, -1].reshape(N)\n",
        "    gini = 1 - ((pd.Series(y).value_counts()/N)**2).sum()\n",
        "    return gini"
      ],
      "id": "d48785dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CART Algorithms \n",
        "\n",
        "### Ideas\n",
        "Consider a labeled dataset $S$ with totally $m$ elements. We use a feature $k$ and a threshold $t_k$ to split it into two subsets: $S_l$ with $m_l$ elements and $S_r$ with $m_r$ elements. Then the cost function of this split is\n",
        "\n",
        "$$\n",
        "J(k, t_k)=\\frac{m_l}mGini(S_l)+\\frac{m_r}{m}Gini(S_r).\n",
        "$$\n",
        "It is not hard to see that the more pure the two subsets are the lower the cost function is. Therefore our goal is find a split that can minimize the cost function.\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "# Algorithm: Split the Dataset\n",
        "\n",
        "**Inputs** Given a labeled dataset $S=\\{[features, label]\\}$.\n",
        "\n",
        "**Outputs** A best split $(k, t_k)$.\n",
        "\n",
        "1. For each feature $k$:\n",
        "    1. For each value $t$ of the feature:\n",
        "        1. Split the dataset $S$ into two subsets, one with $k\\leq t$ and one with $k>t$.\n",
        "        2. Compute the cost function $J(k,t)$. \n",
        "        3. Compare it with the current smallest cost. If this split has smaller cost, replace the current smallest cost and pair with $(k, t)$.\n",
        "2. Return the pair $(k,t_k)$ that has the smallest cost function.\n",
        ":::\n",
        "\n",
        "\n",
        "We then use this split algorithm recursively to get the decision tree.\n",
        "\n",
        ":::{.callout-note}\n",
        "# Classification and Regression Tree, CART\n",
        "\n",
        "**Inputs** Given a labeled dataset $S=\\{[features, label]\\}$ and a maximal depth `max_depth`.\n",
        "\n",
        "**Outputs** A decision tree.\n",
        "\n",
        "1. Starting from the original dataset $S$. Set the working dataset $G=S$.\n",
        "2. Consider a dataset $G$. If $Gini(G)\\neq0$, split $G$ into $G_l$ and $G_r$ to minimize the cost function. Record the split pair $(k, t_k)$.\n",
        "3. Now set the working dataset $G=G_l$ and $G=G_r$ respectively, and apply the above two steps to each of them.\n",
        "4. Repeat the above steps, until `max_depth` is reached.\n",
        ":::\n",
        "\n",
        "Here are the sample codes."
      ],
      "id": "b0d716ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def split(G):\n",
        "    m = G.shape[0]\n",
        "    gmini = gini(G)\n",
        "    pair = None\n",
        "    if gini(G) != 0:\n",
        "        numOffeatures = G.shape[1] - 1\n",
        "        for k in range(numOffeatures):\n",
        "            for t in range(m):\n",
        "                Gl = G[G[:, k] <= G[t, k]]\n",
        "                Gr = G[G[:, k] > G[t, k]]\n",
        "                gl = gini(Gl)\n",
        "                gr = gini(Gr)\n",
        "                ml = Gl.shape[0]\n",
        "                mr = Gr.shape[0]\n",
        "                g = gl*ml/m + gr*mr/m\n",
        "                if g < gmini:\n",
        "                    gmini = g\n",
        "                    pair = (k, G[t, k])\n",
        "                    Glm = Gl\n",
        "                    Grm = Gr\n",
        "        res = {'split': True,\n",
        "               'pair': pair,\n",
        "               'sets': (Glm, Grm)}\n",
        "    else:\n",
        "        res = {'split': False,\n",
        "               'pair': pair,\n",
        "               'sets': G}\n",
        "    return res"
      ],
      "id": "ade54f07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the purpose of counting labels, we also write a code to do so."
      ],
      "id": "c3109710"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "def countlabels(S):\n",
        "    y = S[:, -1].reshape(S.shape[0])\n",
        "    labelCount = dict(pd.Series(y).value_counts())\n",
        "    return labelCount"
      ],
      "id": "7cc3b697",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Tree Project 1: the `iris` dataset\n",
        "\n",
        "We are going to use the Decision Tree model to study the `iris` dataset. This dataset has already studied previously using k-NN. Again we will only use the first two features for visualization purpose.\n",
        "\n",
        "### Initial setup\n",
        "\n",
        "Since the dataset will be splitted, we will put `X` and `y` together as a single variable `S`. In this case when we split the dataset by selecting rows, the features and the labels are still paired correctly. \n",
        "\n",
        "We also print the labels and the feature names for our convenience.\n"
      ],
      "id": "9db41195"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "from assests.codes.dt import gini, split, countlabels\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:]\n",
        "y = iris.target\n",
        "y = y.reshape((y.shape[0],1))\n",
        "S = np.concatenate([X,y], axis=1)\n",
        "\n",
        "print(iris.target_names)\n",
        "print(iris.feature_names)"
      ],
      "id": "a67eaf80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply CART manually\n",
        "\n",
        "We apply `split` to the dataset `S`. \n"
      ],
      "id": "35b38804"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "r = split(S)\n",
        "if r['split'] is True:\n",
        "    Gl, Gr = r['sets']\n",
        "    print(r['pair'])\n",
        "    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gl)),\n",
        "          ' and its label counts is {d:}'.format(d=countlabels(Gl)))\n",
        "    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gr)),\n",
        "          ' and its label counts is {d}'.format(d=countlabels(Gr)))"
      ],
      "id": "e0447fd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results shows that `S` is splitted into two subsets based on the `0`-th feature and the split value is `1.9`. \n",
        "\n",
        "The left subset is already pure since its Gini impurity is `0`. All elements in the left subset is label `0` (which is `setosa`). The right one is mixed since its Gini impurity is `0.5`. Therefore we need to apply `split` again to the right subset.\n"
      ],
      "id": "4138af2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "r = split(Gr)\n",
        "if r['split'] is True:\n",
        "    Grl, Grr = r['sets']\n",
        "    print(r['pair'])\n",
        "    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grl)),\n",
        "          ' and its label counts is {d:}'.format(d=countlabels(Grl)))\n",
        "    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grr)),\n",
        "          ' and its label counts is {d}'.format(d=countlabels(Grr)))"
      ],
      "id": "8b9bd6a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time the subset is splitted into two more subsets based on the `1`-st feature and the split value is `1.7`. The total Gini impurity is minimized using this split. \n",
        "\n",
        "The decision we created so far can be described as follows:\n",
        "\n",
        "1. Check the first feature `sepal length (cm)` to see whether it is smaller or equal to `1.9`.\n",
        "   1. If it is, classify it as lable `0` which is `setosa`.\n",
        "   2. If not, continue to the next stage.\n",
        "2. Check the second feature `sepal width (cm)` to see whether it is smaller or equal to `1.7`. \n",
        "   1. If it is, classify it as label `1` which is `versicolor`.\n",
        "   2. If not, classify it as label `2` which is `virginica`.\n",
        "\n",
        "### Use package `sklearn`\n",
        "\n",
        "Now we would like to use the decision tree package provided by `sklearn`. The process is straightforward. The parameter `random_state=40` will be discussed {ref}`later<note-random_state>`, and it is not necessary in most cases.\n"
      ],
      "id": "3403c678"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(max_depth=2, random_state=40)\n",
        "clf.fit(X, y)"
      ],
      "id": "e3dfebe3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`sklearn` provide a way to automatically generate the tree view of the decision tree. The code is as follows. \n"
      ],
      "id": "69f13787"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(2, 2), dpi=200)\n",
        "tree.plot_tree(clf, filled=True, impurity=True)"
      ],
      "id": "39cae66b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to k-NN, we may use `sklearn.inspection.DecisionBoundaryDisplay` to visualize the decision boundary of this decision tree.\n"
      ],
      "id": "cf114924"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    X,\n",
        "    cmap='coolwarm',\n",
        "    response_method=\"predict\",\n",
        "    xlabel=iris.feature_names[0],\n",
        "    ylabel=iris.feature_names[1],\n",
        ")\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=15)"
      ],
      "id": "5437a158",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze the differences between the two methods\n",
        "The tree generated by `sklearn` and the tree we got manually is a little bit different. Let us explore the differences here. \n",
        "\n",
        "To make it easier to split the set, we could convert the `numpy.ndarray` to `pandas.DataFrame`.\n"
      ],
      "id": "875a0aac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(X)\n",
        "df.head()"
      ],
      "id": "b25632c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now based on our tree, we would like to get all data points that the first feature (which is marked as `0`) is smaller or equal to `1.9`. We save it as `df1`. Similarly based on the tree gotten from `sklearn`, we would like to get all data points taht the second feature (which is marked as `1`) is smaller or equal to `0.8` and save it to `df2`. \n"
      ],
      "id": "2bf9647a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df1 = df[df[0]<=1.9]\n",
        "df2 = df[df[1]<=0.8]"
      ],
      "id": "2f40a991",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we would like to compare these two dataframes. What we want is to see whether they are the same regardless the order. One way to do this is to sort the two dataframes and then compare them directly.\n",
        "\n",
        "To sort the dataframe we use the method `DataFrame.sort_values`. The details can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html). Note that after `sort_values` we apply `reset_index` to reset the index just in case the index is massed by the sort operation.\n",
        "\n",
        "Then we use `DataFrame.equals` to check whether they are the same.\n"
      ],
      "id": "ac904bfa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df1sorted = df1.sort_values(by=df1.columns.tolist()).reset_index(drop=True)\n",
        "df2sorted = df2.sort_values(by=df2.columns.tolist()).reset_index(drop=True)\n",
        "print(df1sorted.equals(df2sorted))"
      ],
      "id": "353a03d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So these two sets are really the same. The reason this happens can be seen from the following two graphs.\n",
        "\n",
        "\n",
        ":::{layout-ncol=2}\n",
        "\n",
        "\n",
        "![From our code](assests/img/20220809120531.png)\n",
        "\n",
        "![From `sklearn`](assests/img/20220809122643.png)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "So you can see that either way can give us the same classification. This means that given one dataset the construction of the decision tree might be random at some points.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "# note-random_state\n",
        "Since the split is random, when using `sklearn.DecisionTreeClassifier` to construct decision trees, sometimes we might get the same tree as what we get from our naive codes. \n",
        "\n",
        "To illustrate this phenomenaon I need to set the random state in case it will generate the same tree as ours when I need it to generate a different tree. The parameter `random_state=40` mentioned before is for this purpose.\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "Another difference is the split value of the second branch. In our case it is `1.7` and in `sklearn` case it is `1.75`. So after we get the right subset from the first split (which is called `dfr`), we would split it into two sets based on whether the second feature is above or below `1.7`.\n"
      ],
      "id": "af93205e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dfr = df[df[0]>1.9]\n",
        "df2a = dfr[dfr[1]>1.7]\n",
        "df2b = dfr[dfr[1]<=1.7]\n",
        "print(df2b[1].max())\n",
        "print(df2a[1].min())"
      ],
      "id": "13bd36c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can see where the split number comes from. In our code, when we found a split, we will directly use that number as the cut. In this case it is `1.7`. \n",
        "\n",
        "In `sklearn`, when it finds a split, the algorithm will go for the middle of the gap as the cut. In this case it is `(1.7+1.8)/2=1.75`. \n",
        "\n",
        "## Decision Tree Project 2: `make_moons` dataset\n",
        "\n",
        "`sklearn` includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. We are going to use `make_moons` in this section. More details can be found [here](https://scikit-learn.org/stable/datasets/sample_generators.html).\n",
        "\n",
        "`make_moons` generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. `make_moons` produces two interleaving half circles. It is useful for visualization. \n",
        "\n",
        "Let us explorer the dataset first.\n"
      ],
      "id": "ff9dd951"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
        "plt.scatter(x=X[:, 0], y=X[:, 1], c=y)"
      ],
      "id": "251c912e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are applying `sklearn.DecisionTreeClassifier` to construct the decision tree. The steps are as follows.\n",
        "\n",
        "1. Split the dataset into training data and test data. \n",
        "2. Construct the pipeline. Since we won't apply any transformers there for this problem, we may just use the classifier `sklearn.DecisionTreeClassifier` directly without really construct the pipeline object.\n",
        "3. Consider the hyperparameter space for grid search. For this problme we choose `min_samples_split` and `max_leaf_nodes` as the hyperparameters we need. We will let `min_samples_split` run through 2 to 5, and `max_leaf_nodes` run through 2 to 50. We will use `grid_search_cv` to find the best hyperparameter for our model. For cross-validation, the number of split is set to be `3` which means that we will run trainning 3 times for each pair of hyperparameters.\n",
        "4. Run `grid_search_cv`. Find the best hyperparameters and the best estimator. Test it on the test set to get the accuracy score.\n"
      ],
      "id": "27a16a51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "85477f33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "params = {'min_samples_split': list(range(2, 5)),\n",
        "          'max_leaf_nodes': list(range(2, 50))}\n",
        "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), \n",
        "                              params, verbose=1, cv=3)\n",
        "grid_search_cv.fit(X_train, y_train)"
      ],
      "id": "830b11fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = grid_search_cv.best_estimator_\n",
        "print(grid_search_cv.best_params_)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "id": "627c35c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can see that for this `make_moons` dataset, the best decision tree should have at most `17` leaf nodes and the minimum number of samples required to be at a leaft node is `2`. The fitted decision tree can get 86.95% accuracy on the test set. \n",
        "\n",
        "Now we can plot the decision tree and the decision surface.\n"
      ],
      "id": "fef43d73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import tree\n",
        "plt.figure(figsize=(15, 15), dpi=300)\n",
        "tree.plot_tree(clf, filled=True)"
      ],
      "id": "6486e73e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    X,\n",
        "    cmap=plt.cm.RdYlBu,\n",
        "    response_method=\"predict\"\n",
        ")\n",
        "plt.scatter(\n",
        "    X[:, 0],\n",
        "    X[:, 1],\n",
        "    c=y,\n",
        "    cmap='gray',\n",
        "    edgecolor=\"black\",\n",
        "    s=15,\n",
        "    alpha=.15)"
      ],
      "id": "cc9c8e5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since it is not very clear what the boundary looks like, I will draw the decision surface individually below."
      ],
      "id": "54323f48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    X,\n",
        "    cmap=plt.cm.RdYlBu,\n",
        "    response_method=\"predict\"\n",
        ")"
      ],
      "id": "72d18513",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises and Projects\n",
        "\n",
        "\n",
        "::: {#exr-}\n",
        "The dataset and its scattering plot is given below.\n",
        "\n",
        "1. Please calculate the Gini impurity of the whole set by hand.\n",
        "2. Please apply CART to create the decision tree by hand. \n",
        "3. Please use the tree you created to classify the following points:\n",
        "    - $(0.4, 1.0)$\n",
        "    - $(0.6, 1.0)$\n",
        "    - $(0.6, 0)$\n",
        "\n",
        "The following code is for ploting. You may also get the precise data points by reading the code. You don't need to write codes to solve the problem."
      ],
      "id": "d4bbf8d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "data = {'x0': [0.22, 0.37, 0.42, 0.45, 0.18, 0.20, 0.21, 0.23, 0.35, 0.58,\n",
        "               0.60, 0.61, 0.62, 0.65, 0.70, 0.75, 0.82, 0.88, 0.90, 0.92],\n",
        "        'x1': [0.83, 0.78, 0.65, 0.37, 0.57, 0.45, 0.67, 0.22, 0.43, 0.33,\n",
        "               0.75, 0.50, 0.21, 0.31, 0.64, 0.70, 0.80, 0.82, 0.61, 0.81],\n",
        "        'y': ['r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', \n",
        "              'b', 'b', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(df['x0'], df['x1'], c=df['y'])\n",
        "_ = plt.xlim(0, 1)\n",
        "_ = plt.ylim(0, 1)"
      ],
      "id": "8601c561",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: {#exr-}\n",
        "CHOOSE ONE: Please apply the Decision Tree to one of the following datasets. \n",
        "\n",
        "- dating dataset (in Chpater 2). \n",
        "- the `titanic` dataset.\n",
        "\n",
        "Please answer the following questions.\n",
        "\n",
        "1. Please use grid search to find the good `max_leaf_nodes` and `max_depth`.\n",
        "2. Please record the accuracy (or cross-validation score) of your model and compare it with the models you learned before (kNN). \n",
        "3. Please find the two most important features and explane your reason.\n",
        "4. (Optional) Use the two most important features to draw the Decision Boundary if possible.\n",
        ":::\n"
      ],
      "id": "53490f5f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ds24",
      "language": "python",
      "display_name": "ds24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}