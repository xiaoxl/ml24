## Rewrite in classes




```{python}
#| echo: false
import numpy as np
from sklearn.model_selection import train_test_split
import torch

RANDOMSEED = 42
np.random.seed(RANDOMSEED)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

X = np.random.rand(100)
y = 2.3 + 1.2 * X + np.random.randn(100) * 0.1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,
                                                    random_state=RANDOMSEED)
X_tensor_train = torch.as_tensor(X_train, device=device, dtype=torch.float)
y_tensor_train = torch.as_tensor(y_train, device=device, dtype=torch.float)
```

### Use class to describe the model

We now want to upgrade the code we wrote in previous sections in terms of classes, since it is a good way to wrap up our own code.


```{python}
import torch
import torch.nn as nn

class LR(nn.Module):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        self.b = nn.Parameter(torch.tensor(1, requires_grad=True, dtype=torch.float))
        self.w = nn.Parameter(torch.tensor(1.5, requires_grad=True, dtype=torch.float))

    def forward(self, x):
        return self.b + self.w * x

RANDOMSEED = 42
torch.manual_seed(RANDOMSEED)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = LR().to(device)
model.state_dict()
```

We could use `model.state_dict()` to look at the parameters of the model. Another way to see the parameters is to use `model.parameters()` method. The latter will return an iterator that help you go through all parameters.

```{python}
for item in model.parameters():
    print(item)
```


Now we reproduce the training code for `LR` class.


```{python}
from torch.optim import SGD

def loss_fn(yhat, y):
    return ((yhat-y)**2).mean()

lr = 0.2
optimizer = SGD(model.parameters(), lr=lr)

epoch_num = 10

for epoch in range(epoch_num):
    model.train()

    yhat = model(X_tensor_train)
    loss = loss_fn(yhat, y_tensor_train)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

model.state_dict()
```




### Model standard models

We hand write our models and set parameters in our previous versions. `PyTorch` provides many standard modules that we can use directly. For example, the linear regression model can be found in `nn.modules` as `Linear`, while our loss function is the mean square differene function which is `MSELoss` from `nn`.

Note that we apply `reshape` to the dataset in the following code. The reason is that the standard functions has strict requirements for the dimensions of the dataset. We don't follow the requirement when we prepare our dataset for simplicity. So we have to correct them at this point. It is better to follow the requirement since it is more general and more flexible in most cases.

```{python}
from torch.optim import SGD
from torch.nn.modules import Linear
from torch.nn import MSELoss

class BetterLR(nn.Module):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        self.linear = Linear(in_features=1, out_features=1)
        self.linear.bias = torch.nn.Parameter(torch.tensor([1.0], dtype=torch.float))
        self.linear.weight = torch.nn.Parameter(torch.tensor([[1.5]], dtype=torch.float))

    def forward(self, x):
        return self.linear(x)

lr = 0.2

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model2 = BetterLR().to(device)
optimizer2 = SGD(model2.parameters(), lr=lr)

epoch_num = 10

for epoch in range(epoch_num):
    model2.train()

    yhat = model2(X_tensor_train.reshape(-1, 1))
    loss2 = MSELoss(reduction='mean')(yhat, y_tensor_train.reshape(-1, 1))
    loss2.backward()
    optimizer2.step()
    optimizer2.zero_grad()

model2.state_dict()
```



::: {.callout-note}
# Initialize the parameters
In all our examples we initialize the parameters to be $(1, 1.5)$ for the purpose of comparision. In most cases, we don't manually set the intial values, but use random numbers. In this case, we simply delete the manual codes.
:::


### Create templetes


```{python}
#| eval: false

# def model
# def loss_fn
# def optimizer

epoch_num = 10

for epoch in range(epoch_num):
    model.train()
    yhat = model(X_train)
    loss = loss_fn(yhat, y_train)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```



### Dataloader {#sec-dataloader}
Usually we use a class to provide data. The class is based on `Dataset` class, and need to implement the constructor, `__getitem__` method and `__len__` method. Here is an example.


```{python}
from torch.utils.data import Dataset

class MyData(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __getitem__(self, index):
        return (self.x[index], self.y[index])

    def __len__(self):
        return len(self.y)

train_data = MyData(X_train, y_train)
train_data[1]
```


Then we use `Dataloader` to feed the data into our model.


```{python}
from torch.utils.data import DataLoader 

lr = 0.2
epoch_num = 10

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = BetterLR().to(device)
optimizer = SGD(model.parameters(), lr=lr)

train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)

for epoch in range(epoch_num):
    for X_batch, y_batch in train_loader:
        yhat = model(X_batch.reshape(-1, 1).float())
        loss = MSELoss(reduction='mean')(yhat, y_batch.reshape(-1, 1).float())
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

model.state_dict()
```

When applying mini-batch, usually we will shuffle the dataset. If we disable the shuffle here as well as the shuffle in `numpy` case, you will see that we get exactly the same answer.
