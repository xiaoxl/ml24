{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic regression \n",
        "\n",
        "\n",
        "Consider a set of training data $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots$, where $x^{(i)}=(x^{(i)}_1, x^{(i)}_2, \\ldots, x^{(i)}_n)$ is a $n$-dim vector, and $y^{(i)}$ is a real number. We would like to use Linear regression to find the relation between $x$ and $y$. \n",
        "\n",
        "In this case, we assume that $y$ is a linear function of $x$:\n",
        "\n",
        "$$\n",
        "y=\\theta_0 + \\sum_{j=1}^n\\theta_jx_j.\n",
        "$$\n",
        "The purpose of Linear regression is to used the given training data to find out the best $\\Theta=(\\theta_0, \\theta_1, \\theta_2,\\ldots,\\theta_n)$. \n",
        "\n",
        "If we set $\\hat{x}=(1, x_1, \\ldots,x_n)$, then the above formula can be reformulated by matrix multiplication.\n",
        "\n",
        "$$\n",
        "y=\\Theta \\hat{x}^T.\n",
        "$$\n",
        "\n",
        "When we want to deal with classification problem, we may still use this regression idea, but we have to do some modification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Basic idea\n",
        "The Logsitic regression is used to predict the probability of a data point belonging to a specific class. It is based on linear regression. The major difference is that logistic regreesion will have an activation function $\\sigma$ at the final stage to change the predicted values of the linear regression to the values that indicate classes. In the case of binary classification, the outcome of $\\sigma$ will be between $0$ and $1$, which is related to the two classes respectively. In this case, the number is interepted as the probability of the data to be in one of the specific class.\n",
        "\n",
        "The model for Logistic regression is as follows:\n",
        "\n",
        "$$\n",
        "p=\\sigma(L(x))=\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\sigma\\left(\\Theta \\hat{x}^T\\right).\n",
        "$$\n",
        "\n",
        "In most cases, this activation function is chosen to be the Sigmoid funciton.\n",
        "\n",
        "### Sigmoid function\n",
        "\n",
        "The *Sigmoid* function is defined as follows:\n",
        "\n",
        "$$\n",
        "\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n",
        "$$\n",
        "The graph of the function is shown below.\n"
      ],
      "id": "0d14e136"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(-6, 6, 1001)\n",
        "y = 1/(1+np.exp(-x))\n",
        "_ = plt.plot(x, y)"
      ],
      "id": "f24b918c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main properties of $\\sigma$ are listed below as a Lemma.\n",
        "\n",
        "\n",
        "::: {#lem-sig}\n",
        "\n",
        "The Sigmoid function $\\sigma(z)$ satisfies the following properties.\n",
        "\n",
        "1. $\\sigma(z)\\rightarrow \\infty$ when $z\\mapsto \\infty$.\n",
        "2. $\\sigma(z)\\rightarrow -\\infty$ when $z\\mapsto -\\infty$.\n",
        "3. $\\sigma(0)=0.5$.\n",
        "4. $\\sigma(z)$ is always increasing.\n",
        "5. $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "::: {.solution}\n",
        "We will only look at the last one.\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "\\sigma'(z)&=-\\frac{(1+\\mathrm e^{-z})'}{(1+\\mathrm e^{-z})^2}=\\frac{\\mathrm e^{-z}}{(1+\\mathrm e^{-z})^2}=\\frac{1}{1+\\mathrm e^{-z}}\\frac{\\mathrm e^{-z}}{1+\\mathrm e^{-z}}\\\\\n",
        "&=\\sigma(z)\\left(\\frac{1+\\mathrm e^{-z}}{1+\\mathrm e^{-z}}-\\frac{1}{1+\\mathrm e^{-z}}\\right)=\\sigma(z)(1-\\sigma(z)).\n",
        "\\end{split}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "### Gradient descent\n",
        "Assume that we would like to minimize a function $J(\\Theta)$, where this $\\Theta$ is an $N$-dim vector. Geometricly, we could treat $J$ as a height function, and it tells us the height of the mountain. Then to minimize $J$ is the same thing as to find the lowest point. One idea is to move towards the lowest point step by step. During each step we only need to lower our current height. After several steps we will be around the lowest point.\n",
        "\n",
        "The geometric meaning of $\\nabla J$ is the direction that $J$ increase the most. Therefore the opposite direction is the one we want to move in. The formula to update $x$ is \n",
        "\n",
        "$$\n",
        "\\Theta_{\\text{new}} = \\Theta_{\\text{old}}-\\alpha \\nabla J(\\Theta_{\\text{old}}),\n",
        "$$\n",
        "where $\\alpha$ is called the *learning rate* which controls how fast you want to learn. Usually if $\\alpha$ is small, the learning tends to be slow and stble, and when $\\alpha$ is big, the learning tends to be fast and unstable.\n",
        "\n",
        "In machine learning, in most cases we would like to formulate the problem in terms of finding the lowest point of a *cost function* $J(\\Theta)$. Then we could start to use Logistic regression to solve it. For binary classification problem, the cost function is defined to be\n",
        "\n",
        "$$\n",
        "J(\\Theta)=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\log(p^{(i)})+(1-y^{(i)})\\log(1-p^{(i)})\\right].\n",
        "$$\n",
        "Here $m$ is the number of data points, $y^{(i)}$ is the labelled result (which is either $0$ or $1$), $p^{(i)}$ is the predicted value (which is between $0$ and $1$). \n",
        "\n",
        "::: {.callout-note}\n",
        "The algorithm gets its name since we are using the gradient to find a direction to lower our height. \n",
        ":::\n",
        "\n",
        "\n",
        "### The Formulas\n",
        "\n",
        "\n",
        "::: {#thm-reggrad}\n",
        "The gradient of $J$ is computed by\n",
        "\n",
        "$$\n",
        "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n",
        "$$ {#eq-nablaJ}\n",
        ":::\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>Click for details.</summary>\n",
        "\n",
        "::: {.proof}\n",
        "\n",
        "\n",
        "The formula is an application of the chain rule for the multivariable functions.\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "\\dfrac{\\partial p}{\\partial \\theta_k}&=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma(L(\\Theta))\\\\\n",
        "&=\\sigma(L)(1-\\sigma(L))\\dfrac{\\partial}{\\partial \\theta_k}\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)\\\\\n",
        "&=\\begin{cases}\n",
        "p(1-p)&\\text{ if }k=0,\\\\\n",
        "p(1-p)x_k&\\text{ otherwise}.\n",
        "\\end{cases}\n",
        "\\end{split}\n",
        "$$\n",
        "Then \n",
        "\n",
        "$$\n",
        "\\nabla p = \\left(\\frac{\\partial p}{\\partial\\theta_0},\\ldots,\\frac{\\partial p}{\\partial\\theta_n}\\right) = p(1-p)\\hat{x}.\n",
        "$$\n",
        "\n",
        "Then \n",
        "\n",
        "$$\n",
        "\\nabla \\log(p) = \\frac{\\nabla p}p =\\frac{p(1-p)\\hat{x}}{p}=(1-p)\\hat{x}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\nabla \\log(1-p) = \\frac{-\\nabla p}{1-p} =-\\frac{p(1-p)\\hat{x}}{1-p}=-p\\hat{x}.\n",
        "$$\n",
        "\n",
        "Then \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "\\nabla J& = -\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\nabla \\log(p^{(i)})+(1-y^{(i)})\\nabla \\log(1-p^{(i)})\\right]\\\\\n",
        "&=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}(1-p^{(i)})\\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\\hat{x}^{(i)})\\right]\\\\\n",
        "&=-\\frac1m\\sum_{i=1}^m\\left[(y^{(i)}-p^{(i)})\\hat{x}^{(i)}\\right].\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We write $\\hat{x}^{(i)}$ as row vectors, and stack all these row vectors vertically. What we get is a matrix $\\hat{\\textbf X}$ of the size $m\\times (1+n)$. We stack all $y^{(i)}$ (resp. $p^{(i)}$) vectically to get the $m$-dim column vector $\\textbf y$ (resp. $\\textbf p$). \n",
        "\n",
        "Using this notation, the previous formula becomes\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n",
        "$$\n",
        "\n",
        "After the gradient can be computed, we can start to use the gradient descent method. Note that, although $\\Theta$ are not explicitly presented in the formula of $\\nabla J$, this is used to modify $\\Theta$:\n",
        "\n",
        "$$\n",
        "\\Theta_{s+1} = \\Theta_s - \\alpha\\nabla J.\n",
        "$$\n",
        "\n",
        ":::\n",
        "</details>\n",
        "\n",
        "\n",
        "### Codes\n",
        "We will only talk about using packages. `sklearn` provides two methods to implement the Logistic regression. The API interface is very similar to other models. \n",
        "\n",
        "Note that Logistic regression is very sensitive to the scale of features. Therefore we need to normalize the features before throwing them into the model.\n",
        "\n",
        "Let's still take `iris` as an example.\n"
      ],
      "id": "8ed79403"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
      ],
      "id": "26a77cee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first method is `sklearn.linear_model.LogisticRegression`. \n"
      ],
      "id": "edee827b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "steps = [('normalize', MinMaxScaler()),\n",
        "         ('log', LogisticRegression())]\n",
        "\n",
        "log_reg = Pipeline(steps=steps)\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_reg.score(X_test, y_test)"
      ],
      "id": "4c4b151a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this method has an option `solver` that will set the way to solve the Logistic regression problem, and there is no \"stochastic gradient descent\" provided. The default solver for this `LogsiticRegression` is `lbfgs` which will NOT be discussed in lectures.\n",
        "\n",
        "The second method is `sklearn.linear_model.SGDClassifier`.\n"
      ],
      "id": "e8b85c4a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "steps = [('normalize', MinMaxScaler()),\n",
        "         ('log', SGDClassifier(loss='log_loss', max_iter=100))]\n",
        "\n",
        "sgd_clf = Pipeline(steps=steps)\n",
        "sgd_clf.fit(X_train, y_train)\n",
        "sgd_clf.score(X_test, y_test)"
      ],
      "id": "0b4af438",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method is the one we discussed in lectures. The `log_loss` loss function is the binary entropy function we mentioned in lectures. If you change to other loss functions it will become other models.\n",
        "\n",
        "From the above example, you may notice that `SGDClassifier` doesn't perform as well as `LogisticRegression`. This is due to the algorithm. To make `SGDClassifier` better you need to tune the hyperparameters, like `max_iter`, `learning_rate`/`alpha`, `penalty`, etc..\n",
        "\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "The argument `warm_start` is used to set whether you want to use your previous model. When set to `True`, it will reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. The default is `False`.  \n",
        "\n",
        "Repeatedly calling `fit` when `warm_start` is `True` can result in a different solution than when calling `fit` a single time because of the way the data is shuffled. \n",
        ":::\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "Note that for both methods, regularization (which will be discussed later) is applied by default.\n",
        ":::\n",
        "\n",
        "\n",
        "### Several important side topics\n",
        "\n",
        "#### Epochs\n",
        "We use epoch to describe feeding data into the model. One *Epoch* is when an entire dataset is passed through the model once. When using gradient descent, we tend to run several epochs. The number of maximal epochs is one important hyperparameter of this model.\n",
        "\n",
        "The general idea is that more epochs are better for the score of the model, but it will definitely be slower. In addition, sometimes due to many other factors, after a few epochs, the model becomes stall. To train for more epochs cannot improve the model. In this case you have to turn to other methods.\n",
        "\n",
        "\n",
        "#### Batch Gradient Descent vs SGD vs Minibatch\n",
        "Recall the Formula @eq-nablaJ: \n",
        "\n",
        "$$\n",
        "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n",
        "$$\n",
        "We could rewrite this formula:\n",
        "\n",
        "$$\n",
        "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}=\\frac1m\\sum_{i=1}^m\\left[(p^{(i)}-y^{(i)})\\hat{x}^{(i)}\\right].\n",
        "$$\n",
        "This new formula can be understood in the following way: For every data point, we could get one gradient direction. Then $\\nabla J$ is the average of all gradient directions. So this algorithm can be expressed as that compute the gradient for every data points and then take the average, and finally update the parameters once. This algorithm is called *batch gradient descent*. \n",
        "\n",
        "\n",
        "Following the idea, there is another way to update the model. For every data point, we could compute one gradient direction, and we could use the gradient direction to update the parameters of the model. This algorithm is called *stochastic gradient descent*. \n",
        "\n",
        "Then there is an algrothm living in the middle, called *mini-batch gradient descent*. In this case, we will group the data set into a collection of subsets of a fiexed number of training examples. Each subset is called a *mini-batch*, and the fixed number of elements of each mini-batch is called the *batch size*. Using this method, we will just go through mini-batches one at a time, compute the average of the gradient for these data, and then update the parameters of the model after we finish one mini-batch. Assume that the total number of the dataset is `N`, the mini-batch size is `m`. Then there are `N/m` mini-batches, and during one epoch we will update the model `N/m` times.\n",
        "\n",
        "\n",
        "Mini-batch size is one important hyperparameters of this model. Usually the larger the batch size is, the less variance the model has. Then it tends to behave more smoothly, but it will also be slower, and might be stuck to a local minimal. The smaller batch size is more chaotic. It might go faster, but it tends not to converge.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Regularization\n",
        "\n",
        "### Three types of errors\n",
        "Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The **bias** of an estimator is its average error for different training sets. The **variance** of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data. \n",
        "\n",
        "\n",
        "### Underfit vs Overfit\n",
        "\n",
        "When fit a model to data, it is highly possible that the model is underfit or overfit. \n",
        "\n",
        "Roughly speaking, **underfit** means the model is not sufficient to fit the training samples, and **overfit** means that the models learns too many noise from the data. In many cases, high bias is related to underfit, and high variance is related to overfit.\n",
        "\n",
        "The following example is from [the `sklearn` guide](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py). Although it is a polynomial regression example, it grasps the key idea of underfit and overfit.\n"
      ],
      "id": "e2d87a82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| column: page\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline(\n",
        "        [\n",
        "            (\"polynomial_features\", polynomial_features),\n",
        "            (\"linear_regression\", linear_regression),\n",
        "        ]\n",
        "    )\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(\n",
        "        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\n",
        "    )\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\n",
        "        \"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
        "            degrees[i], -scores.mean(), scores.std()\n",
        "        )\n",
        "    )\n",
        "# plt.show()"
      ],
      "id": "aa19e685",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning curves (accuracy vs training size)\n",
        "\n",
        "A learning curve shows the validation and training score of an estimator for varying a key hyperparameter. In most cases the key hyperparameter is the training size or the number of epochs. It is a tool to find out how much we benefit from altering the hyperparameter by training more data or training for more epochs, and whether the estimator suffers more from a variance error or a bias error. \n",
        "\n",
        "`sklearn` provides `sklearn.model_selection.learning_curve()` to generate the values that are required to plot such a learning curve. However this function is just related to the sample size. If we would like to talk about epochs, we need other packages.\n",
        "\n",
        "Let us first look at the learning curve about sample size. The official document page is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html). The function takes input `estimator`, dataset `X`, `y`, and an arry-like argument `train_sizes`. The dataset `(X, y)` will be split into pieces using the cross-validation technique. The number of pieces is set by the argument `cv`. The default value is `cv=5`. For details about cross-validation please see @sec-cross-validation.\n",
        "\n",
        "Then the model is trained over a random sample of the training set, and evaluate the score over the test set. The size of the sample of the training set is set by the argument `train_sizes`. This argument is array-like. Therefore the process will be repeated several times, and we can see the impact of increasing the training size. \n",
        "\n",
        "The output contains three pieces. The first is `train_sizes_abs` which is the number of elements in each training set. This output is mainly for reference. The difference between the output and the input `train_sizes` is that the input can be float which represents the percentagy. The output is always the exact number of elements.\n",
        "\n",
        "The second output is `train_scores` and the third is `test_scores`, both of which are the scores we get from the training and testing process. Note that both are 2D `numpy` arrays, of the size `(number of different sizes, cv)`. Each row is a 1D `numpy` array representing the cross-validation scores, which is corresponding to a train size. If we want the mean as the cross-validation score, we could use `train_scores.mean(axis=1)`.\n",
        "\n",
        "After understanding the input and output, we could plot the learning curve. We still use the `horse colic` as the example. The details about the dataset can be found [here](https://xiaoxl.github.io/Datasets/contents/horse_colic.html).\n"
      ],
      "id": "7ca8c54e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\n",
        "df = pd.read_csv(url, delim_whitespace=True, header=None)\n",
        "df = df.replace(\"?\", np.NaN)\n",
        "\n",
        "df.fillna(0, inplace=True)\n",
        "df.drop(columns=[2, 24, 25, 26, 27], inplace=True)\n",
        "df[23].replace({1: 1, 2: 0}, inplace=True)\n",
        "X = df.iloc[:, :-1].to_numpy().astype(float)\n",
        "y = df[23].to_numpy().astype(int)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
      ],
      "id": "753355f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the model `LogisticRegression`. The following code plot the learning curve for this model.\n"
      ],
      "id": "8206bf9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "steps = [('scalar', MinMaxScaler()),\n",
        "         ('log', clf)]\n",
        "pipe = Pipeline(steps=steps)\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "train_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n",
        "                                                        train_sizes=np.linspace(0.1, 1, 20))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), label='train')\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), label='test')\n",
        "plt.legend()"
      ],
      "id": "b7c2ecf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learning curve is a primary tool for us to study the bias and variance. Usually\n",
        "\n",
        "- If the two training curve and the testing curve are very close to each other, this means that the variance is low. Otherwise the variance is high, and this means that the model probabily suffer from overfitting. \n",
        "- If the absolute training curve score is high, this means that the bias is low. Otherwise the bias is high, and this means that the model probabily suffer from underfitting.\n",
        "\n",
        "In the above example, although regularization is applied by default, you may still notice some overfitting there.\n",
        "\n",
        "\n",
        "### Regularization\n",
        "Regularization is a technique to deal with overfitting. Here we only talk about the simplest method: ridge regression, also known as Tikhonov regularizaiton. Because of the formula given below, it is also called *$L_2$ regularization*. The idea is to add an additional term $\\dfrac{\\alpha}{2m}\\sum_{i=1}^m\\theta_i^2$ to the original cost function. When training with the new cost function, this additional term will force the parameters in the original term to be as small as possible. After finishing training, the additional term will be dropped, and we use the original cost function for validation and testing. Note that in the additional term $\\theta_0$ is not presented.\n",
        "\n",
        "The hyperparameter $\\alpha$ is the *regularization strength*. If $\\alpha=0$, the new cost function becomes the original one; If $\\alpha$ is very large, the additional term dominates, and it will force all parameters to be almost $0$. In different context, the regularization strength is also given by $C=\\dfrac{1}{2\\alpha}$, called *inverse of regularization strength*.\n",
        "\n",
        "\n",
        "#### The math of regularization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: {#thm-ridgegrad}\n",
        "The gradient of the ridge regression cost function is\n",
        "\n",
        "$$\n",
        "\\nabla J=\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}+\\frac{\\alpha}{m}\\Theta.\n",
        "$$\n",
        "\n",
        "Note that $\\Theta$ doesn't contain $\\theta_0$, or you may treat $\\theta_0=0$.\n",
        "\n",
        ":::\n",
        "\n",
        "The computation is straightforward.\n",
        "\n",
        "#### The code\n",
        "\n",
        "Regularization is directly provided by the logistic regression functions.\n",
        "\n",
        "- In `LogisticRegression`, the regularization is given by the argument `penalty` and `C`. `penalty` specifies the regularizaiton method. It is `l2` by default, which is the method above. `C` is the inverse of regularization strength, whose default value is `1`.\n",
        "- In `SGDClassifier`, the regularization is given by the argument `penalty` and `alpha`. `penalty` is the same as that in `LogisticRegression`, and `alpha` is the regularization strength, whose default value is `0.0001`.\n",
        "\n",
        "Let us see the above example.\n"
      ],
      "id": "7b8924cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf = LogisticRegression(max_iter=1000, C=0.1)\n",
        "steps = [('scalar', MinMaxScaler()),\n",
        "         ('log', clf)]\n",
        "pipe = Pipeline(steps=steps)\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "train_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n",
        "                                                        train_sizes=np.linspace(0.1, 1, 20))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), label='train')\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), label='test')\n",
        "plt.legend()"
      ],
      "id": "b124473f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we reduce `C` from `1` to `0.1`, the regularization strength is increased. Then you may find that the gap between the two curves are reduced. However the overall performace is also reduced, from 85%~90% in `C=1` case to around 80% in `C=0.1` case. This means that the model doesn't fit the data well as the previous one. Therefore this is a trade-off: decrease the variance but increase the bias.\n",
        "\n",
        "## Neural network implement of Logistic regression\n",
        "In the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. But we didn't implement them. In fact `sklearn` doesn't provide a very good tool to do all these computations. Hence we turn to another package for this model. We are going to use `keras` to build a Logistic regression model, and plot the \"loss vs epochs\" learning curves.\n",
        "\n",
        "`keras` is high level Neural network library. It is now in the phase of transition from single backend `tensorflow` to multi-backend. The old version is installed along with `tensorflow`. You may use the following command to install it. \n",
        "\n",
        "```{.bash}\n",
        "pip install tensorflow\n",
        "```\n",
        "If you want to use new version, besides installing `tensorflow`, you should also install `keras-core`. Currently we still use `tensorflow` as the backend. This part may be updated after the stable version is released.\n",
        "\n",
        "```{.bash}\n",
        "pip install keras-core\n",
        "```\n",
        "You may follow the instructions for [`tensforflow`](https://www.tensorflow.org/install) and [`keras`](https://github.com/keras-team/keras) for more details.\n",
        "\n",
        "\n",
        "\n",
        "To use `keras` to implement logistic regression, we need the following modules: a `Sequential` model, a `Dense` layer. The model is organized as follows.\n",
        "\n",
        "We still use the horse colic dataset as an example.\n"
      ],
      "id": "67b4e9f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\n",
        "df = pd.read_csv(url, delim_whitespace=True, header=None)\n",
        "df = df.replace(\"?\", np.NaN)\n",
        "\n",
        "df.fillna(0, inplace=True)\n",
        "df.drop(columns=[2, 24, 25, 26, 27], inplace=True)\n",
        "df[23].replace({1: 1, 2: 0}, inplace=True)\n",
        "X = df.iloc[:, :-1].to_numpy().astype(float)\n",
        "y = df[23].to_numpy().astype(int)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
      ],
      "id": "7a53be21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we need to perform normalization before throwing the data into the model. Here we use the `MinMaxScaler()` from `sklearn` package. The normalization layer in keras is a little bit more complicated and doesn't fit into situation.\n"
      ],
      "id": "4eab02a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "mms.fit(X_train)\n",
        "X_train = mms.transform(X_train)\n",
        "X_test = mms.transform(X_test)"
      ],
      "id": "b0a06c3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code, we first set up the model, and then add one `Dense` layer. This `Dense` layer means that we would perform a linear transformation to the input, by the formula $\\theta_0+\\theta_1x_1+\\theta_2x_2+\\ldots+\\theta_nx_n$. Then there are three arguments:\n",
        "\n",
        "- `1`: means that there is only output.\n",
        "- `activation='sigmoid'`: means that we will apply the `sigmoid` function after the linear transformation.\n",
        "- `input_dim`: means the dimension of the input. Note that this dimension is the dimension of one individual data point. You don't take the size of the training set into consideration.\n",
        "\n",
        "After building the basic architectal of the model, we need to speicify a few more arguments. In the `model.compile()` step, we have to input the `optimizer`, the loss function (which is the `binary_crossentropy` in our case) and the metrics to test the performance of the model (which is accuracy in our case).\n",
        "\n",
        "The `optimizer` is how the parameters are updated. The best choice in general is `adam`. The default setting is `RMSprop` and the optimizer discussed in our lecture is `sgd`. We will use `adam` here, since the learning curve it produces looks better (for illustration).\n",
        "\n",
        "Finally we could train the model. The argument is straightforward.\n"
      ],
      "id": "5f183c71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "import keras_core as keras\n",
        "from keras import models, layers\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid', input_dim=X_train.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit(X_train, y_train, epochs=400, batch_size=30, validation_data=(X_test, y_test))"
      ],
      "id": "89166123",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we assign the output of `model.fit()` to a variable `hist`. The infomation about this training process is recorded inside. We will extract those information.\n"
      ],
      "id": "2ea67fbf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_train = hist.history['loss']\n",
        "loss_val = hist.history['val_loss']\n",
        "\n",
        "acc_train = hist.history['accuracy']\n",
        "acc_val = hist.history['val_accuracy']"
      ],
      "id": "03973d4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now could plot the learning curve (loss vs epochs) and the learning curve (accuracy vs epochs).\n"
      ],
      "id": "21edbfb5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_train, label='train_loss')\n",
        "plt.plot(loss_val, label='val_loss')\n",
        "plt.legend()"
      ],
      "id": "bdc4ec69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(acc_train, label='train_acc')\n",
        "plt.plot(acc_val, label='val_acc')\n",
        "plt.legend()"
      ],
      "id": "2d1d7b18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularization\n",
        "To apply regularization, we just need to modify the layer we added to the model. The argument is `kernel_regularizer`. We would like to set it to be `keras.regularizers.L2(alpha)`, where `alpha` is the regularization strength.\n"
      ],
      "id": "0eaacbd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "from keras import models, layers, regularizers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid', input_dim=X_train.shape[1],\n",
        "                       kernel_regularizer=regularizers.L2(0.5)))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit(X_train, y_train, epochs=400, batch_size=30,\n",
        "                 validation_data=(X_test, y_test))"
      ],
      "id": "3a20c3d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_train = hist.history['loss']\n",
        "loss_val = hist.history['val_loss']\n",
        "\n",
        "acc_train = hist.history['accuracy']\n",
        "acc_val = hist.history['val_accuracy']\n",
        "\n",
        "plt.plot(loss_train, label='train_loss')\n",
        "plt.plot(loss_val, label='val_loss')\n",
        "plt.legend()"
      ],
      "id": "f2d2007a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(acc_train, label='train_acc')\n",
        "plt.plot(acc_val, label='val_acc')\n",
        "plt.legend()"
      ],
      "id": "51bfd734",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may compare what we get here with the codes we get before.\n",
        "\n",
        "## Multi class case\n",
        "\n",
        "### Naive idea (one-vs-all)\n",
        "Assume that there are $N$ classes. The naive idea is to decompose this $N$-class classification problem into $N$ binary classification problems. The model will contains $N$ classifiers. The $i$th classifer is used to classify whehter the given features has the label `i` or not.\n",
        "\n",
        "For example, asusme we are dealing with the `iris` dataset. There are 3 classes. By the naive idea, we will modify the labels into `Setosa` and `not Setosa`, and use it to train the first classifier. Similarly we can have two more classifiers to tell `Versicolour`/`not Versicolour` and `Virginica`/`not Virginica`. Then we combine all three classifiers to get a final classifier to put the data into one of the three classes.\n",
        "\n",
        "\n",
        "### `Softmax` function\n",
        "A better method is mimic the sigmoid function. Recall that in binary classification problem, after \n",
        "\n",
        "$$\n",
        "z=L(x)=\\theta_0+\\sum_{i=1}^n\\theta_ix_i,\n",
        "$$\n",
        "the sigmoid function is applied $p=\\sigma(z)$. This $p$ is interepreted as the probability for the data belonging to class $1$. For $N$-class problem, we could generalize the sigmoid function to `softmax` function, whose value is a $N$-dimensional vector $p=[p_k]_{i=1}^N$. Here $p_k$ represents the probability for the data belonging to class $k$. Then after we get the vector $p$, we then find the highest probability and that indicates the class of the data point.\n",
        "\n",
        "The `softmax` function is defined in the following way:\n",
        "\n",
        "$$\n",
        "p_k=\\sigma(z)=\\dfrac{\\exp(z_k)}{\\sum_{i=1}^N\\exp(z_i)},\\quad \\text{ for }z=[z_1, z_2,\\ldots,z_N].\n",
        "$$\n",
        "In the model, each $z_i=L_i(x)=\\theta^{(i)}_0+\\sum_{i=1}^n\\theta^{(i)}_ix_i,$ has its own weights. \n",
        "\n",
        "The related cost function is also updated:\n",
        "\n",
        "$$\n",
        "J(\\Theta)=-\\sum_{i=1}^Ny_k\\ln(p_i).\n",
        "$$\n",
        "Therefore the same gradient descent algorithm can be applied.\n",
        "\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "Note that `sigmoid` function and the `binary crossentropy` cost functions are the special case of `softmax` function.\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "The labels are not recorded as labels, but as vectors. This is called dummy variables, or one-hot encodings.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "### Codes\n",
        "\n",
        "- Both `LogisticRegression()` and `SGDClassifier()` by default uses the one-vs-all naive idea. \n",
        "- Using `kears`, `softmax` can be implemented. The key configuration is the loss function `loss='categorical_crossentropy'` and the activation function `softmax`. Note that in this case the labels should be translated into one-hot vectors.\n",
        "\n",
        "We use `make_classification` as an example. To save time we won't carefully tune the hyperparameters here.\n"
      ],
      "id": "e7a43a89"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=2, n_repeated=2, n_classes=3, n_clusters_per_class=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
      ],
      "id": "c11b85bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although there are totally 10 features, the dataset can be visualized using the informative features. By description, the informative features are the first two.\n"
      ],
      "id": "5da4f782"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)"
      ],
      "id": "a3818747",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "id": "08a5624e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "clf = SGDClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "id": "fb650a9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To apply `keras` package, we should first change `y` into one-hot vectors. Here we use the function provided by `keras`.\n"
      ],
      "id": "49298a61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "import keras_core as keras\n",
        "from keras.utils import to_categorical\n",
        "from keras import models, layers\n",
        "\n",
        "vy_train = to_categorical(y_train, num_classes=3)\n",
        "vy_test = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(3, activation='softmax', input_dim=10))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, vy_train, epochs=50, batch_size=50, verbose=0)\n",
        "_ = model.evaluate(X_test, vy_test)"
      ],
      "id": "9bac279f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises and Projects\n",
        "\n",
        "\n",
        "\n",
        "::: {#exr-}\n",
        "Please hand write a report about the details of the math formulas for Logistic regression.\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: {#exr-}\n",
        "CHOOSE ONE: Please use `sklearn` to apply the LogisticRegression to one of the following datasets. You may either use `LogisticRegression` or `SGDClassifier`.\n",
        "\n",
        "- the `iris` dataset.\n",
        "- the dating dataset.\n",
        "- the `titanic` dataset.\n",
        "\n",
        "Please in addition answer the following questions.\n",
        "\n",
        "1. What is your accuracy score?\n",
        "2. How many epochs do you use?\n",
        "3. Plot the learning curve (accuracy vs training sizes).\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "::: {#exr-}\n",
        "\n",
        "CHOOSE ONE: Please use `keras` to apply the LogisticRegression to one of the following datasets.\n",
        "\n",
        "- the `iris` dataset.\n",
        "- the dating dataset.\n",
        "- the `titanic` dataset.\n",
        "\n",
        "Please in addition answer the following questions.\n",
        "\n",
        "1. What is your accuracy score?\n",
        "2. How many epochs do you use?\n",
        "3. What is the batch size do you use?\n",
        "4. Plot the learning curve (loss vs epochs, accuracy vs epochs).\n",
        "5. Analyze the bias / variance status.\n",
        "\n",
        "\n",
        ":::\n"
      ],
      "id": "3c12a7cb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ds24",
      "language": "python",
      "display_name": "ds24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}